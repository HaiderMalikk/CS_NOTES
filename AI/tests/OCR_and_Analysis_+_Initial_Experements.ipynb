{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed001617",
   "metadata": {},
   "source": [
    "# OCR and Document Analysis\n",
    "\n",
    "This section of the nootbook demonstrates how to:\n",
    "1. Extract text from PDFs and images using OCR or simple pdf extraction based on whats avalible \n",
    "2. Process and analyze the extracted text\n",
    "3. Use LLMs to extract structured information from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ace673",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain-community langchain-openai chromadb\n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv\n",
    "!pip3 install --upgrade --quiet pytesseract opencv-python pillow\n",
    "!pip3 install --upgrade --quiet pdf2image PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e4c97",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Import all necessary libraries for OCR, document processing, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c81d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Langchain modules for a lot of things\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import OCR libraries\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OPENAPI_API_KEY = os.environ.get('OPENAI_API_KEY')  # Fetch the API key from .env file\n",
    "\n",
    "# Defining LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAPI_API_KEY, temperature=0.2, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6760e2",
   "metadata": {},
   "source": [
    "## OCR Functions\n",
    "\n",
    "Define functions for OCR processing and image preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4986cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess the image to improve OCR accuracy.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive threshold to handle different lighting conditions\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                 cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Noise removal using median blur\n",
    "    processed = cv2.medianBlur(thresh, 3)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def perform_ocr(image, lang='eng', config='--psm 6'):\n",
    "    \"\"\"\n",
    "    Perform OCR on the given image.\n",
    "    \"\"\"\n",
    "    text = pytesseract.image_to_string(image, lang=lang, config=config)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_image(image_path, preprocess=True):\n",
    "    \"\"\"\n",
    "    Extract text from an image with optional preprocessing.\n",
    "    \"\"\"\n",
    "    if preprocess:\n",
    "        img = preprocess_image(image_path)\n",
    "    else:\n",
    "        img = cv2.imread(image_path)\n",
    "    \n",
    "    text = perform_ocr(img)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file using PyPDF and OCR.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "    from pdf2image import convert_from_path\n",
    "    import tempfile\n",
    "    \n",
    "    text_results = []\n",
    "    \n",
    "    try:\n",
    "        # First try direct text extraction with PyPDF\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text.strip():  # If we got meaningful text\n",
    "                text_results.append(text)\n",
    "            else:  # If no text was extracted, use OCR\n",
    "                # Convert PDF page to image and run OCR\n",
    "                images = convert_from_path(pdf_path)\n",
    "                for i, image in enumerate(images):\n",
    "                    # Save image to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp:\n",
    "                        temp_filename = temp.name\n",
    "                        image.save(temp_filename, 'PNG')\n",
    "                    \n",
    "                    # Run OCR on the image\n",
    "                    img = cv2.imread(temp_filename)\n",
    "                    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                    text = perform_ocr(gray)\n",
    "                    text_results.append(text)\n",
    "                    \n",
    "                    # Clean up the temporary file\n",
    "                    os.remove(temp_filename)\n",
    "                    \n",
    "        return \"\\n\\n\".join(text_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def visualize_ocr_results(image_path, text):\n",
    "    \"\"\"\n",
    "    Display the original image and extracted text side by side.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.text(0.1, 0.5, text, fontsize=12, wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.title('Extracted Text')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d9b42",
   "metadata": {},
   "source": [
    "## PDF Processing\n",
    "\n",
    "Load and process PDF documents for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7157fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the PDF file\n",
    "pdf_loader = PyPDFLoader('./assets/Month-to-Month-Lease-Agreement-Sample.pdf')  # Load a legal document\n",
    "pdf_pages = pdf_loader.load()  # Load all pages\n",
    "\n",
    "# Split the PDF into relevant chunks (paragraphs/sentences)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200, length_function=len, separators=['\\n\\n', '\\n', \" \"])\n",
    "pdf_chunks = text_splitter.split_documents(pdf_pages)\n",
    "\n",
    "# Display sample chunks to verify content\n",
    "print(f\"Total chunks: {len(pdf_chunks)}\")\n",
    "print(\"\\nSample chunk content:\")\n",
    "if pdf_chunks:\n",
    "    print(pdf_chunks[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9b5da",
   "metadata": {},
   "source": [
    "## Vector Embeddings and Storage\n",
    "\n",
    "Create embeddings and store them in a vector database for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings model to represent the document chunks numerically\n",
    "def get_embeddings():\n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=OPENAPI_API_KEY)\n",
    "    return embeddings_model\n",
    "\n",
    "embedding_model = get_embeddings()\n",
    "\n",
    "# Create a vector database\n",
    "def create_vector_store(pdf_chunks, embedding_model, store_name):\n",
    "    vectorstore = Chroma.from_documents(documents=pdf_chunks, embedding=embedding_model, persist_directory=store_name) \n",
    "    vectorstore.persist()  # Persist the vector store to make sure the folder is created\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = create_vector_store(pdf_chunks, embedding_model, \"assets/legal_vector_store\")\n",
    "\n",
    "# Query the database for relevant legal data\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f505432",
   "metadata": {},
   "source": [
    "## RAG for Legal Document Analysis\n",
    "\n",
    "Implement Retrieval Augmented Generation for legal document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442baaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal-specific prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are a legal assistant specialized in extracting relevant legal information.\n",
    "Use the following context to answer the question, if you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the context given above: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Modify the format_docs function to return the context as a string\n",
    "def format_docs(docs): \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# RAG chain construction to handle string input\n",
    "def rag_chain(question):\n",
    "    # 1. Retrieve relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    # 2. Format documents to get context\n",
    "    context = format_docs(retrieved_docs)\n",
    "    # 3. Create the input dictionary for the prompt\n",
    "    input_data = {\"context\": context, \"question\": question}\n",
    "    # 4. Format prompt with input data\n",
    "    formatted_prompt = prompt.format(**input_data)\n",
    "    # 5. Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return response\n",
    "\n",
    "# Example query using the RAG chain\n",
    "question = \"What are the key legal points discussed in this rental agreement?\"\n",
    "response = rag_chain(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ed6f8",
   "metadata": {},
   "source": [
    "## Structured Information Extraction\n",
    "\n",
    "Extract structured information from legal documents using Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28134ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Response using Pydantic to extract relevant legal data\n",
    "class LegalAnswerWithSources(BaseModel):\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "\n",
    "class ExtractedLegalInfo(BaseModel):\n",
    "    case_names_and_rulings: LegalAnswerWithSources  # Key legal cases and rulings discussed in the document\n",
    "    statutory_references: LegalAnswerWithSources  # Relevant legal statutes or references\n",
    "    court_decisions_and_outcomes: LegalAnswerWithSources  # Court decisions, opinions, and outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d36736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using structured output with the fixed chain approach\n",
    "def structured_rag_chain(question):\n",
    "    # Get relevant documents\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context = format_docs(retrieved_docs)\n",
    "    \n",
    "    # Create input for the prompt\n",
    "    input_data = {\"context\": context, \"question\": question}\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt.format(**input_data)\n",
    "    \n",
    "    # Get structured response\n",
    "    structured_response = llm.with_structured_output(ExtractedLegalInfo).invoke(formatted_prompt)\n",
    "    return structured_response\n",
    "\n",
    "# Get structured response\n",
    "structured_result = structured_rag_chain(\"What are the key legal points discussed in this rental agreement?\")\n",
    "print(structured_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6d87a",
   "metadata": {},
   "source": [
    "## Format and Display Results\n",
    "\n",
    "Process structured results into a formatted dataframe for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6706426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the structured result to a more readable format\n",
    "def format_structured_result(result):\n",
    "    # Create dictionary representation of the result\n",
    "    result_dict = {\n",
    "        \"case_names_and_rulings\": {\n",
    "            \"answer\": result.case_names_and_rulings.answer,\n",
    "            \"sources\": result.case_names_and_rulings.sources,\n",
    "            \"reasoning\": result.case_names_and_rulings.reasoning\n",
    "        },\n",
    "        \"statutory_references\": {\n",
    "            \"answer\": result.statutory_references.answer,\n",
    "            \"sources\": result.statutory_references.sources,\n",
    "            \"reasoning\": result.statutory_references.reasoning\n",
    "        },\n",
    "        \"court_decisions_and_outcomes\": {\n",
    "            \"answer\": result.court_decisions_and_outcomes.answer,\n",
    "            \"sources\": result.court_decisions_and_outcomes.sources,\n",
    "            \"reasoning\": result.court_decisions_and_outcomes.reasoning\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Set pandas display options to show full content\n",
    "    pd.set_option('display.max_colwidth', None)  # Don't truncate column content\n",
    "    pd.set_option('display.width', 1000)         # Wider display area\n",
    "    pd.set_option('display.max_rows', None)      # Show all rows\n",
    "    \n",
    "    # Create a nicely formatted dataframe\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        (category, field): value\n",
    "        for category, fields in result_dict.items()\n",
    "        for field, value in fields.items()\n",
    "    }, orient='index')\n",
    "    \n",
    "    # Rename the columns for clarity\n",
    "    df.columns = ['Content']\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index, names=['Category', 'Field'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Format and display the structured result\n",
    "formatted_df = format_structured_result(structured_result)\n",
    "print(\"Structured Legal Information Extracted from Document:\")\n",
    "display(formatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae64667",
   "metadata": {},
   "source": [
    "## OCR Example\n",
    "\n",
    "Example of using OCR to extract text from an image. To use this, you'll need to provide a path to an image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have the required libraries\n",
    "# Example OCR usage - handling both PDFs and images\n",
    "image_path = \"./assets/Month-to-Month-Lease-Agreement-Sample.pdf\"\n",
    "\n",
    "if image_path.lower().endswith('.pdf'):\n",
    "    extracted_text = extract_text_from_pdf(image_path)\n",
    "    # We can't use visualize_ocr_results directly with a PDF\n",
    "    print(f\"Extracted text from PDF (first 500 chars):\\n{extracted_text[:500]}...\")\n",
    "else:\n",
    "    # For regular image files\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    visualize_ocr_results(image_path, extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91660e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Extract text from documents using OCR or PDF parsing based on avalibility\n",
    "2. Process the text using NLP techniques\n",
    "3. Use LLMs to extract structured information from legal documents\n",
    "4. Present the information in a well-organized format\n",
    "\n",
    "The techniques shown here can be applied to various document analysis tasks beyond legal documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a17ecc",
   "metadata": {},
   "source": [
    "# Initial Experiments\n",
    "The code loads pre-trained summarization models (like BART, T5, and DistilBART) and uses them to generate summaries from input text. It includes functions to load models and tokenizers, summarize text, and evaluate the summarization quality using the ROUGE metric. This setup allows you to automatically summarize large texts and assess the quality of the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install torch\n",
    "!pip3 install evaluate\n",
    "!pip3 install tf-keras\n",
    "!pip3 install rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time\n",
    "import torch\n",
    "import evaluate  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load summarization pipeline (early prototypes)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Example: Summarize a long article\n",
    "def summarize(text):\n",
    "    return summarizer(text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "\n",
    "# Models evaluated\n",
    "models = {\n",
    "    \"BART\": \"facebook/bart-large-cnn\",\n",
    "    \"T5\": \"t5-small\",\n",
    "    \"DistilBART\": \"sshleifer/distilbart-cnn-12-6\"\n",
    "}\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")  \n",
    "\n",
    "# Function to load a model and tokenizer\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to evaluate summary quality\n",
    "def evaluate_summary(reference, generated):\n",
    "    results = rouge.compute(predictions=[generated], references=[reference])\n",
    "    return {\n",
    "        \"ROUGE-1\": results[\"rouge1\"],\n",
    "        \"ROUGE-2\": results[\"rouge2\"],\n",
    "        \"ROUGE-L\": results[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "# Function to measure compute cost\n",
    "def measure_inference_time(model_name, text):\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40)\n",
    "    end_time = time.time()\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "    return inference_time\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "This is a sample text that we will use to evaluate the summarization models. It contains a lot of information that we want to summarize. \n",
    "The goal is to see how well each model performs in terms of generating a concise summary while retaining the key points from the original text.\n",
    "We will compare the models based on their ROUGE scores and inference time. \n",
    "\"\"\"\n",
    "\n",
    "# Measure inference time for each model\n",
    "times = []\n",
    "for model_name, model_path in models.items():\n",
    "    t = measure_inference_time(model_path, text)\n",
    "    print(f\"{model_name}: {t:.4f} seconds\")\n",
    "    times.append(t)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(models.keys(), times, color='#4278aa')\n",
    "plt.title(\"Inference Time Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
