{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d53226",
   "metadata": {},
   "source": [
    "1. Introduction to PyTorch\n",
    "PyTorch is a deep learning framework primarily used for building machine learning models. It's a flexible and efficient library that offers strong support for dynamic computation graphs (eager execution) and deep integration with Python.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Dynamic Computation Graph: PyTorch allows you to modify the computation graph dynamically, which is more intuitive for debugging and easier to work with for complex models.\n",
    "Automatic Differentiation: PyTorch’s Autograd module automatically computes gradients for backpropagation, simplifying the training process.\n",
    "GPU Acceleration: PyTorch seamlessly integrates with CUDA, enabling you to run models on GPUs for faster computations.\n",
    "Deep Learning Modules: PyTorch comes with a high-level neural network module, torch.nn, for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b878cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio -f https://download.pytorch.org/whl/metal.html # latest stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.backends.mps.is_available())  # Should return True\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\") \n",
    "# to use device do: x.to(device) where x can be a tensor or model etc.\n",
    "# just making the device var or doing torch.device(\"mps\") does not move the tensor/model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27556f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch import \n",
    "import torch\n",
    "\n",
    "\"\"\" \n",
    "In pytorch, a tensor is a multi-dimensional array that can be used to store data. same as TF tensor\n",
    "- NOTE unlike Tf-metal pytorch does not have a special tensor for GPU, it uses the same tensor class for CPU and GPU. unless you have cuda (nvidia GPU)\n",
    "\n",
    "We can preform operations on tensors, such as addition, multiplication, etc. the operations are similar to numpy beacause they are element-wise\n",
    "meaning each element in the tensor is operated on independently with the element in the other tensor.\n",
    "EX: \n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = a + b # c = [5, 7, 9] (1+4, 2+5, 3+6)\n",
    "\n",
    "WE can also reshape the tensor, and change the data type of the tensor, such as int32, int64, float32, float64\n",
    "for reshaping this means we can change the number of rows and columns in the tensor, but the total number of elements must remain the same.\n",
    "i.e 2x3 -> 3x2\n",
    "\"\"\"\n",
    "# Creating a tensor from a list\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Creating a tensor with zeros, ones, or random values\n",
    "zeros_tensor = torch.zeros(2, 3)  # 2x3 matrix of zeros\n",
    "ones_tensor = torch.ones(2, 3)  # 2x3 matrix of ones\n",
    "random_tensor = torch.rand(2, 3)  # 2x3 matrix with random values\n",
    "\n",
    "# Creating a tensor with a specific device (CPU or GPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "tensor_on_device = torch.tensor([1.0, 2.0, 3.0]).to(device)  # move tensor to GPU if available else CPU\n",
    "\n",
    "# Basic operations\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "c = a + b  # Element-wise addition\n",
    "d = a * b  # Element-wise multiplication\n",
    "\n",
    "# Matrix multiplication\n",
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(3, 2)\n",
    "z = torch.mm(x, y)  # Matrix multiplication\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "x_reshaped = x.view(3, 2)  # Reshaping to a different dimension\n",
    "\n",
    "\"\"\" \n",
    "PyTorch Autograd (Automatic Differentiation)\n",
    "\n",
    "- backpropagation: the process of computing the gradients of the loss function with respect to the model parameters.\n",
    "- a backward pass is the step wher you use backpropagation to compute the gradients of the loss function with respect to the model parameters.\n",
    "for ex in a neural network, we need to compute the gradients of the loss function with respect to the weights and biases of the network.\n",
    "what this dose for us is it allows us to update the weights and biases of the network in the direction that reduces the loss function.\n",
    "this makes the network learn from the data and improve its performance.\n",
    "in our case we backpropagate the gradients from out to x this gradient vector tells us how much the output changes with respect to the input\n",
    "with this gradient vector we can update the weights and biases of the network in the direction that reduces the loss function.\n",
    "\n",
    "- Autograd is PyTorch's automatic differentiation engine that powers the backpropagation in neural networks. It automatically calculates the gradients for tensor operations.\n",
    "- Creating Tensors with requires_grad: Tensors that require gradients (for backpropagation) are marked by setting requires_grad=True.\n",
    "when we backpropagate, we calculate the gradient of the output with respect to the input.\n",
    "\n",
    "The backward() function: Computes all the gradients in the computation graph. It accumulates gradients into .grad attributes of tensors.\n",
    "For scalar outputs, out.backward() is enough. For non-scalar outputs, you need to pass a gradient argument to backward().\n",
    "\n",
    "NOTE: after backpropagation, the gradients are accumulated in the .grad attribute of the tensor. hence x.grad will contain the gradients of the output with respect to x.\n",
    "we store it in X as thats the starting point of the computation graph. (see TF notes) but basically we can store it in any tensor but be default we store it in x as its the start.\n",
    "\n",
    "the second EX has number so \n",
    "v= [1, 1]\n",
    "out = v ** 2 = [1, 1]\n",
    "out.sum() = 1 + 1 = 2\n",
    "out.sum().backward() computes the gradients of out with respect to v and stores them in v.grad so 2 is the gradient of out with respect to v and it is stored in v.grad\n",
    "v.grad = [2, 2] because the gradient of v with respect to itself is 1 and we have 2 elements in v so the gradient is 2 for each element.\n",
    "\"\"\"\n",
    "x = torch.randn(2, 2, requires_grad=True) # Create a tensor with requires_grad=True initially it has no gradient and is just a 2x2 matrix\n",
    "y = x + 2 # y is a tensor that is the result of adding 2 to each element of x. y = [[x[0][0]+2, x[0][1]+2], [x[1][0]+2, x[1][1]+2]] x is rand so we dont know\n",
    "z = y * y * 3 # z is a tensor that is the result of multiplying y by itself and then multiplying by 3. z = 3 * y * y\n",
    "out = z.mean() # out is a scalar that is the mean of z. out = (3 * y * y).mean() the mean is just a number \n",
    "\n",
    "# Backpropagate, we backpropagate the gradients from out to x this gradient vector tells us how much the output changes with respect to the input\n",
    "out.backward() # this computes the gradients of out with respect to x and stores them in x.grad \n",
    "print(x.grad) # this will print the gradients of out with respect to x. this is a 2x2 matrix that tells us how much the output changes with respect to the input\n",
    "\n",
    "#EX2\n",
    "# in this we use the sum of the squares of the elements of v as the output insted of the mean\n",
    "v = torch.tensor([1.0, 1.0], requires_grad=True) # Create a tensor with requires_grad=True\n",
    "out = v ** 2 # out is a tensor that is the result of raising v to the power of 2\n",
    "out.sum().backward()  # Backpropagate the sum of the squares what this does is it computes the gradients of out with respect to v and stores them in v.grad the sum means we are summing the elements of out\n",
    "print(v.grad)  # Gradients of v\n",
    "\n",
    "\"\"\" \n",
    "Neural Networks with torch.nn\n",
    "The torch.nn module provides a set of tools to build and train neural networks.\n",
    "\n",
    "Building a Model (Class-based Approach)\n",
    "\n",
    "You define a neural network by subclassing torch.nn.Module.\n",
    "\n",
    "what is a FULLY CONNECTED LAYER?: a fully connected layer is a layer in a neural network where each neuron in the layer is connected to every neuron in the previous layer.\n",
    "from tensorflow we know that the dense layer is a fully connected layer.\n",
    "\n",
    "Forward Pass: The forward pass is the process of passing the input data through the layers of the neural network and computing the output.\n",
    "- NOTE a backward pass is the process of computing the gradients of the loss function with respect to the model parameters. not the same as the forward pass.\n",
    "\n",
    "The RELU is a non-linear activation function that is used to introduce non-linearity into the network. see TF notes for more details.\n",
    "\n",
    "EX:\n",
    "in our example we have 2 layers, the first layer has 784 input neurons and 128 output neurons, and the second layer has 128 input neurons and 10 output neurons.\n",
    "in the forward pass we pass the input data through the first layer, apply the ReLU activation function what relu dose in our case is lets us activate the neurons based on the input, \n",
    "and then pass the output through the second layer and return that as the output of the network.\n",
    "\n",
    "ReLU (Rectified Linear Unit) sets all negative values to zero and keeps positive values unchanged.\n",
    "In a simple NN, it adds non-linearity, helping the model learn complex patterns instead of just straight lines. another one is signmoid (see tf)\n",
    "it basically tells the NN what input values to keep and what to ignore. i.e what is important and what is not.\n",
    "Mathematically:\n",
    "ReLU(x)=max⁡(0,x)\n",
    "ReLU(x)=max(0,x)\n",
    "✅ Keeps positives → same\n",
    "✅ Turns negatives → 0\n",
    "\"\"\"\n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim as optim # for optimizers\n",
    "\n",
    "class SimpleNN(nn.Module): # inherit from nn.Module\n",
    "    def __init__(self): \n",
    "        super(SimpleNN, self).__init__() # call the parent class constructor to initialize the module\n",
    "        self.fc1 = nn.Linear(784, 128)  # Fully connected layer (input 784, output 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Fully connected layer (input 128, output 10)\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation function \n",
    "        x = self.fc2(x) # output layer takes input from previous layer\n",
    "        return x # return the output of the network\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleNN()\n",
    "\n",
    "\"\"\" \n",
    "Training a Model\n",
    "\n",
    "Training a model involves the following steps:\n",
    "\n",
    "Define the loss function (e.g., nn.CrossEntropyLoss). this is used for multi-class classification problems i.e predicting numbers from 0 to 9 for ex the loss function is used to compute the difference between the predicted output and the target output help us measure how well the model is performing.\n",
    "Define an optimizer (e.g., optim.SGD or optim.Adam). this is used to update the model parameters based on the gradients computed in the backward pass. basically we use the optimizer to update the weights and biases of the network in the direction that reduces the loss function.\n",
    "Perform the forward pass and compute the loss. this is used to compute the difference between the predicted output and the target output help us measure how well the model is performing.\n",
    "Backpropagate to compute gradients. this is used to compute the gradients of the loss function with respect to the model parameters. after we compute the gradients we can update the weights and biases of the network in the direction that reduces the loss function.\n",
    "Update weights using the optimizer. this is used to update the model parameters based on the gradients computed in the backward pass. basically we use the optimizer to update the weights and biases of the network in the direction that reduces the loss function.\n",
    "\n",
    "Backwards pass vs Backpropagation: \n",
    "Backpropagation = the method (the algorithm for calculating gradients).\n",
    "Backward pass = the action (the model doing the gradient calculation step during training).\n",
    "\n",
    "Simple Timeline in Training:\n",
    "Forward pass → compute output.\n",
    "Compute loss.\n",
    "Backward pass → run backpropagation → compute gradients.\n",
    "Optimizer step → update weights.\n",
    "\n",
    "# NOTE: a epoch is one complete pass through the entire training dataset so here we have 10 epochs meaning we will pass through the entire training dataset (X, y) 10 times.\n",
    "\"\"\"\n",
    "# NOTE: you random data maches the model meaning they are the same shape, so you can use any data you want, but in a real world example you would use real data.\n",
    "# Create 1000 samples, each with 10 features a feature is a column in the input data that represents a specific characteristic of the data like e.g. height, weight, age, etc.\n",
    "X = torch.randn(1000, 784)  # inputs (features)\n",
    "y = torch.randint(0, 10, (1000,))  # outputs (labels) - for binary classification (0 or 1)\n",
    "# Wrap into a dataset\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "# Create a trainloader\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs): # for each epoch\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(data)  # Forward pass using the model above\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update weights\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print loss for each epoch\n",
    "# we should see the loss decrease as we train the model\n",
    "\n",
    "\"\"\" \n",
    "Datasets and DataLoader\n",
    "PyTorch provides utilities for loading and batching data using torch.utils.data.Dataset and DataLoader.\n",
    "\n",
    "Creating a Custom Dataset\n",
    "- A dataset is a class that inherits from torch.utils.data.Dataset and implements the __len__ and __getitem__ methods.\n",
    "- in torch a dataset is a class that represents a collection of data samples. it is used to load and preprocess the data for training and testing the model.\n",
    "\n",
    "### what we achove here is we can load the data in batches of 64 samples and shuffle the data so that we dont get the same data every time we train the model.\n",
    "\n",
    "- you can also use Predefined Datasets (e.g., MNIST, CIFAR)\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_data = torch.randn(1000, 784)  # Random training data with 1000 samples and 784 features\n",
    "train_labels = torch.randint(0, 10, (1000,))  # Random training labels numbers from 0 to 9\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "train_dataset = CustomDataset(train_data, train_labels) # this is the dataset we created above\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # this is the dataloader that loads the dataset in batches of 64 samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8e3ec",
   "metadata": {},
   "source": [
    "LLM example with MPS (mac gpu acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # for cpu do device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a simple transformer-based language model\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=512, nhead=8, \n",
    "                 num_layers=6, dim_feedforward=2048):\n",
    "        super(SimpleTransformerLM, self).__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding (simplified for this example)\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return x\n",
    "\n",
    "# Create model and move to MPS device\n",
    "vocab_size = 10000  # Vocabulary size\n",
    "seq_length = 128    # Sequence length\n",
    "batch_size = 16     # Batch size\n",
    "\n",
    "model = SimpleTransformerLM(vocab_size=vocab_size)\n",
    "model.to(device)  # Move model to GPU\n",
    "print(f\"Model moved to {device}\")\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "target_data = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model for a few steps to demonstrate GPU usage\n",
    "def train_step(model, inputs, targets):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    # Reshape for cross entropy\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = targets.view(-1)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Test training performance\n",
    "print(\"\\nTraining for 20 iterations to test performance:\")\n",
    "start_time = time.time()\n",
    "for i in range(20):\n",
    "    loss = train_step(model, input_data, target_data)\n",
    "    print(f\"Iteration {i+1}, Loss: {loss:.4f}\")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\") # 2.80 sec on GPU, 13.35 sec on CPU (MacBook Pro M2 16GB)\n",
    "\n",
    "# Generate predictions with the model\n",
    "def generate_text(model, start_tokens, max_length=20):\n",
    "    model.eval()\n",
    "    current_tokens = start_tokens.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions for the next token\n",
    "            logits = model(current_tokens)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append new token to sequence\n",
    "            current_tokens = torch.cat([current_tokens, next_token], dim=1)\n",
    "    \n",
    "    return current_tokens\n",
    "\n",
    "# Try generating some \"text\" (just token IDs in this example)\n",
    "start_seq = torch.randint(0, vocab_size, (1, 5)).to(device)\n",
    "generated = generate_text(model, start_seq)\n",
    "print(\"\\nGenerated token sequence:\")\n",
    "print(generated.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
