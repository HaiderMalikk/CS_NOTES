{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba17df",
   "metadata": {},
   "source": [
    "# Data Pipelines in TensorFlow\n",
    "\n",
    "### What is a “data pipeline” in machine learning?\n",
    "A data pipeline is the system that feeds data into your model during training and evaluation.\n",
    "Think of it like a kitchen conveyor belt:\n",
    "- At one end, you load in raw ingredients (text files, CSVs, TFRecords, images, etc.).\n",
    "- Along the belt, you wash, cut, and prepare them (decode, normalize, tokenize, batch).\n",
    "- At the other end, your model gets perfectly prepared “mini-meals” (tensors ready for training).\n",
    "If this belt is slow, your model waits idle, wasting GPU power.\n",
    "If it’s too fast, you waste memory.\n",
    "So ML engineers tune it carefully for balance and throughput.\n",
    "\n",
    "### Why TensorFlow needs pipelines\n",
    "TensorFlow models are trained in graphs — computations that run on CPUs, GPUs, or TPUs.\n",
    "Those devices are fast, but the bottleneck is usually the data loading step:\n",
    "reading from disk, decoding files, augmenting images, or tokenizing text.\n",
    "\n",
    "To fix this, TensorFlow provides the tf.data API — a high-performance, graph-integrated data pipeline system.\n",
    "It lets you:\n",
    "- Stream data efficiently from disk or memory\n",
    "- Parallelize operations across CPU cores\n",
    "- Prefetch batches so the GPU never waits\n",
    "- Cache preprocessed data to avoid recomputation\n",
    "- Compose your data transformations like a chain\n",
    "  \n",
    "So when we say “TensorFlow pipeline,” we really mean:\n",
    "A tf.data.Dataset object that describes how to load, process, and batch your data efficiently, often entirely inside the TensorFlow graph.\n",
    "\n",
    "### What actually happens inside a pipeline\n",
    "Let’s walk through a typical training example:\n",
    "\n",
    "```py\n",
    "dataset = (\n",
    "    tf.data.TextLineDataset(\"reviews.txt\")     # 1️⃣ Read from file(s)\n",
    "    .map(parse_line)                           # 2️⃣ Parse or tokenize text\n",
    "    .shuffle(buffer_size=10000)                # 3️⃣ Shuffle samples for randomness\n",
    "    .batch(32)                                 # 4️⃣ Group into batches\n",
    "    .prefetch(tf.data.AUTOTUNE)                # 5️⃣ Prepare next batch while GPU trains\n",
    ")\n",
    "```\n",
    "Let’s break it down:\n",
    "\n",
    "| Step   | Function                          | What It Does                                                   | Why It Matters                                |\n",
    "|--------|-----------------------------------|----------------------------------------------------------------|----------------------------------------------|\n",
    "| 1. Read | TextLineDataset, TFRecordDataset, etc. | Loads data efficiently from disk, streaming, or memory.         | Prevents “file I/O bottlenecks”.             |\n",
    "| 2. Map  | .map(func)                       | Applies a transformation to each element (like parsing JSON, tokenizing text, decoding images). | Lets you preprocess inside TF (parallelizable). |\n",
    "| 3. Shuffle | .shuffle(buffer_size)          | Randomizes sample order each epoch.                            | Improves generalization, prevents overfitting. |\n",
    "| 4. Batch | .batch(batch_size)              | Groups samples into mini-batches for training.                 | Allows vectorized GPU operations.            |\n",
    "| 5. Prefetch | .prefetch(tf.data.AUTOTUNE)   | Loads next batch while GPU is training on current batch.       | Maximizes GPU utilization.                   |\n",
    "\n",
    "That’s a complete data pipeline — from reading → preprocessing → batching → feeding.\n",
    "\n",
    "### What makes TensorFlow pipelines special\n",
    "TensorFlow’s tf.data pipelines are not just loops — they are part of the graph, meaning:\n",
    "- They can run asynchronously from the model.\n",
    "- They can overlap CPU preprocessing and GPU training.\n",
    "- They can automatically tune performance using tf.data.AUTOTUNE.\n",
    "- They can scale across devices (e.g., multiple GPUs or TPUs).\n",
    "This is what makes them much faster and cleaner than writing your own Python loop like:\n",
    "``` py\n",
    "for x, y in dataset:\n",
    "    model.train_on_batch(x, y)\n",
    "```\n",
    "That’s fine for small demos — but real-world ML needs throughput and reproducibility, which tf.data gives you.\n",
    "\n",
    "### TF pipeline in context of NLP\n",
    "When your data is text, the pipeline also does:\n",
    "1. Reading raw text (from files, CSVs, TFRecords).\n",
    "2. Tokenizing (turning words → integers).\n",
    "3. Padding/truncating sequences.\n",
    "4. Building attention masks or features.\n",
    "5. Batching for model input.\n",
    "\n",
    "Example:\n",
    "```py\n",
    "dataset = (\n",
    "    tf.data.TextLineDataset(\"data.txt\")\n",
    "    .map(tokenize_and_pad, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "```\n",
    "\n",
    "The tokenize_and_pad function might call:\n",
    "- tf.strings.split() (basic whitespace)\n",
    "- or a pretrained tokenizer like keras_nlp.tokenizers.WordPieceTokenizer\n",
    "- or tf.py_function wrapping a Python tokenizer\n",
    "That’s what “integrating a tokenizer into the pipeline” means — it becomes part of this conveyor belt.\n",
    "\n",
    "### Why we care about things like cache(), prefetch(), AUTOTUNE\n",
    "These are performance tuning knobs for your data loader:\n",
    "\n",
    "| Function                  | Description                                           | Common Use                                      |\n",
    "|---------------------------|-------------------------------------------------------|------------------------------------------------|\n",
    "| cache()                  | Stores the preprocessed dataset in memory or on disk after first epoch. | When dataset fits in RAM or is small.          |\n",
    "| prefetch()               | Loads the next batch while the model trains on the current one. | Always use with AUTOTUNE.                      |\n",
    "| AUTOTUNE                 | Lets TF automatically pick parallelism/prefetch settings. | Default best practice.                         |\n",
    "| map(num_parallel_calls=AUTOTUNE) | Runs preprocessing functions in parallel threads. | Speeds up CPU-bound steps like decoding/tokenizing. |\n",
    "\n",
    "Together, these let TensorFlow stream data continuously to your GPU.\n",
    "\n",
    "### How it connects to what you’ll build later\n",
    "Eventually, your model training loop will look like this:\n",
    "``` py\n",
    "for x_batch, y_batch in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "The dataset in this loop is what the pipeline built.\n",
    "It keeps producing ready-to-train batches infinitely or per epoch.\n",
    "\n",
    "That’s why every TensorFlow engineer must master tf.data — it’s how you feed your models at scale.\n",
    "\n",
    "Summary — “What are TensorFlow pipelines?”\n",
    "| Concept         | Intuition                          | Analogy                                      |\n",
    "|------------------|------------------------------------|----------------------------------------------|\n",
    "| Pipeline         | The complete data flow from disk → ready tensors | A kitchen conveyor belt for data            |\n",
    "| tf.data.Dataset  | TensorFlow object representing a pipeline | Recipe for data preparation                 |\n",
    "| map()            | Transform each data sample        | Chop vegetables on the belt                 |\n",
    "| batch()          | Group samples together            | Pack boxes of meals                         |\n",
    "| prefetch()       | Get the next batch ready          | Chef preps next dish while plating current one |\n",
    "| cache()          | Save processed data               | Store pre-chopped ingredients               |\n",
    "| AUTOTUNE         | Auto-optimizes performance        | Smart chef who adjusts speed                |\n",
    "\n",
    "NOTE: in scikit-learn, pipelines chain processing + modeling steps, while in TensorFlow, pipelines mainly handle data loading, preprocessing, and feeding efficiently to the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983675a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Example: High-Level Text Classification Pipeline\n",
    "Let’s build a clean pipeline for a text classification task — say, classifying IMDB movie reviews as positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6535 - loss: 0.5926 - val_accuracy: 0.8550 - val_loss: 0.3496\n",
      "Epoch 2/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.8736 - loss: 0.3008 - val_accuracy: 0.8558 - val_loss: 0.3340\n",
      "Epoch 3/3\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.9040 - loss: 0.2412 - val_accuracy: 0.8660 - val_loss: 0.3235\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Load dataset\n",
    "# TensorFlow Datasets (TFDS) gives us ready-to-use data\n",
    "train_ds, test_ds = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,  # returns (text, label)\n",
    ")\n",
    "\n",
    "# 2. Tokenization and TextVectorization layer, tokenizing is to convert text to numbers (vectors) and textVectorization is a layer that helps with that\n",
    "# This is a built-in Keras preprocessing layer for text\n",
    "vocab_size = 10000 # Limit vocabulary size to top 10,000 words to save memory this is 10000 words from the dataset\n",
    "seq_length = 250 # Limit each review to 250 words\n",
    "\n",
    "# a vectorization layer is created to handle the tokenization and vectorization of text data it converts text into sequences of integers using a fixed vocabulary size and sequence length the word vocab for the layer is learned from the training data\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length\n",
    ")\n",
    "\n",
    "# You must \"adapt\" the layer to learn the vocabulary from the text\n",
    "train_text = train_ds.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "# 3. Preprocessing pipeline function\n",
    "def preprocess_text(text, label):\n",
    "    text = vectorize_layer(text)  # Apply the TextVectorization layer to the text this will convert the text to integer sequences from out learned vocabulary \n",
    "    return text, label # Return the processed text and label as a tuple\n",
    "\n",
    "# 4. Apply preprocessing, shuffle, batch, and prefetch\n",
    "batch_size = 32\n",
    "\n",
    "# Apply preprocessing to the datasets and optimize them for performance\n",
    "train_ds = (\n",
    "    train_ds \n",
    "    .shuffle(10000) # Shuffle the dataset with a buffer size of 10,000 to ensure randomness\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE) # Apply the preprocessing function in parallel\n",
    "    .batch(batch_size) # Batch the data\n",
    "    .prefetch(tf.data.AUTOTUNE) # Prefetch data for better performance AUTOTUNE lets TensorFlow decide the optimal number of batches to prefetch\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the test dataset\n",
    "test_ds = (\n",
    "    test_ds\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE) # Apply the preprocessing function in parallel\n",
    "    .batch(batch_size) # Batch the data\n",
    "    .prefetch(tf.data.AUTOTUNE) # Prefetch data for better performance\n",
    ")\n",
    "\n",
    "# 5. Build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length=seq_length), # Embedding layer to convert integer sequences to dense vectors of fixed size why because neural networks work better with dense vectors\n",
    "    tf.keras.layers.GlobalAveragePooling1D(), # Global average pooling to reduce the sequence dimension pooling is a way to downsample the data means taking the average of all the elements in the sequence and reducing the dimensionality of the data here we reduce the dimentions of the sequence to a single vector our sequence is now represented by a single vector (our sequence here was 250 words long now its just one vector) this vector represents the entire review the embedding process is done by pooling function\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"), # A dense hidden layer with ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\") # Output layer for binary classification (positive/negative review)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", # Adam optimizer is an efficient optimization algorithm that adjusts the learning rate during training we use the optimizer to apply gradients to the model's weights based on the loss function\n",
    "    loss=\"binary_crossentropy\", # Binary crossentropy loss function for binary classification tasks\n",
    "    metrics=[\"accuracy\"] # Track accuracy during training\n",
    ")\n",
    "\n",
    "# 6. Train the model — the dataset is already optimized\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=3)\n",
    "\n",
    "# In this code example we built a complete TensorFlow data pipeline for text data using the IMDB reviews dataset we loaded the data, tokenized and vectorized the text using a TextVectorization layer, applied preprocessing, shuffling, batching, and prefetching to optimize performance finally we built and trained a simple neural network model for sentiment analysis on the preprocessed data\n",
    "# what we mean by a data pipeline is a series of steps that process and prepare data for training machine learning models these steps typically include loading the data, preprocessing it (like tokenization and vectorization for text data), batching it into manageable sizes, and optimizing the data flow for performance during training\n",
    "# in our case that part (the data pipeline part) was: loading the dataset, applying the TextVectorization layer, shuffling, batching, and prefetching the data\n",
    "# this ensures that the data is in the right format and is efficiently fed into the model during training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
