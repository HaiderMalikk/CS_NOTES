{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765f36bc",
   "metadata": {},
   "source": [
    "# Environment & Reproductibility in ML\n",
    "\n",
    "Reproducibility = when you run the same code twice (same data, same config), you should get the same results (model weights, metrics, outputs) — or at least very close. This is crucial for debugging, comparing experiments, and being confident changes in results are due to code/config changes, not random noise.\n",
    "\n",
    "Getting reproducible runs in TensorFlow requires controlling many randomness sources: Python, NumPy, TensorFlow, OS-level hashing, dataset shuffling, and GPU library nondeterminism. We'll set seeds and flags to control these, explain why each is necessary, and show tests you can run.\n",
    "\n",
    "## Sources of randomness & what to control\n",
    "\n",
    "- Python-level randomness (random module) Used by many libraries and your own code (e.g., random.shuffle).\n",
    "- NumPy RNG (numpy.random) Many preprocessing or ML libs use NumPy random.\n",
    "- TensorFlow RNG (tf.random) Weight initialization, dropout, shuffling inside TF ops, randomized GPU algorithms.\n",
    "- tf.data shuffling dataset.shuffle(buffer_size) uses its own RNG (you can pass a seed).\n",
    "- OS-level hashing (PYTHONHASHSEED) Affects iteration order of hashed collections (dict set order) and can subtly change execution. accsessing is in order but when inserting in a dict the order in which keys are inserted into a dict might change depending on input hash order (hash values of objects like str) EX: \n",
    "```py\n",
    "# save as test.py\n",
    "data = {'a', 'b', 'c'}  # a set is unordered (hash-based)\n",
    "d = {k: None for k in data}\n",
    "print(list(d.keys()))\n",
    "# if your run in terminal\n",
    "PYTHONHASHSEED=0 python test.py\n",
    "PYTHONHASHSEED=1 python test.py\n",
    "# you might get\n",
    "# Run 1\n",
    "['a', 'b', 'c']\n",
    "# Run 2\n",
    "['b', 'c', 'a']\n",
    "# running without hash seed its randomized \n",
    "python test.py # runs with random seed like PYTHONHASHSEED=987654\n",
    "# The set {'a', 'b', 'c'} doesn’t preserve order — it depends on hash values.\n",
    "# When PYTHONHASHSEED changes, the hash of each string changes → the set’s iteration order changes → keys get inserted into the dict in a different order.\n",
    "# so for stable results make it a constant like 0, 1 etc\n",
    "```\n",
    "- Non-deterministic GPU kernels (cuDNN/cuBLAS) Some GPU ops (e.g., certain convolution/cudnn algorithms, atomic adds, reductions) are inherently nondeterministic for performance reasons. Environment flags can force deterministic algorithms but may reduce performance or even be unavailable for certain ops.\n",
    "- Other system-level variance Different TF/CUDA/cuDNN versions, different device counts, different threading settings, or multi-process race conditions can change outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def set_seeds(seed=42, deterministic_ops=False):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)        # stabilizes hashing order, fixes hash randomization that can affect iteration order of dicts/sets across processes.\n",
    "    random.seed(seed)                               # python random (makes all random numbers the same but still exhibit randomness)\n",
    "    np.random.seed(seed)                            # numpy random, deterministic behavior for numpy functions same as random.seed but for numpy\n",
    "    tf.random.set_seed(seed)                        # tensorflow random sets the graph-level seed for TensorFlow operations (affects initializers, dropout, etc.) for example it will initialize the weights as the same random numbers at the start of training process and will pick the same random neurons to drop during dropout layers etc.\n",
    "\n",
    "    # NOTE: enabling deterministic ops may slow things or break some ops, tells TF to prefer deterministic kernels where TF provides them; availability and effect depend on TF, CUDA, cuDNN versions and on which ops you use.\n",
    "    if deterministic_ops:\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'    # try to force deterministic kernels\n",
    "        # optionally: os.environ['TF_CUDNN_DETERMINISTIC'] = '1' (older TF guidance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097c5ba",
   "metadata": {},
   "source": [
    "### Recommended extras (for tf.data and reproducible shuffling)\n",
    "- Use explicit seeds in tf.data shuffle and repeat:\n",
    "- Use deterministic batching order when needed:\n",
    "- If you want exact same batch composition across runs, set reshuffle_each_iteration=False and use the same seed.\n",
    "- If you rely on tf.data.experimental.service or parallelism, be aware they can introduce nondeterminism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX:\n",
    "ds = tf.data.Dataset.range(10) # create a simple dataset of 10 elements inside a tf dataset\n",
    "seed = 1234\n",
    "ds = ds.shuffle(buffer_size=10000, seed=seed, reshuffle_each_iteration=False)\n",
    "# or for training with repeat:\n",
    "ds = ds.shuffle(buffer_size=10000, seed=seed, reshuffle_each_iteration=True)\n",
    "\n",
    "# print dataset as a list\n",
    "print(list(ds.as_numpy_iterator())) # will always print the same order across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f58d94",
   "metadata": {},
   "source": [
    "### Optional: stronger OS / thread controls (when you need very strict reproducibility)\n",
    "- Set OMP_NUM_THREADS, TF_NUM_INTRAOP_THREADS, TF_NUM_INTEROP_THREADS to fixed values so thread scheduling is stable.\n",
    "- Run inside a Docker image with pinned TF/CUDA/cuDNN versions (this helps when you want to share experiments across machines).\n",
    "\n",
    "EX:\n",
    "``` bash\n",
    "export OMP_NUM_THREADS=1\n",
    "export TF_NUM_INTRAOP_THREADS=1\n",
    "export TF_NUM_INTEROP_THREADS=1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7914b",
   "metadata": {},
   "source": [
    "### Tradeoffs \n",
    "- Performance vs determinism: Making TensorFlow operations fully deterministic (e.g., TF_DETERMINISTIC_OPS=1) can slow training because faster GPU algorithms (like cuDNN) are often nondeterministic. Use determinism for debugging or CI; disable it for max speed in final training.\n",
    "- Not everything can be deterministic: Some ops (collective operations, certain GPU kernels) remain nondeterministic, causing small numeric differences.\n",
    "- Multi-GPU / distributed training: Determinism is harder across multiple GPUs or machines due to variable reduction orders and NCCL algorithms. Exact reproducibility may be limited.\n",
    "- TF/CUDA version changes: Results can differ across TensorFlow, CUDA, or cuDNN versions; pin versions to reproduce results exactly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2680eb",
   "metadata": {},
   "source": [
    "### Practical checklist to include in your repo README / experiment.json\n",
    "- seed: 1337\n",
    "- deterministic_ops: (true/false)\n",
    "- tf_version: tf.__version__\n",
    "- cuda_version, cudnn_version (if GPU)\n",
    "- python_version\n",
    "- git commit hash\n",
    "- dataset id / TFRecord path\n",
    "- shuffle buffer size, batch size, tokenizer mode, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca6351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config =  {\n",
    "  \"seed\": 1337,\n",
    "  \"deterministic_ops\": False,\n",
    "  \"tf_version\": \"2.14.0\",\n",
    "  \"python_version\": \"3.10.12\",\n",
    "  \"git_commit\": \"abc1234\",\n",
    "  \"shuffle_buffer_size\": 10000,\n",
    "  \"batch_size\": 32,\n",
    "  \"tokenizer_mode\": \"python\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb29be",
   "metadata": {},
   "source": [
    "### Example usage \n",
    "output:\n",
    "```\n",
    "Sum w1: 21.006514500710182\n",
    "Sum w2: 21.006514500710182\n",
    "Are weights equal? True\n",
    "```\n",
    "- same everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "716a9262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum w1: 21.006514500710182\n",
      "Sum w2: 21.006514500710182\n",
      "Are weights equal? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Ensure assets folder exists\n",
    "os.makedirs(\"assets\", exist_ok=True)\n",
    "\n",
    "# --- Function to run your training and save weights ---\n",
    "def run_model(weights_fname, seed=42, deterministic_ops=False):\n",
    "    # Set seeds for reproducibility\n",
    "    set_seeds(seed, deterministic_ops=deterministic_ops)\n",
    "    \n",
    "    # Small model\n",
    "    inputs = tf.keras.Input(shape=(4,), dtype=tf.float32)\n",
    "    x = tf.keras.layers.Dense(8, activation='relu')(inputs)\n",
    "    outputs = tf.keras.layers.Dense(2)(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Synthetic data\n",
    "    X = np.random.randn(100, 4).astype('float32')\n",
    "    y = np.random.randint(0, 2, size=(100,)).astype('int32')\n",
    "    \n",
    "    # Compile and train\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    model.fit(X, y, epochs=3, batch_size=16, verbose=0)\n",
    "    \n",
    "    # Save weights\n",
    "    weights_path = os.path.join(\"assets\", weights_fname)\n",
    "    model.save_weights(weights_path)\n",
    "    return weights_path\n",
    "\n",
    "# --- Run first time ---\n",
    "w1_path = run_model(\"w1.weights.h5\", seed=42)\n",
    "# --- Run second time ---\n",
    "w2_path = run_model(\"w2.weights.h5\", seed=42)\n",
    "\n",
    "# --- Function to sum weights robustly ---\n",
    "def sum_weights(fname):\n",
    "    total = 0.0\n",
    "    with h5py.File(fname,'r') as f:\n",
    "        # visititems will call the visitor for every object in the file\n",
    "        def visitor(name, obj):\n",
    "            nonlocal total\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                # use [...] to read full dataset\n",
    "                total += obj[...].sum()\n",
    "        f.visititems(visitor)\n",
    "    return total\n",
    "\n",
    "# --- Compare weights ---\n",
    "s1 = sum_weights(w1_path)\n",
    "s2 = sum_weights(w2_path)\n",
    "print(\"Sum w1:\", s1)\n",
    "print(\"Sum w2:\", s2)\n",
    "print(\"Are weights equal?\", s1 == s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769a930",
   "metadata": {},
   "source": [
    "### How tf.random.set_seed interacts with operations (important conceptual detail)\n",
    "\n",
    "tf.random.set_seed(s) sets the graph-level seed. When combined with operation-level seeds (some ops accept a seed argument), TensorFlow composes them to produce deterministic sequences.  \n",
    "If you set the seed before creating layers/initializers, the layer initializers will be deterministic.  \n",
    "For eager runs, tf.random.set_seed affects subsequent TF RNG calls.  \n",
    "\n",
    "### tf.data specifics (practical patterns)\n",
    "\n",
    "### Shuffle with seed\n",
    "\n",
    "```python\n",
    "ds = ds.shuffle(buffer_size=10000, seed=seed, reshuffle_each_iteration=False)\n",
    "```\n",
    "\n",
    "- seed gives deterministic shuffle order.\n",
    "- reshuffle_each_iteration=True will reshuffle on each epoch — if you want same ordering across epochs, set to False.\n",
    "\n",
    "- When using map() with num_parallel_calls and parallel transformations, the order of parallel execution can cause nondeterminism unless deterministic=True is provided to map/prefetch (TF versions vary).\n",
    "Example:\n",
    "``` py\n",
    "ds = ds.map(fn, num_parallel_calls=tf.data.AUTOTUNE, deterministic=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048f229",
   "metadata": {},
   "source": [
    "#### Practical recommendations (what I’d do as your teacher)\n",
    "- Default: add a set_seeds(seed) helper in src/utils/seed.py and call it at the top of every script.\n",
    "- During development: run with deterministic_ops=True to make debugging easier (accept slower run time).\n",
    "- For final training: set deterministic_ops=False for speed, but keep seeds & save seeds in experiment.json.\n",
    "- For pipelines: prefer pre-tokenizing and storing token ids in TFRecord if tf.py_function slows your pipeline. Pre-tokenized TFRecords remove Python-based randomness in tokenization.\n",
    "- For CI: Add a small test like the repro_test.py above to validate that model weight fingerprints match across runs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
