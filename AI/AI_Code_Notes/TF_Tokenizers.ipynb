{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d87a09b",
   "metadata": {},
   "source": [
    "# Tokanization\n",
    "\n",
    "### Why Tokenization Matters\n",
    "Neural networks can‚Äôt read text directly ‚Äî they work with numbers (vectors, tensors).\n",
    "Tokenization is the bridge between human language and numerical representation.\n",
    "\n",
    "You‚Äôre learning to:\n",
    "- Convert raw text ‚Üí numeric sequences (input_ids)\n",
    "- Control how meaning and structure are preserved\n",
    "- Integrate this process efficiently in TensorFlow pipelines\n",
    "  \n",
    "### What Tokenization Actually Does\n",
    "\n",
    "At its simplest:\n",
    "```\n",
    "\"I love TensorFlow\" \n",
    "‚Üí [\"I\", \"love\", \"TensorFlow\"]\n",
    "‚Üí [101, 202, 303]   # token IDs\n",
    "```\n",
    "But the real world isn‚Äôt so clean.\n",
    "What about ‚ÄúTensorFlow‚Äôs‚Äù, emojis, different scripts, or rare words like ‚Äúelectroencephalogram‚Äù?\n",
    "So we need smarter ways to split text. Tokenization isn‚Äôt just about splitting ‚Äî it‚Äôs about encoding language structure efficiently.\n",
    "\n",
    "### Types of Tokenization (Conceptually)\n",
    "#### 1Ô∏è‚É£ Word-level\n",
    "Splits text by words.\n",
    "- ‚ÄúI love TensorFlow‚Äù ‚Üí [\"I\", \"love\", \"TensorFlow\"]\n",
    "- Each unique word gets an ID.\n",
    "- Pros: Simple, intuitive.\n",
    "- Cons: Fails on unknown words (OOV problem), needs a huge vocabulary.\n",
    "  \n",
    "### 2Ô∏è‚É£ Subword-level (most common)\n",
    "Splits words into smaller, reusable parts.\n",
    "- Example (BPE / SentencePiece / WordPiece):\n",
    "- \"TensorFlow\" ‚Üí [\"Tensor\", \"Flow\"]\n",
    "- \"Flowing\"    ‚Üí [\"Flow\", \"ing\"]\n",
    "- Pros:\n",
    "  - Handles new words (compositional)\n",
    "  - Keeps vocab manageable (e.g., 30k tokens)\n",
    "- Cons:\n",
    "  - Slightly more complex training & encoding\n",
    "  - Tokens no longer perfectly align with words\n",
    "\n",
    "\n",
    "\n",
    "### 3Ô∏è‚É£ Character-level\n",
    "Every character (letter, punctuation, emoji) is a token.\n",
    "- \"cat\" ‚Üí [\"c\", \"a\", \"t\"]\n",
    "- Pros: Never OOV.\n",
    "- Cons: Very long sequences ‚Üí slower training.\n",
    "  \n",
    "### 4Ô∏è‚É£ Byte-level / Unicode-aware\n",
    "- Each byte or Unicode character is a token.\n",
    "- Used by GPT-2 and newer tokenizers (robust to any input).\n",
    "\n",
    "### Vocabulary, IDs, and Special Tokens\n",
    "The vocabulary is the mapping between tokens and IDs:\n",
    "```\n",
    "{\"<PAD>\": 0, \"<UNK>\": 1, \"I\": 2, \"love\": 3, \"Tensor\": 4, \"Flow\": 5}\n",
    "```\n",
    "You‚Äôll also have special tokens:\n",
    "```\n",
    "- <PAD> ‚Äî padding shorter sequences\n",
    "- <UNK> ‚Äî unknown token\n",
    "- <CLS> / <SEP> ‚Äî sentence markers (used in Transformers)\n",
    "```\n",
    "\n",
    "### Padding and Truncation\n",
    "padding referes to adding special tokens to make all sequences in a batch the same length.\n",
    "\n",
    "EX: [\"I love TF\"]  --> [\"I love TF <PAD> <PAD>\"] \n",
    "- we do this because models often require inputs of uniform length.\n",
    "  \n",
    "truncation refers to cutting off sequences that are too long to fit a specified maximum length.\n",
    "EX: [\"I love TensorFlow and KerasNLP\"] --> [\"I love TensorFlow\"] # we do this to ensure inputs \n",
    "\n",
    "- don't exceed model limits.\n",
    "\n",
    "### From Tokens ‚Üí Numbers ‚Üí TensorFlow Tensors\n",
    "Once tokenized and converted to IDs, we can turn sequences into tensors:\n",
    "``` py\n",
    "tokens = [\"I\", \"love\", \"TensorFlow\"]\n",
    "ids = [2, 3, 4]\n",
    "tensor = tf.constant(ids)\n",
    "print(tensor)\n",
    "# tf.Tensor([2 3 4], shape=(3,), dtype=int32)\n",
    "```\n",
    "When batching, we need uniform shapes, so we pad sequences:\n",
    "# Example\n",
    "``` py\n",
    "batch = tf.constant([[2,3,4,0,0],\n",
    "                     [2,5,0,0,0]])\n",
    "```\n",
    "Now each example is [max_length].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8be93",
   "metadata": {},
   "source": [
    "## Tokenization Tools in TensorFlow Ecosystem\n",
    "You have three major paths depending on how deep or flexible you want to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958203e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras-nlp\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f874ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A. KerasNLP Tokenizers (newest & cleanest)\n",
    "KerasNLP provides tokenization as layers ‚Äî meaning they fit naturally inside models.\n",
    "\n",
    "why: It integrates with your tf.data pipeline or even directly in your model.\n",
    "\n",
    "Example:\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# creating your own tokenizer\n",
    "\"\"\" \n",
    "‚û°Ô∏è The BytePairTokenizer class doesnt come pre-trained.\n",
    "‚û°Ô∏è It expects that you already trained a BPE tokenizer on your data and saved the resulting files (vocab.txt and merges.txt).\n",
    "So this example demonstrates the pattern:\n",
    "You train a BPE tokenizer on your corpus.\n",
    "It outputs vocab.txt and merges.txt.\n",
    "You then load them locally into the KerasNLP tokenizer class for inference or model training.\n",
    "\n",
    "tokenizer = keras_nlp.tokenizers.BytePairTokenizer(\n",
    "    vocabulary=\"assets/vocab.txt\",\n",
    "    merges=\"assets/merges.txt\",\n",
    ")\n",
    "text = tf.constant([\"I love TensorFlow\"])\n",
    "tokens = tokenizer.tokenize(text) # Output: tf.RaggedTensor (variable-length token lists).\n",
    "\"\"\"\n",
    "\n",
    "# using pre-trained tokenizer (GPT2) recommended\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "text = tf.constant([\"I Love TensorFlow\"])\n",
    "tokens = tokenizer.tokenize(text) \n",
    "print(tokens)  # Output: [[40, 5896, 309, 22854, 37535]] list of token ids here each one maps to a token in the vocab these are defined by the pre-trained model but for ex 40 = \"I\", 5896 = \" Love\", 309 = \" Flow\" etc.\n",
    "# Decode tokens back to text\n",
    "decoded_text = tokenizer.detokenize(tokens)\n",
    "print(decoded_text)  # Output: tf.Tensor([b'I love TensorFlow'], shape=(1,), dtype=string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "B. tensorflow_text (lower-level, powerful)\n",
    "This library provides TensorFlow-native tokenization ops (runs on GPU/TPU).\n",
    "\n",
    "You can later map tokens to IDs using lookup tables.\n",
    "\n",
    "Example: \n",
    "\"\"\"\n",
    "import tensorflow_text as tf_text\n",
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "text = tf.constant([\"I love TensorFlow\"])\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)  # RaggedTensor\n",
    "\n",
    "\"\"\" \n",
    "Why you need a lookup table? \n",
    "\n",
    "Most neural models (e.g., embeddings, transformers) dont work directly with string tokens ‚Äî they expect integer IDs representing each word or subword.\n",
    "Thats where a lookup table comes in.\n",
    "\n",
    "# You define a mapping like:\n",
    "word_to_id = {\n",
    "    \"I\": 1,\n",
    "    \"love\": 2,\n",
    "    \"TensorFlow\": 3,\n",
    "    \"<UNK>\": 0\n",
    "}\n",
    "# Then you create a TensorFlow lookup layer:\n",
    "table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(list(word_to_id.keys())),\n",
    "        values=tf.constant(list(word_to_id.values()), dtype=tf.int64),\n",
    "    ),\n",
    "    num_oov_buckets=1\n",
    ")\n",
    "ids = table.lookup(tokens)\n",
    "print(ids) # would ouput: [[1, 2, 3]]\n",
    "\n",
    "or \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example vocab and IDs\n",
    "vocab = tf.constant([\"I\", \"love\", \"TensorFlow\", \"<UNK>\"])\n",
    "ids = tf.range(tf.size(vocab, out_type=tf.int64))\n",
    "\n",
    "# Create lookup table\n",
    "table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(vocab, ids),\n",
    "    num_oov_buckets=1  # for unknown tokens\n",
    ")\n",
    "\n",
    "# Apply it to your tokens\n",
    "token_ids = table.lookup(tokens)\n",
    "print(token_ids) # <tf.RaggedTensor [[0, 1, 2]]>\n",
    "\n",
    "# Now each token string becomes an integer ID that your model can process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 1045, 2293, 23435, 12314, 102]\n",
      "Decoded text: [CLS] i love tensorflow [SEP]\n",
      "tf.Tensor(101, shape=(), dtype=int32)\n",
      "tf.Tensor(101, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "üß© C. External Tokenizers (Hugging Face, SentencePiece)\n",
    "You can use the same tokenizers that big LLMs use, like SentencePiece or BERT tokenizers.\n",
    "Example (SentencePiece): \n",
    "\n",
    "‚ö†Ô∏è But these usually run in Python, not in the TensorFlow graph.\n",
    "So if used inside tf.data, they require tf.py_function wrappers (can slow things down a bit).\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained tokenizer (e.g., BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Encode text into token IDs\n",
    "encoded = tokenizer.encode(\"I love TensorFlow\", return_tensors=None)\n",
    "print(\"Token IDs:\", encoded)\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(\"Decoded text:\", decoded)\n",
    "\n",
    "# Token IDs: [101, 1045, 2293, 23435, 12314, 102]\n",
    "# Decoded text: [CLS] i love tensorflow [SEP]\n",
    "\n",
    "# These tokenizers usually run in Python, not inside the TensorFlow graph ‚Äî so if you want to use them in a tf.data pipeline, you‚Äôll need to wrap them in tf.py_function, like:\n",
    "import tensorflow as tf\n",
    "\n",
    "def encode_py(text):\n",
    "    return tokenizer.encode(text.numpy().decode(\"utf-8\"))\n",
    "\n",
    "def encode_tf(text):\n",
    "    ids = tf.py_function(func=encode_py, inp=[text], Tout=tf.int32)\n",
    "    return ids\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([\"I love TensorFlow\", \"This is fun!\"])\n",
    "ds = ds.map(encode_tf)\n",
    "for d in ds:\n",
    "    print(d)\n",
    "    \n",
    "# Output:\n",
    "# tf.Tensor([ 101  1045  2293  9899  8710  102], shape=(6,), dtype=int32)\n",
    "# tf.Tensor([ 101  2023  2003  4569  999  102], shape=(6,), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee0a1f",
   "metadata": {},
   "source": [
    "### Ragged vs Dense tensors (important TensorFlow detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f24de5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[2, 3, 4], [5, 6]]>\n"
     ]
    }
   ],
   "source": [
    "# Ragged vs Dense tensors (important TensorFlow detail)\n",
    "# Text sequences vary in length. TensorFlow uses RaggedTensors to handle that efficiently:\n",
    "tokens = tf.ragged.constant([\n",
    "    [2, 3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(tokens)\n",
    "# shape = [2, None]\n",
    "# Later, you can convert to dense and pad to fixed length:\n",
    "max_len = 4\n",
    "dense = tokens.to_tensor(default_value=0, shape=[None, max_len])\n",
    "# This step is crucial when you batch examples for model input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7c88d",
   "metadata": {},
   "source": [
    "### What‚Äôs actually happening under the hood\n",
    "Let‚Äôs trace what happens when you run:\n",
    "```tokens = tokenizer.tokenize(tf.constant([\"I love TensorFlow\"]))```\n",
    "1. String tensor goes into the tokenizer layer.\n",
    "2. The tokenizer splits strings ‚Üí outputs a RaggedTensor of subwords.\n",
    "3. (Optional) a vocab lookup converts subwords ‚Üí numeric IDs.\n",
    "4. Padding/truncation shapes the data into [batch, max_len].\n",
    "5. You feed that tensor into an embedding layer.\n",
    "\n",
    "You can even combine these in a preprocessing model:\n",
    "```py \n",
    "inputs = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "x = tokenizer.tokenize(inputs)\n",
    "x = tokenizer.detokenize(x)\n",
    "preproc = tf.keras.Model(inputs, x)\n",
    "```\n",
    "\n",
    "### Typical Tokenization Flow in TensorFlow\n",
    "Conceptually:\n",
    "\n",
    "Raw text (string)\n",
    "\n",
    "   ‚Üì\n",
    "\n",
    "Tokenization (split into subwords)\n",
    "\n",
    "   ‚Üì\n",
    "\n",
    "Vocab lookup (convert to IDs)\n",
    "\n",
    "   ‚Üì\n",
    "\n",
    "Padding/truncation\n",
    "\n",
    "   ‚Üì\n",
    "\n",
    "Batching\n",
    "\n",
    "   ‚Üì\n",
    "\n",
    "Feed to Embedding layer\n",
    "\n",
    "### Choosing the Right Approach (as an Engineer)\n",
    "\n",
    "| **Use Case**                 | **Recommended Approach**                                         |\n",
    "|------------------------------|------------------------------------------------------------------|\n",
    "| Simple demos, small tasks    | `tensorflow_text.WhitespaceTokenizer()`                          |\n",
    "| Custom tokenizer/vocab       | `tensorflow_text` + lookup table                                 |\n",
    "| Production / TPU / Performance | TF-native ops (`tensorflow_text` or `KerasNLP`)                |\n",
    "| LLM / pretrained models      | Hugging Face or SentencePiece                                    |\n",
    "| Reproducibility & sharing    | SentencePiece (saves model + vocab)                              |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebceb2f4",
   "metadata": {},
   "source": [
    "# Tokenizer selection & integration (advanced points)\n",
    "\n",
    "You‚Äôre converting raw text into integer token IDs that a model can process. Tokenization is a huge topic ‚Äî choices here affect model size, speed, ability to handle new words, and final performance. You're also deciding whether tokenization runs in Python (off-graph) or as TF ops (in-graph).\n",
    "\n",
    "### Tokenizer families (short overview)\n",
    "- Rule-based / simple tokenizers: whitespace split, regex. Quick and sometimes fine for simple tasks.\n",
    "- Subword tokenizers (most common): Byte-Pair Encoding (BPE), WordPiece, SentencePiece (Unigram). They balance vocabulary size and OOV handling.\n",
    "- Character-level: every char is a token ‚Äî robust but long sequences.\n",
    "- Neural / learned tokenizers: KerasNLP or tokenizers libraries with end-to-end integration.\n",
    "  \n",
    "### Common libs:\n",
    "- SentencePiece: trains a model (BPE or unigram); outputs ids; has fast C++ bindings.\n",
    "- HuggingFace tokenizers (Rust): very fast (but using it in TF graph requires py_function or pre-tokenizing).\n",
    "- tensorflow_text: TF ops for tokenization, normalizing, BERT-style WordPiece.\n",
    "- KerasNLP: high-level tokenizers that integrate as TF layers (if available in your TF/Keras version).\n",
    "\n",
    "### Two tokenization modes you should support (learning framing)\n",
    "- Python/tokenizer-mode: call SentencePiece or HuggingFace tokenizers from Python; simplest to implement for prototyping.\n",
    "- TF/tokenizer-mode: use TF-native tokenizers (tensorflow_text or KerasNLP layers) to tokenize inside the tf.data pipeline (faster, avoids py_function overhead).\n",
    "  \n",
    "### Core concepts to implement regardless of tokenizer\n",
    "- Special tokens: [PAD], [CLS], [SEP], [UNK] ‚Äî decide IDs and reserve them in the vocab.\n",
    "- Max length (max_len): choose a sequence length. Truncate longer sequences, pad shorter ones.\n",
    "- Attention masks: binary mask (1 for real tokens, 0 for padding) used by many models.\n",
    "- Padding side: left or right (commonly right-pad for sequence models).\n",
    "- Token type ids: for tasks like Next Sentence Prediction or pair inputs ‚Äî optional.\n",
    "  \n",
    "### Example pipeline choices\n",
    "- Raw text ‚Üí Python tokenizer ‚Üí pad/truncate ‚Üí batch.\n",
    "- Raw text ‚Üí tf.py_function wrapping a Python tokenizer inside a tf.data.map.\n",
    "- Raw text ‚Üí TF tokenizer (tensorflow_text or KerasNLP) inside tf.data.map (pure graph).\n",
    "- Pre-tokenized TFRecord ‚Üí parse input_ids and attention_mask and batch (fastest).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
