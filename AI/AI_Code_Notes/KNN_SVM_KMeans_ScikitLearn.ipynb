{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdebb98f",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Complete Guide to KNN, SVM, and K-Means in Scikit-Learn\n",
    "### For Aspiring Machine Learning Engineers (with generalizable concepts)\n",
    "\n",
    "This notebook provides a practical and conceptual introduction to **KNN**, **SVM**, and **K-Means** ‚Äî three cornerstone algorithms in machine learning.  \n",
    "You‚Äôll not only learn how to implement them in **Scikit-Learn**, but also how to think about them in a language-agnostic, engineering-focused way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.datasets import load_iris, make_classification, make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00649233",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Idea:** Predict a sample‚Äôs class by looking at its *k* closest data points.  \n",
    "No training phase ‚Äî it‚Äôs a **lazy learner**.\n",
    "\n",
    "**Pros:** Simple, interpretable, flexible.  \n",
    "**Cons:** Slow on large data, sensitive to scaling and irrelevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris() # iris dataset of flowers with 4 features and 3 classes\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler() # feature scaling p7 in notes\n",
    "X_train = scaler.fit_transform(X_train) # this fits the scaler on training data and transforms it into standardized form i.e mean=0, std=1\n",
    "X_test = scaler.transform(X_test) # this transforms the test data using the same scaler fitted on training data i.e mean=0, std=1 \n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5) # k-nearest neighbors classifier with k=5\n",
    "knn.fit(X_train, y_train) # fitting the model on training data\n",
    "y_pred = knn.predict(X_test) # predicting the labels for test data\n",
    "\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred)) # printing accuracy of the model\n",
    "print(classification_report(y_test, y_pred)) # detailed classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df4d7ba",
   "metadata": {},
   "source": [
    "### Ploting K values vs accuracy to find best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_values = range(1, 21)\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.plot(k_values, scores, marker='o')\n",
    "plt.title(\"KNN Accuracy vs k\")\n",
    "plt.xlabel(\"Number of Neighbors (k)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8411c",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öîÔ∏è Support Vector Machines (SVM)\n",
    "\n",
    "**Idea:** Find the optimal boundary (hyperplane) that maximizes the margin between classes.\n",
    "\n",
    "**Key features:**\n",
    "- Works for linear and nonlinear data (via **kernels**)\n",
    "- Strong theoretical foundation\n",
    "- Robust to high-dimensional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_linear = SVC(kernel='linear', C=1.0) # support vector machine with linear kernel function this function can be a hyperplane or even more complex functions some libraries dont require you to specify kernel as linear is default C=1.0 is standard regularization parameter \n",
    "svm_linear.fit(X_train, y_train) # fitting the model on training data\n",
    "y_pred = svm_linear.predict(X_test) # predicting the labels for test data\n",
    "\n",
    "print(\"SVM (Linear) Accuracy:\", accuracy_score(y_test, y_pred)) # printing accuracy of the model\n",
    "print(classification_report(y_test, y_pred)) # detailed classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Non-linear SVM visualization (RBF Kernel)\n",
    "X_blob, y_blob = make_classification(n_samples=300, n_features=2, n_informative=2,\n",
    "                                     n_redundant=0, n_clusters_per_class=1, class_sep=1.0, random_state=42)\n",
    "\n",
    "svm_rbf = SVC(kernel='rbf', gamma=0.7, C=1.0) # support vector machine with RBF kernel function gamma=0.7 controls the influence of a single training example C=1.0 is standard regularization parameter\n",
    "svm_rbf.fit(X_blob, y_blob) # fitting the model on training data\n",
    "\n",
    "# here we create a mesh grid to plot decision boundaries a mesh grid is a grid of points covering the feature space it allows us to visualize how the model classifies different regions i.e which class each point in the feature space belongs to\n",
    "x_min, x_max = X_blob[:, 0].min() - 1, X_blob[:, 0].max() + 1 \n",
    "y_min, y_max = X_blob[:, 1].min() - 1, X_blob[:, 1].max() + 1 \n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = svm_rbf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.coolwarm, edgecolors=\"k\")\n",
    "plt.title(\"SVM with RBF Kernel\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2450a",
   "metadata": {},
   "source": [
    "\n",
    "## üåÄ K-Means Clustering\n",
    "\n",
    "**Idea:** Partition data into *k* clusters by minimizing the distance to cluster centers (centroids).  \n",
    "Unsupervised algorithm ‚Äî no labels required.\n",
    "\n",
    "**Steps:**\n",
    "1. Initialize *k* centroids randomly.\n",
    "2. Assign points to nearest centroid.\n",
    "3. Recalculate centroids.\n",
    "4. Repeat until convergence.\n",
    "\n",
    "**Applications:** segmentation, image compression, anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_kmeans, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # some algos like DBSCAN do not require number of clusters to be specified\n",
    "kmeans.fit(X_kmeans)\n",
    "y_kmeans = kmeans.labels_\n",
    "\n",
    "plt.scatter(X_kmeans[:, 0], X_kmeans[:, 1], c=y_kmeans, cmap='viridis', s=30)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200)\n",
    "plt.title(\"K-Means Clustering Results\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elbow Method to choose k\n",
    "inertias = []\n",
    "k_values = range(1, 10)\n",
    "for k in k_values:\n",
    "    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    model.fit(X_kmeans)\n",
    "    inertias.append(model.inertia_)\n",
    "\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.title(\"Elbow Method for K Selection\")\n",
    "plt.xlabel(\"k (number of clusters)\")\n",
    "plt.ylabel(\"Inertia (Sum of Squared Distances)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213213f5",
   "metadata": {},
   "source": [
    "\n",
    "## üß≠ Summary\n",
    "\n",
    "| Algorithm | Type | Supervised? | Core Idea | Common Use Cases |\n",
    "|------------|------|-------------|------------|------------------|\n",
    "| KNN | Instance-based | ‚úÖ Yes | Predict by nearest neighbors | Classification, Regression |\n",
    "| SVM | Margin-based | ‚úÖ Yes | Find optimal separating boundary | Text, Image, Bio data |\n",
    "| K-Means | Centroid-based | ‚ùå No | Cluster points by proximity to centroids | Segmentation, Anomaly detection |\n",
    "\n",
    "---\n",
    "\n",
    "### Engineering Takeaways\n",
    "- **Scaling is critical** for KNN, SVM, and K-Means.\n",
    "- Learn the math behind distance and margin ‚Äî this generalizes to all frameworks.\n",
    "- Always visualize and tune hyperparameters for intuition.\n",
    "- Data preprocessing often matters more than model choice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
