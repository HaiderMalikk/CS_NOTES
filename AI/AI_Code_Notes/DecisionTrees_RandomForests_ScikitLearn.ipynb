{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b759ef9b",
   "metadata": {},
   "source": [
    "# Decision Trees & Random Forests — scikit-learn\n",
    "\n",
    "**A compact course notebook (classification + regression)**\n",
    "\n",
    "This notebook contains notes, code examples, visualizations and exercises covering Decision Trees and Random Forests in scikit-learn. It includes both **classification** and **regression** examples, hyperparameter tuning, pipelines, feature importances, evaluation, and common gotchas.\n",
    "\n",
    "---\n",
    "\n",
    "**Contents**\n",
    "\n",
    "1. Introduction & key concepts\n",
    "2. Setup & imports\n",
    "3. Decision Tree — Classification (Iris)\n",
    "4. Decision Tree — Regression (synthetic house price)\n",
    "5. Random Forest — Classification (Iris)\n",
    "6. Random Forest — Regression (synthetic house price)\n",
    "7. Hyperparameter tuning with GridSearchCV\n",
    "8. Practical tips, pitfalls, and exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b2012",
   "metadata": {},
   "source": [
    "## 1 — Quick conceptual notes (short and sweet)\n",
    "\n",
    "- **Decision tree**: a flowchart-like model that splits the feature space into regions using feature thresholds. Easy to interpret; can overfit if deep.\n",
    "- **Random forest**: an ensemble of many decision trees trained on bootstrap samples and feature subsamples; reduces variance and improves generalization.\n",
    "- **Advantages**: handles numeric and categorical (with encoding), captures non-linear relationships, provides feature importances, little preprocessing required for trees.\n",
    "- **Disadvantages**: single trees overfit easily, forests are less interpretable and larger; random forests can be slow and memory-heavy.\n",
    "\n",
    "Key hyperparameters:\n",
    "- `DecisionTree`: `max_depth`, `min_samples_split`, `min_samples_leaf`, `criterion` (`gini`/`entropy` for classification, `squared_error` for regression)\n",
    "- `RandomForest`: `n_estimators`, `max_features`, plus the tree hyperparams above; `oob_score=True` gives out-of-bag estimate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf7e3b",
   "metadata": {},
   "source": [
    "## 2 — Setup & imports\n",
    "\n",
    "Run the cell below to import required libraries and set a reproducible random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd05428",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade pandas\n",
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade matplotlib\n",
    "%pip install --upgrade seaborn\n",
    "%pip install --upgrade jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    r2_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# this is for reproducibility and consistent plot sizes first we make a seed with numpy seed 0 will always generate the same random numbers\n",
    "np.random.seed(0)\n",
    "plt.rcParams['figure.figsize'] = (8,6) # set default figure size for matplotlib\n",
    "\n",
    "print('scikit-learn version:', end=' ')\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c547a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part I — Classification with Decision Trees (Iris dataset)\n",
    "\n",
    "We will:\n",
    "- Load Iris dataset\n",
    "- Train a DecisionTreeClassifier\n",
    "- Visualize the tree\n",
    "- Evaluate performance\n",
    "- Inspect feature importances\n",
    "\n",
    "The goal here is that we have fetures for a plant and the correct lable for that plant we will train a model on these fetures and there correct lables to evaluate new unseen plants using tehre fetures \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris iris is a classic dataset for classification tasks it includes 3 classes of iris plants with 4 features each\n",
    "iris = load_iris()\n",
    "X = iris.data # features\n",
    "y = iris.target # target labels\n",
    "# we only pick 2 features for easy visualization\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Quick EDA (Exploratory Data Analysis) this is just to understand the dataset better\n",
    "print('X shape:', X.shape)\n",
    "print('Classes:', class_names)\n",
    "\n",
    "# Make a DataFrame for convenience\n",
    "df_iris = pd.DataFrame(X, columns=feature_names)\n",
    "df_iris['target'] = y\n",
    "df_iris['target_name'] = df_iris['target'].map(lambda v: class_names[v])\n",
    "df_iris.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split 25% of data for testing rest for training stratify to maintain class distribution which means each class will have same proportion in train and test as in original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) # random_state for reproducibility and stratify to maintain class distribution (class dist means how many samples of each class we have in train and test)\n",
    "\n",
    "# Fit a Decision Tree\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix a way to evaluate model performance (confusion matrix shows how many samples were correctly classified and how many were misclassified for each class)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "\n",
    "# Plot tree (simplified) we limit depth to 3 for better visualization after that depth its just more subtrees\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(dt_clf, feature_names=feature_names, class_names=class_names, filled=True, rounded=True, max_depth=3)\n",
    "plt.title('Decision Tree (truncated to depth 3)')\n",
    "plt.show()\n",
    "\n",
    "# ex matrix we get \n",
    "\"\"\" \n",
    "Confusion matrix:\n",
    " [[12  0  0]\n",
    " [ 0 12  1]\n",
    " [ 0  3 10]]\n",
    " \n",
    " meaning: \n",
    "- 12 samples of class 0 were correctly classified as class 0\n",
    "- 12 samples of class 1 were correctly classified as class 1, 1 sample of class 1 was misclassified as class 2\n",
    "- 10 samples of class 2 were correctly classified as class 2, 3 samples of class 2 were misclassified as class 1\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da725525",
   "metadata": {},
   "source": [
    "### Inspect feature importances and overfitting\n",
    "\n",
    "Decision trees often overfit when unconstrained. Let's look at feature importances and compare training vs test accuracy for different `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "depths = range(1, 11)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_scores.append(clf.score(X_train, y_train))\n",
    "    test_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "plt.plot(depths, train_scores, label='train')\n",
    "plt.plot(depths, test_scores, label='test')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Decision Tree: train vs test accuracy by max_depth')\n",
    "plt.legend();\n",
    "\n",
    "# Feature importances\n",
    "fi = pd.Series(dt_clf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print('Feature importances:\\n', fi)\n",
    "\n",
    "# Decision trees often overfit when unconstrained. Let's look at feature importances and compare training vs test accuracy for different `max_depth` values.\n",
    "# why do this? To understand how the complexity of the decision tree affects its performance and to identify which features are most important in making predictions.\n",
    "# Feature importances are a way to measure the relative importance of each feature in a decision tree. The higher the feature importance, the more important the feature is in making predictions.\n",
    "# By looking at feature importances, we can identify which features are most important in making predictions and use them to train a more accurate decision tree model.\n",
    "# what we mean by Decision trees often overfit when unconstrained is that when we allow the decision tree to grow without any restrictions (like max depth, min samples per leaf, etc.), it can create a model that is too complex and fits the training data very closely. This can lead to poor generalization to new, unseen data because the model has essentially memorized the training data rather than learning the underlying patterns.\n",
    "# in a nutshell feature importance mesures the importance by looking at how much each feature contributes to reducing the impurity (or uncertainty) in the data at each split in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ce748",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part II — Regression with Decision Trees (Synthetic House Price)\n",
    "\n",
    "We will:\n",
    "- Create a synthetic dataset representing House Price vs SquareFeet and extra noise/features\n",
    "- Fit a DecisionTreeRegressor\n",
    "- Visualize predictions and residuals\n",
    "- Compare to linear baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ab677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset (house price-like)\n",
    "from sklearn.datasets import make_regression\n",
    "X_reg, y_reg = make_regression(n_samples=300, n_features=3, n_informative=2, noise=30.0, random_state=42)\n",
    "# Let's make the first feature represent 'square feet' scaled reasonably\n",
    "X_reg[:,0] = np.clip((X_reg[:,0] * 200) + 1000, 300, None)  # roughly 300-3000 sqft\n",
    "\n",
    "# Put in a DataFrame\n",
    "df_house = pd.DataFrame(X_reg, columns=['SquareFeet','Feature2','Feature3'])\n",
    "df_house['Price'] = y_reg\n",
    "\n",
    "df_house.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a413bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "Xr = df_house[['SquareFeet','Feature2','Feature3']].values\n",
    "yr = df_house['Price'].values\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "# Predictions & evaluation\n",
    "yr_pred = dt_reg.predict(Xr_test)\n",
    "print('R2:', r2_score(yr_test, yr_pred))\n",
    "print('MAE:', mean_absolute_error(yr_test, yr_pred))\n",
    "\n",
    "# Plot true vs predicted\n",
    "plt.scatter(yr_test, yr_pred, alpha=0.6)\n",
    "plt.plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()], 'k--')\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Decision Tree Regressor: True vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Visualize tree (truncated depth)\n",
    "plt.figure(figsize=(14,8))\n",
    "plot_tree(dt_reg, feature_names=['SquareFeet','Feature2','Feature3'], filled=True, rounded=True, max_depth=3)\n",
    "plt.title('Decision Tree Regressor (truncated to depth 3)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bc628",
   "metadata": {},
   "source": [
    "### Compare to a linear regression baseline\n",
    "A linear model is a useful baseline to compare against tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin = LinearRegression()\n",
    "lin.fit(Xr_train, yr_train)\n",
    "\n",
    "y_lin_pred = lin.predict(Xr_test)\n",
    "print('Linear R2:', r2_score(yr_test, y_lin_pred))\n",
    "print('Linear MAE:', mean_absolute_error(yr_test, y_lin_pred))\n",
    "\n",
    "# Plot comparison\n",
    "plt.scatter(yr_test, yr_pred, alpha=0.6, label='DecisionTree')\n",
    "plt.scatter(yr_test, y_lin_pred, alpha=0.6, label='Linear')\n",
    "plt.plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()], 'k--')\n",
    "plt.legend()\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Model comparison: Decision Tree vs Linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af61f3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part III — Random Forests (Classification & Regression)\n",
    "\n",
    "Random forests reduce variance by averaging many decorrelated trees. We'll train and evaluate both classifier and regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad590a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification (Iris)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "yp_rf = rf_clf.predict(X_test)\n",
    "print('RF Accuracy (Iris):', accuracy_score(y_test, yp_rf))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, yp_rf, target_names=class_names))\n",
    "\n",
    "# Feature importances\n",
    "fi_rf = pd.Series(rf_clf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "print('\\nRandom Forest feature importances:\\n', fi_rf)\n",
    "\n",
    "# OOB score example (train a new RF with oob)\n",
    "rf_oob = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42, n_jobs=-1)\n",
    "rf_oob.fit(X_train, y_train)\n",
    "print('\\nOOB score:', rf_oob.oob_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc368c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression (house data)\n",
    "rf_reg = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(Xr_train, yr_train)\n",
    "yrf_pred = rf_reg.predict(Xr_test)\n",
    "print('RandomForest R2:', r2_score(yr_test, yrf_pred))\n",
    "print('RandomForest MAE:', mean_absolute_error(yr_test, yrf_pred))\n",
    "\n",
    "# Feature importances\n",
    "fi_rf_reg = pd.Series(rf_reg.feature_importances_, index=['SquareFeet','Feature2','Feature3']).sort_values(ascending=False)\n",
    "print('\\nRF regressor feature importances:\\n', fi_rf_reg)\n",
    "\n",
    "# Compare predictions scatter\n",
    "plt.scatter(yr_test, yrf_pred, alpha=0.6)\n",
    "plt.plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()], 'k--')\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('RF Predicted Price')\n",
    "plt.title('Random Forest Regressor: True vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae045be3",
   "metadata": {},
   "source": [
    "### Learning curves (to diagnose bias vs variance)\n",
    "Let's plot learning curves for the Random Forest regressor to see how train/test error behaves with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d613ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve example (Iris with RF)\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42), X, y, cv=5,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='C0', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='C1', label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388b64d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part IV — Hyperparameter tuning (GridSearchCV) — classification example\n",
    "\n",
    "We'll tune `max_depth` and `min_samples_leaf` for a Decision Tree classifier using grid search with cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best params:', gs.best_params_)\n",
    "print('Best CV score:', gs.best_score_)\n",
    "\n",
    "best_dt = gs.best_estimator_\n",
    "print('\\nTest accuracy of best DT:', best_dt.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f2359",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning notes\n",
    "- For Random Forests, common params to tune: `n_estimators`, `max_features`, `max_depth`, `min_samples_leaf`, `bootstrap`.\n",
    "- Use `n_jobs=-1` to parallelize (if CPU/memory allows).\n",
    "- Consider `RandomizedSearchCV` for large parameter spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f8d77",
   "metadata": {},
   "source": [
    "## Part V — Practical tips, pitfalls, and exercises\n",
    "\n",
    "**Tips & Pitfalls**\n",
    "\n",
    "- Trees do not need feature scaling. But when combining with other models in pipelines, be mindful.\n",
    "- Trees easily overfit—use `max_depth`, `min_samples_leaf`, `min_samples_split` to regularize.\n",
    "- Random forests reduce variance but may hide bias; if all trees are biased, averaging won't fix it.\n",
    "- `oob_score=True` is a convenient estimate of generalization for forests using bootstrap samples.\n",
    "\n",
    "**Exercises**\n",
    "\n",
    "1. Try training a `DecisionTreeClassifier(max_depth=3)` and visualize the tree fully (not truncated).\n",
    "2. For the regression dataset, remove `Feature2` and `Feature3` and see how well the tree and forest do using just `SquareFeet`.\n",
    "3. Use `RandomizedSearchCV` on the RandomForestRegressor to tune `n_estimators`, `max_features`, `max_depth`, and compare results vs default.\n",
    "4. Implement a simple feature importance plot function and use it to compare tree vs forest importance rankings.\n",
    "\n",
    "---\n",
    "\n",
    "_End of notebook — you're ready to experiment!_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
