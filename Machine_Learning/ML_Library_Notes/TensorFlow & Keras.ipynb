{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea6234c",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "TensorFlow is an open-source platform developed by Google for machine learning and deep learning tasks. It allows developers to build and train models using both high-level APIs (like Keras) and low-level operations.\n",
    "\n",
    "üß† Why TensorFlow for AI Research?\n",
    "\n",
    "Scalable across CPUs, GPUs, and TPUs\n",
    "\n",
    "Integrated Keras API for quick prototyping\n",
    "\n",
    "Production-ready (used by Google internally)\n",
    "\n",
    "Tools like TensorBoard, TensorFlow Lite, TensorFlow.js, and more for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2675279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-datasets tensorboard\n",
    "%pip install keras\n",
    "%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0da16",
   "metadata": {},
   "source": [
    "Verify Metal for MACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow detects the GPU\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check if TensorFlow is running on Metal\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Force TensorFlow to run on GPU if available\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf import to install use: pip install tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\" \n",
    "Tersors\n",
    "a tensor is a multi-dimensional array that can be used for various computations. \n",
    "conastants are immutable tensors, meaning their values cannot be changed after they are created.\n",
    "\"\"\"\n",
    "x = tf.constant([[1,2], [3,4]]) # you must wrap the values in a list the outer brackets signify the tensor, this is a 2D tensor\n",
    "print(x) # prints the tensor + shape = (2,2) i.e 2 rows and 2 columns and dtype = int32 i.e 32 bit integer\n",
    "\n",
    "\"\"\" \n",
    "Computational Graph\n",
    "A computational graph is a way to represent the operations and data flow in a TensorFlow program.\n",
    "for example, if you want to add two tensors, you can create a computational graph that represents the addition operation.\n",
    "EX cont: say you have two tensors A and B, and you want to add them together. A= [[1,2],[3,4]] and B=[[5,6],[7,8]]\n",
    "the computational graph to add them will have two nodes, one for each tensor, and an edge that represents the addition operation.\n",
    "The graph will look like this:\n",
    "A ----> + ----> C\n",
    "       ^\n",
    "       |\n",
    "       B\n",
    "where A and B are the input tensors, + is the addition operation, and C is the resulting tensor after the addition operation.\n",
    "\n",
    "- real life graph \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a computational graph\n",
    "x = tf.constant([[3, 4]])\n",
    "y = tf.constant([[5, 6]])\n",
    "print(x+y) # prints the result of the addition operation = [[ 8 10]] as 3+5 = 8 and 4+6 = 10\n",
    "# unlike TF1, TF2 does not require you to create a session to run the graph.\n",
    "# a session is a way to execute the graph and get the result.\n",
    "# in TF2, the graph is executed immediately when you run the operation.\n",
    "# tf functions are used to create a graph and run it in a session. tf functions are used to create a graph and run it in a session. it compiles functions into a static graph and runs them in a session. (better performance)\n",
    "# EX: lets use the function decorator to create a graph and run it in a session. \n",
    "@tf.function\n",
    "def add_tensors(a, b):\n",
    "    return a * b # multipling as also a graph operation (computational graph)\n",
    "print(add_tensors(x, y)) # prints the result of the addition operation = [[ 15 24]] as 3*5 = 15 and 4*6 = 24\n",
    "\n",
    "\"\"\" \n",
    "Building a Model\n",
    "a model is a collection of layers that are connected together to form a neural network.\n",
    "each layer is a function that takes an input tensor and produces an output tensor.\n",
    "the input tensor can be for ex an image's grayscale value, and the output tensor will be some values that represent the image's features. \n",
    "this is then passed to the next layer, and so on, until the final output tensor is produced. which is the final prediction of the model.\n",
    "\n",
    "- We use keras to build a model, keras is a high-level API for building and training deep learning models.\n",
    "- from keras we can get the Sequential model, which is a linear stack of layers. the Sequential model is a simple way to build a model by adding layers one by one.\n",
    "- This means that the output of one layer is the input to the next layer. sequential models are easy to use and understand, and they are a good choice for most applications.\n",
    "- the end result is a model that can be trained on data and used to make predictions.\n",
    "\n",
    "- We also use Dense layers, which are fully connected layers. meaning every neuron in the layer is connected to every neuron in the previous layer.\n",
    "- this is not how all neural networks work, but it is a common way to build a model.\n",
    "- The Dense layer takes an input tensor and produces an output tensor by applying a linear transformation to the input tensor.\n",
    "\n",
    "- So to sum, Sequential model is a linear stack of layers, and Dense layers are fully connected layers we combine then to build a Stack of layers that are fully connected to each other.\n",
    "\"\"\"\n",
    "# to install keras, run the following command in the terminal: pip install keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\"\"\" \n",
    "Here we first define the sequential model, then we add a Dense layer which creates a fully connected layer\n",
    "the input layer has 784 neurons and the output layer has 10 neurons. this can be a image which is 28x28 pixels = 784 pixels\n",
    "and outputs a 10 class prediction. 0-9 for a digit classification problem.\n",
    "\n",
    "relu is a activation function that is used to introduce non-linearity in the model.\n",
    "we cannot use a linear activation function because the model will not be able to learn complex patterns in the data.\n",
    "Foe EX the XOR problem: XOR gate is as follows: \n",
    "XOR gate is a gate that outputs 1 if both inputs are different and 0 if both inputs are the same. in a table: \n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 1\n",
    "1 1 0\n",
    "on a plot they look like:\n",
    "1| 1   0\n",
    " | \n",
    "0| 0   1\n",
    " |______\n",
    "   0   1\n",
    "There is no way the separate the two classes with a straight line. i.e you cannot set apart true and flase values with a linear function.\n",
    "But using a function like signmoid or relu, we can separate the two classes with a curve beacuse there not a straight line they can bend the curve to separate the two classes.\n",
    "\n",
    "Softmax is a activation function that is used to convert the output of the model into a probability distribution.\n",
    "For EXample, if the model outputs a tensor with 10 values, softmax will convert it into a tensor with 10 values that sum to 1.\n",
    "meaning no matter how many outputs we have each output will be a probability and all the probabilities will sum to 1.\n",
    "Each nueron in the output layer will represent a class, and the value of the neuron will represent the probability of that class. this probability is 0-1\n",
    "what is a class: a class is a group of objects that are similar to each other in some way. for example in the image prediction problem\n",
    "we have 10 classes, one for each digit from 0 to 9. the model will output a tensor with 10 values, one for each class.\n",
    "then the softmax function will convert the output into a probability distribution so for example if the model outputs [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "this means the digits 0-9 has the probabilities of 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 respectively. where the model will choose 8 as the prediction.\n",
    "\n",
    "ReLU (Rectified Linear Unit) sets all negative values to zero and keeps positive values unchanged.\n",
    "In a simple NN, it adds non-linearity, helping the model learn complex patterns instead of just straight lines.\n",
    "Mathematically:\n",
    "ReLU(x)=max‚Å°(0,x)\n",
    "ReLU(x)=max(0,x)\n",
    "‚úÖ Keeps positives ‚Üí same\n",
    "‚úÖ Turns negatives ‚Üí 0\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\"\"\" \n",
    "compile and train\n",
    "- in the complile step, we define the loss function, optimizer, and metrics to be used during training.\n",
    "- in the training step, we fit the model to the data and train it for a number of epochs.\n",
    "\n",
    "in our ex wedifine some data first repersenting the 784 pixels of a image and the labels are the classes of the digits.\n",
    "so 784 random inputs and a lable for each input (both are random and generated a 1000 times)\n",
    "\n",
    "then we compile our model using the adam optimizer and sparse_categorical_crossentropy loss function.\n",
    "- the loss function is used to measure how well the model is performing (0-infinite). for Example, if the model is predicting the wrong class, the loss function will return a high value.\n",
    "  and vice versa adam is a popular optimizer that is used for most problems. sparse_categorical_crossentropy is used for multi-class classification problems like out digit classification problem.\n",
    "  optimizer is used to update the weights of the model during training. metrics (accuracy) (0-1 or 0-100%) are used to measure the performance of the model during training. we use accuracy as our metric measuring how well the model is performing.\n",
    "  loss vs metrics: loss is used to measure how well the model is performing, while metrics are used to measure the performance of the model during training.\n",
    "\n",
    "Then we fit the model to the data and train it for 10 epochs.\n",
    "- we define our data and labels using numpy and do 10 epochs of training.\n",
    "  a epoch is one pass through the entire dataset for ex if we have 1000 samples of images, then 1 epoch is 1000 samples. \n",
    "\"\"\"\n",
    "# def some data\n",
    "import numpy as np\n",
    "train_data = np.random.rand(1000, 784) # 1000 samples of 784 features for a image, this generates 784 random values each 784 values are in one row the other 999 rows are the other samples\n",
    "# the data looks like \n",
    "# [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 ..... 784 values],\n",
    "#   [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 .....784 values],\n",
    "#   ...... 1000 rows]\n",
    "# this denotes 100 samples of 784 features for a image\n",
    "train_labels = np.random.randint(10, size=(1000,)) # 1000 samples of labels from 0 to 9 i.e the classes each lable in in a row each 784values row is paired with a label row i.e just one valie 0-9\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=10)\n",
    "# Results : from the last epoch we get the loss and accuracy of the model: accuracy: 0.2501 - loss: 2.1262 our accuracy is bad but the data is random so we cannot expect a good accuracy.\n",
    "\n",
    "\"\"\" \n",
    "Model Subclassing\n",
    "- Model subclassing is a way to create custom models in TensorFlow by subclassing the Model class\n",
    "- this means we define our own model by creating a class that inherits from the Model class.\n",
    "- so we can have as many layers as we want and define our own forward pass which is the way the data flows through the model.\n",
    "- this is good for custom models that are not linear stacks of layers.\n",
    "\n",
    "in our Example the model has only two layers, but we can add as many layers as we want.\n",
    "the frist layer is 64 neurons and the second layer is 10 neurons. we pass the inputs through the first layer and then pass the output of the first layer through the second layer.\n",
    "and we return the output of the second layer as the final output of the model.\n",
    "\"\"\"\n",
    "class MyModel(tf.keras.Model): # inheriting from the Model class\n",
    "    def __init__(self): # constructor\n",
    "        super(MyModel, self).__init__() # call the constructor of the parent class so initialize the model\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu') # first layer 64 neurons + relu activation\n",
    "        self.out = tf.keras.layers.Dense(10) # output layer 10 neurons\n",
    "\n",
    "    def call(self, inputs): # forward pass\n",
    "        x = self.dense1(inputs) # pass the inputs through the first layer\n",
    "        return self.out(x) # pass the output of the first layer through the output layer\n",
    "\n",
    "\"\"\" \n",
    "Model evaluation and prediction\n",
    "- after training the model, we can evaluate the model on the test data to see how well it performs.\n",
    "\n",
    "- the evaluate method takes the test data and labels as input and returns the loss and accuracy of the model on the test data.\n",
    "- the predict method takes the test data as input and returns the predicted labels of the model on the test data.\n",
    "\"\"\"\n",
    "model.evaluate(train_data, train_labels)\n",
    "new_data = np.random.rand(10, 784) # 10 samples of 784 features for a image this will be the new unseen data\n",
    "model.predict(new_data) \n",
    "# output will be a 10x10 matrix of probabilities for each class\n",
    "#EX run\n",
    "\"\"\" \n",
    "array([[0.07370737, 0.06734557, 0.09007742, 0.09084072, 0.15472364,\n",
    "        0.10088342, 0.15501715, 0.0846708 , 0.06713989, 0.11559402],\n",
    "       [0.13559042, 0.08671413, 0.06254287, 0.10968324, 0.10897283,\n",
    "        0.06311018, 0.140859  , 0.08376517, 0.05650004, 0.1522621 ],\n",
    "       [0.06658942, 0.06215975, 0.12574631, 0.08794341, 0.24458086,\n",
    "        0.15032104, 0.03119767, 0.06260405, 0.07711761, 0.09173983],\n",
    "       [0.18518034, 0.03801233, 0.10225594, 0.05082671, 0.14458576,\n",
    "        0.08729083, 0.1261759 , 0.06465442, 0.07568176, 0.12533602],\n",
    "       [0.25580242, 0.0704459 , 0.09154338, 0.12910502, 0.15939018,\n",
    "        0.06951969, 0.07261899, 0.05154628, 0.06342862, 0.03659946],\n",
    "       [0.16323559, 0.06115254, 0.11572555, 0.0481673 , 0.2106075 ,\n",
    "        0.08192023, 0.08052816, 0.06937485, 0.05051486, 0.11877337],\n",
    "       [0.08927758, 0.09356026, 0.07743935, 0.04602603, 0.05058305,\n",
    "        0.2109955 , 0.1288731 , 0.13315801, 0.12589532, 0.04419181],\n",
    "       [0.12444694, 0.21631704, 0.09856055, 0.12358341, 0.12699795,\n",
    "        0.04524506, 0.07598381, 0.09751168, 0.03933394, 0.0520197 ],\n",
    "       [0.14829713, 0.12787852, 0.04917813, 0.08518186, 0.20955724,\n",
    "        0.06541336, 0.12108779, 0.07558911, 0.05305878, 0.06475811],\n",
    "       [0.09848133, 0.1336912 , 0.06284648, 0.1111226 , 0.17866668,\n",
    "        0.10355557, 0.10615086, 0.0537717 , 0.06240985, 0.08930375]],\n",
    "      dtype=float32)\n",
    "      \n",
    "This has 10 rows and 10 columns, each row is a sample and each column is a class.\n",
    "for Example the first row is the prediction for the first sample, and the first column is the probability of class 0.\n",
    "in simple tearms for the number 0 the model is 7% sure it is a 0, 6% sure it is a 1, 9% sure it is a 2, and so on.\n",
    "the second row then dose this again fro numbers 0-9 then dose it again for the third row and so on.\n",
    "since we had 10 samples, we have 10 rows and 10 columns for image 0-9 10 classes.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "Handling Missing Data, a few ways to handle missing data:\n",
    "- Drop the rows with missing data\n",
    "- Fill the missing data with a value\n",
    "- Fill the missing data with the mean, median, or mode of the column\n",
    "- Fill the missing data with the previous or next value in the column (forward or backward fill)\n",
    "- Fill the missing data with a value from another column\n",
    "- Fill the missing data with a value from a different dataset\n",
    "- Fill the missing data with a value from the sorrounding data (interpolation)\n",
    "\"\"\"\n",
    "x = tf.constant([1.0, float('nan'), 2.0, float('nan')]) # creating a tensor with missing data\n",
    "x_clean = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x) # replace the missing data with 0\n",
    "print(x_clean)  # [1.0, 0.0, 2.0, 0.0] \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Tensor Boards\n",
    "- TensorBoard is a tool for visualizing the training process of a model.\n",
    "\n",
    "The ex below will save the running data of the model to a log directory, and then we can use TensorBoard to visualize the data.\n",
    "\"\"\"\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = \"logs/fit/\" # directory to save the logs\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir) # create a TensorBoard callback meaning it will save the logs to the directory\n",
    "model.fit(train_data, train_labels, epochs=10, callbacks=[tensorboard_callback]) # using the same data and labels as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visulize the tf logs using tensorboard \n",
    "# # option 1) tensorboard --logdir=logs/fit (will runn on localhost:6006)\n",
    "# option 2) within jupyter notebook\n",
    "%load_ext tensorboard # load the tensorboard extension\n",
    "%tensorboard --logdir logs/fit # run tensorboard in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1. Dummy data for an image classification problem\n",
    "train_data = np.random.rand(1000, 784)  # 1000 samples, 784 features each i.e a 2D arr ay with 1000 rows and 784 columns where each row is a sample and each column is a feature\n",
    "train_labels = np.random.randint(10, size=(1000,))  # 1000 labels (classes 0-9) each label is an integer between 0 and 9\n",
    "batch_size = 32 # batch size this is how many examples we process before updating the model weights\n",
    "\n",
    "# Create a TensorFlow dataset and batch it, creates a dataset with tarin dat and lables as pairs comes in batches of 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size)\n",
    "\n",
    "# 2. Define the model\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__() # initialize the parent class gives us access to the Model class methods and properties\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')  # hidden layer (64 neurons) with relu activation\n",
    "        self.out = tf.keras.layers.Dense(10)  # output layer (10 classes)\n",
    "\n",
    "    # this defines the forward pass of the model whenever we do model(inputs) this is called automatically\n",
    "    def call(self, inputs): # inputs is the input to the model\n",
    "        x = self.dense1(inputs) # pass the inputs through the first layer this will pass it through the hidden layer (dense layer)\n",
    "        return self.out(x) # pass the output of the first layer through the output layer # this will take the dense layer output and pass it through the output layer to get the final output\n",
    "\n",
    "model = MyModel() # create an instance of the model\n",
    "\n",
    "# 3. Define a custom loss function\n",
    "# Warning: your custom loss is MSE (good for regression, not classification!)\n",
    "# Normally for classification you use SparseCategoricalCrossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Mean Squared Error between true labels and predicted labels MSE calculates the mean squared error between the true labels and the predicted labels\n",
    "    # we take the predicted labels (given as 10 logits this for ex in a digit classification prob can be [0.1, 0.2, 0.3, ..., 1.0] for each class 0-9 with the index corresponding to the class dgits 0-9) \n",
    "    # and the true labels (given as integers 0-9 corresponding to the class digits) we want to subtract this from each other but ones a list and the otehr is a number \n",
    "    # so we use one_hot encoding to convert the true labels into a list of 10 values where the index corresponding to the class digit is 1 and the rest are 0 (for ex if the true lable is 3 then the one hot encoding will be [0,0,0,1,0,0,0,0,0,0])\n",
    "    # now we can subract the two and get the squared error and take the mean of all the errors to get the final loss value\n",
    "    # at one time we will receive a batch of y_true and y_pred so we calculate the loss for the entire batch so thats a 2d array with shape (batch_size, 10) then we take the mean of all the errors using the reduce_mean function\n",
    "    return tf.reduce_mean(tf.square(y_pred - tf.one_hot(y_true, depth=10))) # depth is important and corresponds to the number of classes\n",
    "\n",
    "# 4. Define optimizer\n",
    "# this is used to update the weights of the model during training Adam is a popular optimizer that is used for most problems. it combines the benefits of two other extensions of stochastic gradient descent.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 5. Custom training loop\n",
    "epochs = 10 # a epoch is one pass through the entire dataset\n",
    "for epoch in range(epochs): # for each epoch (in this loop we will see the entire dataset once)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset): # for each step (in this loop we will process a batch of data) here from the dataset we get a batch of data and labels \n",
    "        with tf.GradientTape() as tape: # GradientTape is used to record the operations for automatic differentiation in basic terms it helps us calculate the gradients of the loss with respect to the model weights by recording the operations that are performed on the tensors in the model\n",
    "            # the reson we set training=True is to ensure that any layers that behave differently during training and inference (like dropout or batch normalization) are in training mode, basically we are telling the model we are training it so it should behave accordingly like for ex its allowed to drop neurons in dropout layers\n",
    "            logits = model(x_batch, training=True) # forward pass (get the model predictions for the batch of data) this will give us a list for each sample in the batch with 10 logits (one for each class 0-9) we will have 32 lists if the batch size is 32 with shape (32, 10)\n",
    "            loss = custom_loss(y_batch, logits)  # we calculate the loss using our custom loss function (compare the predicted labels with the true labels for the batch) this will give us the loss value for the entire batch (a single number we will use this to update all the model weights)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables) # compute gradients (get the gradients of the loss with respect to the model weights) this will give us a list of gradients for each weight in the model basically how much we need to change each weight to reduce the loss so theres 1 gradient for each weight in the model\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # update weights (apply the gradients to the model weights) this will update the weights of the model using the gradients we calculated and the optimizer we defined, here we pass a zip object that pairs each gradient with its corresponding weight in the model\n",
    "        if step % 10 == 0: # print loss every 10 steps\n",
    "            print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
    "\n",
    "print(\"Training complete ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a8b353",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff378ac7",
   "metadata": {},
   "source": [
    "# üß† Introduction to Keras with TensorFlow\n",
    "\n",
    "Welcome to your **Beginner-Friendly Introduction to Keras**, the high-level API built on top of **TensorFlow**.\n",
    "\n",
    "Keras is designed to make building, training, and deploying deep learning models easy and intuitive.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò What You'll Learn\n",
    "1. What Keras is and how it relates to TensorFlow  \n",
    "2. How to build models using the **Sequential API**  \n",
    "3. How to train and evaluate models  \n",
    "4. How to use **callbacks** to improve training  \n",
    "5. How to save and load models  \n",
    "6. Two practical examples:\n",
    "   - A simple **regression model**\n",
    "   - A **classification model** using MNIST digits\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc92c6",
   "metadata": {},
   "source": [
    "## üß© 1. Building a Simple Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 2 ways of making layers:\n",
    "# method 1 more complicated:\n",
    "inputs = keras.Input(shape=(784,))\n",
    "x = keras.layers.Dense(8, activation='relu')(inputs) # 8 neurons + relu activation and take in inputs\n",
    "outputs = keras.layers.Dense(1)(x) # output layer with 1 neuron\n",
    "\n",
    "# method 2:\n",
    "# Create a Sequential model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# NOTE: Using strings vs classes for optimizers, loss functions, and activations etc \n",
    "\"\"\" \n",
    "when we specify the optimizer, loss function, and metrics etc in the compiler method we are passing in the optimizer, loss etc as strings which are then mapped to their corresponding classes in keras.\n",
    "i.e these are all classes for ex the adam optimizer is a class in keras.optimizers.Adam it has its own methods and properties. in this case sicne we use the string to specify it keras will internally create an instance of the class for us and apply default parameters.\n",
    "if you want to customize the optimizer or loss function you can create an instance of the class and pass it to the compile method instead of using the string.\n",
    "\n",
    "EX\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001) # create an instance of the Adam optimizer with a custom learning rate\n",
    "model.compile(optimizer=optimizer, loss='mse') # pass the instance of the optimizer to the compile method\n",
    "\n",
    "* FOR the parameters of the optimizers and loss functions you can refer to the keras documentation for more details.\n",
    "\n",
    "# Activations:\n",
    "### IF THE ACTIVATION FUNCTION HAS A LAYERS AND ACTIVATION FUNCTION BOTH (like LeakyReLU, Relu, etc) i.e Activation Layers\n",
    "\n",
    "# Example of passing LeakyReLU as a class with a custom slope\n",
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.05)  # Create an instance of LeakyReLU with a custom slope\n",
    "x = keras.layers.Dense(8, activation=leaky_relu)(inputs)  # Use the instance as the activation function\n",
    "\n",
    "# second example \n",
    "# Example of using Sequential with a custom LeakyReLU activation\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(hidden_units, activation=tf.keras.layers.LeakyReLU(alpha=0.2)), # pass in inline activation\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "or \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(hidden_units),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2), # separate activation layer\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "NOTE: you can import the activation function like: from tensorflow.keras.layers import LeakyReLU and use it directly without tf.keras.layers.LeakyReLU\n",
    "NOTE: here we use the layers apis LeakyReLU there is also a keras.activations.leaky_relu(x, alpha=0.2) function that can be used directly on tensors.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 1) Use the stateless activation function (default alpha=0.2)\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.keras.activations.leaky_relu),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 2) Same but with a custom alpha (e.g. 0.05)\n",
    "model_custom = keras.Sequential([\n",
    "    layers.Dense(64, activation=lambda x: tf.keras.activations.leaky_relu(x, alpha=0.05)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model_custom.summary()\n",
    "\n",
    "#### IN SHORT ####\n",
    "- Use the function for one-off ops; use the Layer when you want to include it in a model (serialization, config, graph). \n",
    "\n",
    "- stateless function: tf.keras.activations.leaky_relu(x, alpha=0.2)\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([[-3.0, -0.5, 0.0, 2.0]])\n",
    "y = tf.keras.activations.leaky_relu(x, alpha=0.2)\n",
    "print(y.numpy())  # tensor with negative values scaled by alpha\n",
    "\n",
    "- as a layer instance: tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "layer = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "y2 = layer(x)    # same effect, but layer is stateful/serializable\n",
    "print(y2.numpy())\n",
    "\n",
    "### IF THE ACTIVATION FUNCTION HAS ONLY A CLASS IN THE ACTIVATIONS CATEGORY (like Gelu, etc) i.e not in Activation Layers\n",
    "\n",
    "# example with GELU activation function. gelu => keras.activations.gelu(x, approximate=False) # x = input tensor approximate = whether to use the approximate version of gelu\n",
    "\n",
    "- using default parameters\n",
    "\n",
    "1) # use string\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='gelu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "2) # use default class\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.keras.activations.gelu),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "- using custom parameters (create custom layer class \n",
    "\n",
    "class GELULayer(tf.keras.layers.Layer): # inheriting from Layer class\n",
    "    def __init__(self, approximate=False, **kwargs): # constructor with custom parameter\n",
    "        super().__init__(**kwargs) # call the constructor of the parent class\n",
    "        self.approximate = approximate # store the custom parameter \n",
    "    def call(self, inputs): # forward pass inputs is the input tensor to the layer i.e wether the neurons of the layer are activated or not\n",
    "        return tf.keras.activations.gelu(inputs, approximate=self.approximate) # apply the gelu activation function with the custom parameter to inputs of the layer\n",
    "    def get_config(self): # method to serialize the layer configuration (so we can save and load the model later)\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"approximate\": self.approximate})\n",
    "        return cfg\n",
    "    \n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=GELULayer(approximate=True)), # use the custom GELULayer with approximate=True\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adc1e1",
   "metadata": {},
   "source": [
    "## üßÆ 2. Regression Example ‚Äî Predicting Continuous Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b173d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 3)\n",
    "y = X @ [3.5, 1.2, -2.0] + np.random.randn(1000) * 0.2\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(3,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871d88",
   "metadata": {},
   "source": [
    "## üî¢ 3. Classification Example ‚Äî MNIST Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten images\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40621ef",
   "metadata": {},
   "source": [
    "## ‚è∏Ô∏è 4. Using Callbacks (EarlyStopping, ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b98069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde465a",
   "metadata": {},
   "source": [
    "## üíæ 5. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b122cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the entire model\n",
    "model.save('my_model.keras')\n",
    "\n",
    "# Load it back\n",
    "new_model = keras.models.load_model('my_model.keras')\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c744c1",
   "metadata": {},
   "source": [
    "## üöÄ 6. Summary ‚Äî What You Learned\n",
    "\n",
    "| Concept | Description |\n",
    "|----------|--------------|\n",
    "| **Sequential API** | Simple stack of layers for feedforward networks |\n",
    "| **Activation Functions** | Add non-linearity (e.g., ReLU, sigmoid, softmax) |\n",
    "| **Loss Function** | Defines how model performance is measured |\n",
    "| **Optimizer** | Controls how weights are updated (Adam, SGD, etc.) |\n",
    "| **Callbacks** | Automate training behavior (e.g., stop early, save model) |\n",
    "| **Saving Models** | `model.save()` and `keras.models.load_model()` |\n",
    "\n",
    "You now have a **solid beginner foundation** in Keras ‚Äî ready to move toward CNNs, RNNs, and advanced deep learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ba62e",
   "metadata": {},
   "source": [
    "## Comparing Tensoflow with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Flow Vs Keras Example:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create random data\n",
    "X = tf.random.normal((100, 3))\n",
    "y = tf.random.normal((100, 1))\n",
    "\n",
    "# Only with TF:\n",
    "# Initialize weights and bias manually\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "# Define a simple forward function\n",
    "def model(x):\n",
    "    return tf.matmul(x, W) + b  # Linear layer\n",
    "\n",
    "# Define loss function (MSE)\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    # Compute gradients and update weights\n",
    "    grads = tape.gradient(loss, [W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "print(\"Trained weights:\", W.numpy())\n",
    "print(\"Trained bias:\", b.numpy())\n",
    "\n",
    "# with keras:\n",
    "from tensorflow.keras import layers, models\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "model.fit(X, y, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7e34a",
   "metadata": {},
   "source": [
    "### Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b387ae4",
   "metadata": {},
   "source": [
    "tf.config.run_functions_eagerly(True) controls how TensorFlow executes operations:\n",
    "\n",
    "**Eager Execution (Enabled)**\n",
    "What it does: Operations execute immediately and return concrete values, just like normal Python code.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Intuitive debugging - You can use print statements, breakpoints, and inspect values directly\n",
    "- Works with Python control flow - Standard if/else, loops work naturally\n",
    "- Easy .numpy() calls - Can convert tensors to NumPy arrays anytime\n",
    "- Great for prototyping - Fast iteration and development\n",
    "  \n",
    "Example:\n",
    "```py\n",
    "x = tf.constant([1, 2, 3])\n",
    "print(x.numpy())  # Works! Prints [1 2 3]\n",
    "```\n",
    "\n",
    "**Graph Execution (Disabled - default in TF 2.x training)**\n",
    "What it does: TensorFlow builds a computational graph first, then executes it optimized.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Much faster - Optimizes the computation graph (2x-10x faster)\n",
    "- Better for production - Can deploy to mobile, edge devices\n",
    "- Parallelization - Automatically optimizes across devices\n",
    "  \n",
    "Trade-off:\n",
    "```py\n",
    "x = tf.constant([1, 2, 3])\n",
    "print(x.numpy())  # Error! Can't access values in graph mode\n",
    "```\n",
    "\n",
    "**NOTE:** you do not need debuging in prod so never enable Eager Execution in production\n",
    "\n",
    "\"Immediate mode\" explained\n",
    "Immediate/Eager Mode (Eager Execution Enabled):\n",
    "```py\n",
    "# Code runs line by line, like normal Python\n",
    "x = tf.constant([1, 2, 3])\n",
    "y = x * 2\n",
    "print(y.numpy())  # Executes immediately ‚Üí [2 4 6]\n",
    "```\n",
    "Operations execute instantly and return real values you can inspect.\n",
    "\n",
    "Graph Mode (default during training) (Eager Execution Disabaled):\n",
    "```py\n",
    "# TensorFlow builds a \"recipe\" first, then executes it all at once\n",
    "x = tf.constant([1, 2, 3])  # Records: \"create constant\"\n",
    "y = x * 2                    # Records: \"multiply by 2\"\n",
    "# Nothing actually computed yet!\n",
    "# Later, TF executes the entire graph optimized\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5b6bd",
   "metadata": {},
   "source": [
    "### Real-World TensorFlow Graph Example\n",
    "\n",
    "Below is a **real computational graph** that TensorFlow builds when training a neural network. This shows actual nodes and operations, not just constants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Disable eager execution to see the graph\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Create a simple neural network\n",
    "model = models.Sequential([\n",
    "    layers.Dense(4, activation='relu', input_shape=(3,), name='hidden_layer'),\n",
    "    layers.Dense(2, activation='softmax', name='output_layer')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create dummy data\n",
    "X_train = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Use @tf.function to trace the computational graph\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions = model(x, training=True)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Run one training step\n",
    "sample_x = tf.constant(X_train[:2])\n",
    "sample_y = tf.constant(y_train[:2])\n",
    "loss = train_step(sample_x, sample_y)\n",
    "\n",
    "print(\"‚úÖ Graph created and executed!\")\n",
    "print(f\"Loss: {tf.reduce_mean(loss).numpy():.4f}\")\n",
    "print(\"\\nüìä The computational graph includes:\")\n",
    "print(\"  - Input tensors (x, y)\")\n",
    "print(\"  - Matrix multiplications (Dense layers)\")\n",
    "print(\"  - ReLU activation\")\n",
    "print(\"  - Softmax activation\")\n",
    "print(\"  - Loss calculation\")\n",
    "print(\"  - Gradient computation\")\n",
    "print(\"  - Weight updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dff697",
   "metadata": {},
   "source": [
    "### Visualizing the Graph with TensorBoard\n",
    "\n",
    "Let's save and visualize the actual computational graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Clear any previous graphs\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='relu', input_shape=(3,), name='hidden'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Dummy data\n",
    "X = np.random.rand(10, 3).astype(np.float32)\n",
    "y = np.random.randint(0, 2, 10)\n",
    "\n",
    "# Set up TensorBoard writer\n",
    "log_dir = \"assets/logs/graph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Trace the graph using @tf.function\n",
    "@tf.function\n",
    "def forward_pass(x):\n",
    "    return model(x)\n",
    "\n",
    "# Execute and trace the graph\n",
    "tf.summary.trace_on(graph=True, profiler=False)\n",
    "forward_pass(X[:1])\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"model_graph\", step=0)\n",
    "\n",
    "print(f\"‚úÖ Graph saved to: {log_dir}\")\n",
    "print(\"\\nüîç To visualize the graph, run:\")\n",
    "print(f\"   tensorboard --logdir={log_dir}\")\n",
    "print(\"   Then open http://localhost:6006 in your browser\")\n",
    "print(\"\\nüìä Or run in notebook:\")\n",
    "print(f\"   %load_ext tensorboard\")\n",
    "print(f\"   %tensorboard --logdir {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ebb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir assets/logs/graph/20251122-161242"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3d64e",
   "metadata": {},
   "source": [
    "There are 2 graphs in the tensorboard (graph1 and graph2)\n",
    "\n",
    "**Small Graph**: High-level conceptual view showing your model architecture (what you designed)\n",
    "\n",
    "**Big Graph**: Low-level detailed view showing every single mathematical operation TensorFlow executes (what actually runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58326eb7",
   "metadata": {},
   "source": [
    "### What a Real Computational Graph Looks Like\n",
    "\n",
    "Here's a detailed breakdown of what happens in the graph above:\n",
    "\n",
    "```\n",
    "INPUT (shape: [batch, 3])\n",
    "    ‚Üì\n",
    "MatMul (weights: [3, 4])  ‚Üê First Dense Layer\n",
    "    ‚Üì\n",
    "BiasAdd (bias: [4])\n",
    "    ‚Üì\n",
    "ReLU Activation\n",
    "    ‚Üì\n",
    "MatMul (weights: [4, 2])  ‚Üê Output Dense Layer\n",
    "    ‚Üì\n",
    "BiasAdd (bias: [2])\n",
    "    ‚Üì\n",
    "Softmax Activation\n",
    "    ‚Üì\n",
    "OUTPUT (shape: [batch, 2])\n",
    "```\n",
    "\n",
    "**During Training (with GradientTape):**\n",
    "```\n",
    "Forward Pass: Input ‚Üí Hidden ‚Üí Output ‚Üí Loss\n",
    "                ‚Üì         ‚Üì       ‚Üì       ‚Üì\n",
    "Backward Pass: ‚àáLoss/‚àáInput ‚Üê ‚àáLoss/‚àáHidden ‚Üê ‚àáLoss/‚àáOutput ‚Üê Loss\n",
    "                        ‚Üì\n",
    "                Update Weights\n",
    "```\n",
    "\n",
    "**Key Nodes in the Graph:**\n",
    "- **Placeholder nodes**: Input data (x, y)\n",
    "- **Variable nodes**: Trainable weights and biases\n",
    "- **Operation nodes**: MatMul, Add, ReLU, Softmax, Loss calculation\n",
    "- **Gradient nodes**: Automatic differentiation operations\n",
    "- **Optimizer nodes**: Weight update operations (Adam optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390c6ff",
   "metadata": {},
   "source": [
    "### Comparing Graph vs Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Simple computation: (a + b) * (a - b)\n",
    "def computation(a, b):\n",
    "    return (a + b) * (a - b)\n",
    "\n",
    "# Test data\n",
    "x = tf.random.normal([1000, 1000])\n",
    "y = tf.random.normal([1000, 1000])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üê¢ EAGER MODE (runs immediately, line by line)\")\n",
    "print(\"=\" * 60)\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = computation(x, y)\n",
    "eager_time = time.time() - start\n",
    "print(f\"Time taken: {eager_time:.3f} seconds\")\n",
    "print(f\"Can inspect: {result[0, 0].numpy():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ GRAPH MODE (builds optimized graph first)\")\n",
    "print(\"=\" * 60)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Wrap in tf.function to create graph\n",
    "@tf.function\n",
    "def graph_computation(a, b):\n",
    "    return (a + b) * (a - b)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = graph_computation(x, y)\n",
    "graph_time = time.time() - start\n",
    "print(f\"Time taken: {graph_time:.3f} seconds\")\n",
    "print(f\"Speedup: {eager_time / graph_time:.2f}x faster!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä WHAT'S HAPPENING IN GRAPH MODE:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. First call: TensorFlow traces the function and builds a graph\")\n",
    "print(\"   - Records: create 'a', create 'b', add, subtract, multiply\")\n",
    "print(\"   - Optimizes: fuses operations, removes redundant computations\")\n",
    "print(\"   - Compiles: converts to efficient low-level operations\")\n",
    "print(\"\\n2. Subsequent calls: Reuses the optimized graph\")\n",
    "print(\"   - No Python overhead\")\n",
    "print(\"   - Parallel execution where possible\")\n",
    "print(\"   - GPU/TPU optimizations applied\")\n",
    "print(\"\\n3. The graph is a static computation plan:\")\n",
    "print(\"   - All operations predefined\")\n",
    "print(\"   - Can be serialized and deployed\")\n",
    "print(\"   - Works on mobile/edge devices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
