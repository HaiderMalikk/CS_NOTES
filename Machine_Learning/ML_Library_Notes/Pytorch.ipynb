{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351cc65e",
   "metadata": {},
   "source": [
    "# PyTorch Handbook\n",
    "\n",
    "A complete guide to PyTorch, from fundamentals to LLM training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# INSTALL DEPENDENCIES\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "%pip install torch numpy transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0b136",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### [1. Core PyTorch Fundamentals](#1-core-pytorch-fundamentals)\n",
    "- [1.1 Tensors](#11-tensors)\n",
    "- [1.2 Autograd and Computation Graphs](#12-autograd-and-computation-graphs)\n",
    "- [1.3 Modules and Parameters](#13-modules-and-parameters)\n",
    "- [1.4 Loss Functions and Optimizers](#14-loss-functions-and-optimizers)\n",
    "\n",
    "### [2. Training Mechanics](#2-training-mechanics)\n",
    "- [2.1 Training Loop Anatomy](#21-training-loop-anatomy)\n",
    "- [2.2 Datasets and DataLoader](#22-datasets-and-dataloader)\n",
    "- [2.3 Device Management](#23-device-management)\n",
    "\n",
    "### [3. PyTorch Building Blocks for Deep Learning](#3-pytorch-building-blocks-for-deep-learning)\n",
    "- [3.1 Layers and Activations](#31-layers-and-activations)\n",
    "- [3.2 Normalization](#32-normalization)\n",
    "- [3.3 Initialization](#33-initialization)\n",
    "\n",
    "### [4. PyTorch for NLP and Sequence Models](#4-pytorch-for-nlp-and-sequence-models)\n",
    "- [4.1 Embeddings and Vocab](#41-embeddings-and-vocab)\n",
    "- [4.2 Attention from Scratch](#42-attention-from-scratch)\n",
    "- [4.3 Transformer Blocks](#43-transformer-blocks)\n",
    "- [4.4 Tokenization Ecosystem](#44-tokenization-ecosystem)\n",
    "\n",
    "### [5. Training Language Models](#5-training-language-models)\n",
    "- [5.1 Autoregressive Training](#51-autoregressive-training)\n",
    "- [5.2 Mixed Precision and Performance](#52-mixed-precision-and-performance)\n",
    "- [5.3 Gradient Control](#53-gradient-control)\n",
    "\n",
    "### [6. Generation and Inference](#6-generation-and-inference)\n",
    "- [6.1 Decoding Mechanics](#61-decoding-mechanics)\n",
    "- [6.2 KV Caching](#62-kv-caching)\n",
    "\n",
    "### [7. Ecosystem and Real-world PyTorch](#7-ecosystem-and-real-world-pytorch)\n",
    "- [7.1 Hugging Face Transformers](#71-hugging-face-transformers)\n",
    "- [7.2 Model Saving and Loading](#72-model-saving-and-loading)\n",
    "- [7.3 Distributed Training Concepts](#73-distributed-training-concepts)\n",
    "\n",
    "### [8. Final Notes](#8-final-notes)\n",
    "- [8.1 Common Questions & Answers](#81-common-questions--answers)\n",
    "- [8.2 Quick Reference Card](#82-quick-reference-card)\n",
    "\n",
    "### [9. PyTorch Examples](#9-pytorch-examples)\n",
    "- [9.1 Basic Neural Network Training](#91-basic-neural-network-training)\n",
    "- [9.2 Transformer Language Model on MPS](#92-transformer-language-model-on-mps)\n",
    "\n",
    "---\n",
    "\n",
    "*This replaces TensorFlow basics, but faster for you.*\n",
    "\n",
    "## 1. Core PyTorch Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb93e87",
   "metadata": {},
   "source": [
    "### 1.1 Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch—multi-dimensional arrays that can run on GPUs and track gradients for automatic differentiation.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `torch.tensor()` | Creates tensor from data, infers dtype |\n",
    "| `torch.Tensor()` | Class constructor, defaults to float32 |\n",
    "| `dtype` | Data type (float32, int64, bool, etc.) |\n",
    "| `device` | Where tensor lives (cpu, cuda, mps) |\n",
    "| `requires_grad` | Enable gradient tracking |\n",
    "\n",
    "**Why this matters:** Everything in PyTorch is a tensor. If you're weak here, everything else leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b63130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([1, 2, 3]), dtype: torch.int64\n",
      "b: tensor([1., 2., 3.]), dtype: torch.float32\n",
      "c shape: torch.Size([2, 3])\n",
      "d: tensor([1., 2., 3.]), dtype: torch.float32\n",
      "e dtype: torch.float64\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TENSOR CREATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# torch.tensor() - Creates tensor from data, infers dtype\n",
    "a = torch.tensor([1, 2, 3])           # int64 inferred\n",
    "b = torch.tensor([1.0, 2.0, 3.0])     # float32 inferred\n",
    "c = torch.tensor([[1, 2, 3], [3, 4, 5]])    # 2D tensor\n",
    "\n",
    "print(f\"a: {a}, dtype: {a.dtype}\")\n",
    "print(f\"b: {b}, dtype: {b.dtype}\")\n",
    "print(f\"c shape: {c.shape}\")\n",
    "\n",
    "# torch.Tensor() - Class constructor, ALWAYS float32\n",
    "d = torch.Tensor([1, 2, 3])           # Always float32\n",
    "print(f\"d: {d}, dtype: {d.dtype}\")\n",
    "\n",
    "# Explicit dtype specification\n",
    "e = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "print(f\"e dtype: {e.dtype}\")\n",
    "\n",
    "# what is the '.' in '1.' \n",
    "a = torch.tensor([1]) # int \n",
    "b = torch.tensor([1.]) # creates a float tensor i.e float32 1.xxx\n",
    "print(a.dtype)  # torch.int64\n",
    "print(b.dtype)  # torch.float32\n",
    "# ! If x were [1] instead of [1.], PyTorch would not allow gradient tracking because it would be an integer tensor not a floating-point tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c940f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "MPS available: True\n",
      "Using device: mps\n",
      "Tensor device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEVICE PLACEMENT\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Check available devices\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")  # Apple Silicon\n",
    "\n",
    "# Create tensor on specific device\n",
    "cpu_tensor = torch.tensor([1, 2, 3], device='cpu')\n",
    "\n",
    "# Move tensor to device (use this pattern for portability)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available() \n",
    "                      else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "print(f\"Tensor device: {gpu_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa77c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad: False\n",
      "requires_grad: True\n",
      "y requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# REQUIRES_GRAD - Enable gradient tracking\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\"\"\" \n",
    "Gradient tracking in PyTorch is automatic differentiation where PyTorch records tensor operations to build a computation graph and computes gradients via backpropagation when you call backward().\n",
    "You use gradient tracking so your model knows how to change its parameters to reduce loss, enabling learning through backpropagation.\n",
    "this is enabled by setting the requires_grad attribute of a tensor to True. it will be on during training and off during inference to save memory and computations.\n",
    "if you dont have this on the backpropagation step will fail because there will be no computation graph to traverse.\n",
    "\"\"\"\n",
    "\n",
    "# By default, tensors don't track gradients\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"requires_grad: {x.requires_grad}\")  # False\n",
    "\n",
    "# Enable gradient tracking\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(f\"requires_grad: {x.requires_grad}\")  # True\n",
    "\n",
    "# Or set it after creation\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "y.requires_grad_(True)  # In-place operation (note the underscore)\n",
    "print(f\"y requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e73530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add: tensor([5., 7., 9.])\n",
      "Multiply: tensor([ 4., 10., 18.])\n",
      "Power: tensor([1., 4., 9.])\n",
      "Matrix multiply (@ operator): \n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "Matrix multiply (torch.matmul): \n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "Broadcast scalar: tensor([11, 12, 13])\n",
      "Broadcast vector to matrix:\n",
      "tensor([[2, 4, 6],\n",
      "        [5, 7, 9]])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# BASIC OPERATIONS AND BROADCASTING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"Add: {a + b}\")\n",
    "print(f\"Multiply: {a * b}\")\n",
    "print(f\"Power: {a ** 2}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Matrix multiply (@ operator): \\n{A @ B}\")\n",
    "print(f\"Matrix multiply (torch.matmul): \\n{torch.matmul(A, B)}\")\n",
    "\n",
    "# Broadcasting - smaller tensor expands to match larger\n",
    "scalar = torch.tensor(10)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(f\"Broadcast scalar: {scalar + vector}\")\n",
    "\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Broadcast vector to matrix:\\n{matrix + vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a254e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element [1,2]: 6\n",
      "Row 0: tensor([1, 2, 3])\n",
      "Column 1: tensor([2, 5, 8])\n",
      "First 2 rows:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Last 2 columns:\n",
      "tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n",
      "Elements > 5: tensor([6, 7, 8, 9])\n",
      "Rows 0 and 2:\n",
      "tensor([[1, 2, 3],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# INDEXING AND SLICING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Basic indexing (same as NumPy)\n",
    "print(f\"Element [1,2]: {t[1, 2]}\")\n",
    "print(f\"Row 0: {t[0]}\")\n",
    "print(f\"Column 1: {t[:, 1]}\")\n",
    "\n",
    "# Slicing\n",
    "print(f\"First 2 rows:\\n{t[:2]}\")\n",
    "print(f\"Last 2 columns:\\n{t[:, -2:]}\")\n",
    "\n",
    "# Boolean indexing\n",
    "mask = t > 5\n",
    "print(f\"Elements > 5: {t[mask]}\")\n",
    "\n",
    "# Fancy indexing\n",
    "indices = torch.tensor([0, 2])\n",
    "print(f\"Rows 0 and 2:\\n{t[indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d240b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From NumPy: tensor([1., 2., 3.], dtype=torch.float64)\n",
      "After modifying NumPy: tensor([999.,   2.,   3.], dtype=torch.float64)\n",
      "To NumPy: [1. 2. 3.]\n",
      "Cloned (independent): tensor([999.,   2.,   3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# PYTORCH TENSORS VS NUMPY ARRAYS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# NumPy to PyTorch (shares memory by default!)\n",
    "np_array = np.array([1.0, 2.0, 3.0])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "print(f\"From NumPy: {tensor_from_np}\")\n",
    "\n",
    "# Modify NumPy array - tensor changes too!\n",
    "np_array[0] = 999\n",
    "print(f\"After modifying NumPy: {tensor_from_np}\")  # Also 999!\n",
    "\n",
    "# PyTorch to NumPy (also shares memory)\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "np_from_tensor = tensor.numpy()\n",
    "print(f\"To NumPy: {np_from_tensor}\")\n",
    "\n",
    "# Use .clone() to avoid shared memory\n",
    "tensor_copy = torch.from_numpy(np_array).clone()\n",
    "np_array[0] = 1  # Original back\n",
    "print(f\"Cloned (independent): {tensor_copy}\")  # Still 999\n",
    "\n",
    "# Key differences:\n",
    "# - PyTorch tensors can live on GPU\n",
    "# - PyTorch tensors track gradients\n",
    "# - PyTorch has autograd integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea5f02",
   "metadata": {},
   "source": [
    "### 1.2 Autograd and Computation Graphs\n",
    "\n",
    "**Critical mindset:** PyTorch builds the computation graph *as Python executes*, not ahead of time. This is the single biggest difference from TensorFlow.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `requires_grad=True` | Tells PyTorch to track operations on this tensor |\n",
    "| Computation graph | DAG of operations built dynamically during forward pass |\n",
    "| `.backward()` | Computes gradients via reverse-mode autodiff |\n",
    "| `.grad` | Stores computed gradients |\n",
    "| `torch.no_grad()` | Context manager to disable gradient tracking |\n",
    "\n",
    "**How it works:**\n",
    "1. Every operation on tensors with `requires_grad=True` is recorded\n",
    "2. The graph is built on-the-fly (dynamic graph)\n",
    "3. Calling `.backward()` on a scalar traverses the graph backwards\n",
    "4. Gradients accumulate in `.grad` attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd4ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "out: 23.0\n",
      "z.grad_fn: <MulBackward0 object at 0x1076e0f40>\n",
      "out.grad_fn: <SumBackward0 object at 0x1079fec80>\n",
      "x.grad: tensor([4., 5.])\n",
      "y.grad: tensor([2., 3.])\n",
      "z.grad: None\n",
      "tensor(4.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/23/rj3hwzds32x8m0hj4z25n9kh0000gn/T/ipykernel_75331/232015005.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"z.grad: {z.grad}\")  # None, because z is not a leaf node\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n═══════════════════════════════════════════════════════════════════════════════\\n                        COMPUTATION GRAPH EXPLANATION\\n═══════════════════════════════════════════════════════════════════════════════\\n\\nFORWARD PASS (Building the Graph):\\n──────────────────────────────────\\n    x = [2., 3.]  (LEAF NODE - user input)\\n    y = [4., 5.]  (LEAF NODE - user input)\\n         │           │\\n         └─────┬─────┘\\n               ↓\\n         z = x * y = [2*4, 3*5] = [8., 15.]  (INTERMEDIATE NODE)\\n         (grad_fn = <MulBackward0>)\\n               │\\n               ↓\\n       out = z.sum() = 8 + 15 = 23.0  (OUTPUT NODE)\\n       (grad_fn = <SumBackward0>)\\n\\n\\nBACKWARD PASS (Computing Gradients):\\n─────────────────────────────────────\\nWhen we call out.backward(), PyTorch computes gradients using the chain rule:\\n\\nStep 1: Start at output node\\n   out = 23.0\\n   d(out)/d(out) = 1.0  (gradient at the starting point)\\n\\nStep 2: Backprop through SumBackward0 (out = z.sum())\\n   d(out)/d(z) = [1., 1.]  (sum distributes gradient equally to all inputs)\\n   \\n   Why? Because:\\n   out = z[0] + z[1]\\n   ∂(out)/∂(z[0]) = 1\\n   ∂(out)/∂(z[1]) = 1\\n\\nStep 3: Backprop through MulBackward0 (z = x * y)\\n   Using chain rule: d(out)/d(x) = d(out)/d(z) * d(z)/d(x)\\n   \\n   For x[0]: d(z[0])/d(x[0]) = y[0] = 4.0\\n             d(out)/d(x[0]) = 1.0 * 4.0 = 4.0\\n   \\n   For x[1]: d(z[1])/d(x[1]) = y[1] = 5.0\\n             d(out)/d(x[1]) = 1.0 * 5.0 = 5.0\\n   \\n   Therefore: x.grad = [4., 5.]  ← This is why x.grad = y!\\n   \\n   Similarly for y:\\n   For y[0]: d(z[0])/d(y[0]) = x[0] = 2.0\\n             d(out)/d(y[0]) = 1.0 * 2.0 = 2.0\\n   \\n   For y[1]: d(z[1])/d(y[1]) = x[1] = 3.0\\n             d(out)/d(y[1]) = 1.0 * 3.0 = 3.0\\n   \\n   Therefore: y.grad = [2., 3.]  ← This is why y.grad = x!\\n\\n\\nWHY z.grad IS None:\\n───────────────────\\n• z is an INTERMEDIATE (non-leaf) node in the computation graph\\n• By default, PyTorch only stores gradients for LEAF nodes (x, y)\\n• This saves memory for large networks\\n• To get z.grad, you must call z.retain_grad() before backward()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# AUTOGRAD BASICS - Building computation graphs dynamically\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0], requires_grad=True)\n",
    "\n",
    "# Every operation creates a node in the graph\n",
    "z = x * y          # Element-wise multiply\n",
    "out = z.sum()      # Sum to scalar (required for .backward())\n",
    "\n",
    "print(f\"z: {z}\")\n",
    "print(f\"out: {out}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")      # MulBackward (the grad_fn shows the operation that created this tensor can be Muultiply, Add, etc.)\n",
    "print(f\"out.grad_fn: {out.grad_fn}\")  # SumBackward\n",
    "\n",
    "# Compute gradients\n",
    "out.backward()\n",
    "\n",
    "# Gradients are stored in .grad\n",
    "print(f\"x.grad: {x.grad}\")  # d(out)/dx = y = [4, 5]\n",
    "print(f\"y.grad: {y.grad}\")  # d(out)/dy = x = [2, 3]\n",
    "print(f\"z.grad: {z.grad}\")  # None, because z is not a leaf node\n",
    "\n",
    "# EX 2 scalar case (no need to sum)\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * x\n",
    "y.backward()\n",
    "print(x.grad)  # 4.0 (all operations so far on x)\n",
    "\n",
    "\"\"\" \n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "                        COMPUTATION GRAPH EXPLANATION\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "FORWARD PASS (Building the Graph):\n",
    "──────────────────────────────────\n",
    "    x = [2., 3.]  (LEAF NODE - user input)\n",
    "    y = [4., 5.]  (LEAF NODE - user input)\n",
    "         │           │\n",
    "         └─────┬─────┘\n",
    "               ↓\n",
    "         z = x * y = [2*4, 3*5] = [8., 15.]  (INTERMEDIATE NODE)\n",
    "         (grad_fn = <MulBackward0>)\n",
    "               │\n",
    "               ↓\n",
    "       out = z.sum() = 8 + 15 = 23.0  (OUTPUT NODE)\n",
    "       (grad_fn = <SumBackward0>)\n",
    "\n",
    "\n",
    "BACKWARD PASS (Computing Gradients):\n",
    "─────────────────────────────────────\n",
    "When we call out.backward(), PyTorch computes gradients using the chain rule:\n",
    "\n",
    "Step 1: Start at output node\n",
    "   out = 23.0\n",
    "   d(out)/d(out) = 1.0  (gradient at the starting point)\n",
    "\n",
    "Step 2: Backprop through SumBackward0 (out = z.sum())\n",
    "   d(out)/d(z) = [1., 1.]  (sum distributes gradient equally to all inputs)\n",
    "   \n",
    "   Why? Because:\n",
    "   out = z[0] + z[1]\n",
    "   ∂(out)/∂(z[0]) = 1\n",
    "   ∂(out)/∂(z[1]) = 1\n",
    "\n",
    "Step 3: Backprop through MulBackward0 (z = x * y)\n",
    "   Using chain rule: d(out)/d(x) = d(out)/d(z) * d(z)/d(x)\n",
    "   \n",
    "   For x[0]: d(z[0])/d(x[0]) = y[0] = 4.0\n",
    "             d(out)/d(x[0]) = 1.0 * 4.0 = 4.0\n",
    "   \n",
    "   For x[1]: d(z[1])/d(x[1]) = y[1] = 5.0\n",
    "             d(out)/d(x[1]) = 1.0 * 5.0 = 5.0\n",
    "   \n",
    "   Therefore: x.grad = [4., 5.]  ← This is why x.grad = y!\n",
    "   \n",
    "   Similarly for y:\n",
    "   For y[0]: d(z[0])/d(y[0]) = x[0] = 2.0\n",
    "             d(out)/d(y[0]) = 1.0 * 2.0 = 2.0\n",
    "   \n",
    "   For y[1]: d(z[1])/d(y[1]) = x[1] = 3.0\n",
    "             d(out)/d(y[1]) = 1.0 * 3.0 = 3.0\n",
    "   \n",
    "   Therefore: y.grad = [2., 3.]  ← This is why y.grad = x!\n",
    "\n",
    "\n",
    "WHY z.grad IS None:\n",
    "───────────────────\n",
    "• z is an INTERMEDIATE (non-leaf) node in the computation graph\n",
    "• By default, PyTorch only stores gradients for LEAF nodes (x, y)\n",
    "• This saves memory for large networks\n",
    "• To get z.grad, you must call z.retain_grad() before backward()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a50f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first backward: tensor([2., 2.])\n",
      "After second backward: tensor([5., 5.])\n",
      "After zeroing: tensor([0., 0.])\n",
      "After third backward: tensor([4., 4.])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# GRADIENT ACCUMULATION - Why zeroing matters\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# First backward pass\n",
    "y1 = (x * 2).sum()\n",
    "y1.backward()\n",
    "print(f\"After first backward: {x.grad}\")  # [2, 2]\n",
    "\n",
    "# Second backward pass - gradients ACCUMULATE!\n",
    "y2 = (x * 3).sum()\n",
    "y2.backward()\n",
    "print(f\"After second backward: {x.grad}\")  # [5, 5] = [2+3, 2+3]\n",
    "\n",
    "# This is why you MUST zero gradients in training loops\n",
    "x.grad.zero_()  # In-place zero\n",
    "print(f\"After zeroing: {x.grad}\")  # [0, 0]\n",
    "\n",
    "# Now fresh gradient\n",
    "y3 = (x * 4).sum()\n",
    "y3.backward()\n",
    "print(f\"After third backward: {x.grad}\")  # [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e20a7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.requires_grad: True\n",
      "y.grad_fn: <MulBackward0 object at 0x115b41f60>\n",
      "z.requires_grad: False\n",
      "z.grad_fn: None\n",
      "w.requires_grad: False\n",
      "v.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TORCH.NO_GRAD() - Disable gradient tracking for inference\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# With gradient tracking (default)\n",
    "y = x * 2\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")  # True\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")  # MulBackward\n",
    "\n",
    "# Without gradient tracking - faster, less memory\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print(f\"z.requires_grad: {z.requires_grad}\")  # False\n",
    "    print(f\"z.grad_fn: {z.grad_fn}\")  # None\n",
    "\n",
    "# Alternative: detach() creates a new tensor without grad\n",
    "w = x.detach() * 2\n",
    "print(f\"w.requires_grad: {w.requires_grad}\")  # False\n",
    "\n",
    "# Use torch.inference_mode() for even more optimization (PyTorch 1.9+)\n",
    "with torch.inference_mode():\n",
    "    v = x * 2\n",
    "    print(f\"v.requires_grad: {v.requires_grad}\")  # False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832a487",
   "metadata": {},
   "source": [
    "### 1.3 Modules and Parameters\n",
    "\n",
    "Every model, block, transformer, and attention head in PyTorch is a `nn.Module`. This maps directly to TensorFlow's subclassed models.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| `nn.Module` | Base class for all neural network modules |\n",
    "| `nn.Parameter` | Tensor that's automatically registered as a parameter |\n",
    "| `forward()` | Defines the computation performed at every call |\n",
    "| `model.parameters()` | Iterator over all learnable parameters |\n",
    "| `state_dict` | Dictionary mapping parameter names to tensors |\n",
    "\n",
    "**The Module Pattern:**\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward pass here\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef514c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([32, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\n- PyTorch linear layers are batch aware.\\n- forward does not care about batch size.\\n- It operates on the last dimension.\\n- nn.Linear expects input shape. batch_size, input_size. here our input size is 10 and batch size is 32.\\n- so here we pas 'x' = (32, 10) to the model and it processes each of the 32 samples independently through the linear layers. so 10 features passed in the model 32 times (batch size).\\n- What forward expects: The last dimension matches input_size. (Shape is anything, input_size).\\n- for ex (8, 32, 10): 8 time steps. 32 samples per time step. Each sample has 10 features.\\n- there 10 festures are mapped to 20 hidden fetures then those are mapped to one of 5 output features (classes).\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# BASIC MODULE DEFINITION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()  # Always call parent __init__\n",
    "        \n",
    "        # Layers are registered automatically when assigned as attributes\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # linear layer is a fully connected layer where each input feature is connected to each output feature with its own weight. a linear layer is simply: xW^T + b; x = input, W = weights, b = bias the ^T is the transpose done to the weights to match dimensions with input size and hidden size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNet(10, 20, 5)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(32, 10)  # Batch of 32, 10 features each\n",
    "output = model(x)        # Calls forward() automatically\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\"\"\" \n",
    "- PyTorch linear layers are batch aware.\n",
    "- forward does not care about batch size.\n",
    "- It operates on the last dimension.\n",
    "- nn.Linear expects input shape. batch_size, input_size. here our input size is 10 and batch size is 32.\n",
    "- so here we pas 'x' = (32, 10) to the model and it processes each of the 32 samples independently through the linear layers. so 10 features passed in the model 32 times (batch size).\n",
    "- What forward expects: The last dimension matches input_size. (Shape is anything, input_size).\n",
    "- for ex (8, 32, 10): 8 time steps. 32 samples per time step. Each sample has 10 features.\n",
    "- there 10 festures are mapped to 20 hidden fetures then those are mapped to one of 5 output features (classes).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e43973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  weights: torch.Size([5]), requires_grad=True\n",
      "  bias: torch.Size([5]), requires_grad=True\n",
      "\n",
      "Buffers:\n",
      "  constant: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NN.PARAMETER - Custom learnable parameters\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        # nn.Parameter wraps a tensor and registers it as a parameter\n",
    "        self.weights = nn.Parameter(torch.randn(size))\n",
    "        self.bias = nn.Parameter(torch.zeros(size))\n",
    "        \n",
    "        # Regular tensors are NOT parameters (not learned)\n",
    "        self.register_buffer('constant', torch.ones(size))  # Saved but not trained\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.weights + self.bias + self.constant\n",
    "\n",
    "layer = CustomLayer(5)\n",
    "\n",
    "# Check what's registered\n",
    "print(\"Parameters:\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "print(\"\\nBuffers:\")\n",
    "for name, buf in layer.named_buffers():\n",
    "    print(f\"  {name}: {buf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ac355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters:\n",
      "  fc1.weight: torch.Size([20, 10])\n",
      "  fc1.bias: torch.Size([20])\n",
      "  fc2.weight: torch.Size([5, 20])\n",
      "  fc2.bias: torch.Size([5])\n",
      "\n",
      "Total parameters: 325\n",
      "Trainable parameters: 325\n",
      "\n",
      "State dict keys: odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# MODEL.PARAMETERS() AND STATE_DICT\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "model = SimpleNet(10, 20, 5)\n",
    "\n",
    "# Iterate over all parameters\n",
    "print(\"All parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# Total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "# State dict - serializable dictionary of all parameters\n",
    "state = model.state_dict()\n",
    "print(f\"\\nState dict keys: {state.keys()}\")\n",
    "\n",
    "# Load state dict (for loading saved models)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e111f",
   "metadata": {},
   "source": [
    "### 1.4 Loss Functions and Optimizers\n",
    "\n",
    "**Important:** PyTorch expects raw logits, not softmaxed outputs, for most losses.\n",
    "\n",
    "**Key Loss Functions:**\n",
    "\n",
    "| Loss | Use Case | Input |\n",
    "|------|----------|-------|\n",
    "| `nn.CrossEntropyLoss` | Multi-class classification | Raw logits |\n",
    "| `nn.BCEWithLogitsLoss` | Binary/multi-label classification | Raw logits |\n",
    "| `nn.MSELoss` | Regression | Any |\n",
    "| `nn.NLLLoss` | Multi-class (after log_softmax) | Log probabilities |\n",
    "\n",
    "**Reduction Modes:**\n",
    "- `reduction='mean'` (default): Average loss over batch\n",
    "- `reduction='sum'`: Sum loss over batch\n",
    "- `reduction='none'`: Return loss per sample\n",
    "\n",
    "**Key Optimizers:**\n",
    "\n",
    "| Optimizer | Description |\n",
    "|-----------|-------------|\n",
    "| `SGD` | Basic stochastic gradient descent |\n",
    "| `Adam` | Adaptive learning rates, most popular |\n",
    "| `AdamW` | Adam with proper weight decay (use for transformers) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc4f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss: 0.3669\n",
      "Log probabilities:\n",
      "tensor([[-0.4170, -1.4170, -2.3170],\n",
      "        [-2.2168, -0.3168, -1.8168]])\n",
      "Manual NLLLoss: 0.3669\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CROSS ENTROPY LOSS - The most important for classification\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# NOTE: the loss function mesures the difference between the predicted class probabilities (logits) and the true class labels. this loss is then used to tell the # model how to adjust its weights during training to improve accuracy. it is especially useful for multi-class classification problems where each input belongs to one of several classes.\n",
    "\n",
    "# CrossEntropyLoss combines LogSoftmax + NLLLoss; wher logSoftmax = log(softmax(x)) and NLLLoss is negative log likelihood loss = -log(p) where p is the predicted probability of the true class.\n",
    "# Input: raw logits (batch_size, num_classes)\n",
    "# Target: class indices (batch_size,) NOT one-hot!\n",
    "\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1],   # Sample 1: likely class 0\n",
    "                       [0.1, 2.0, 0.5]])  # Sample 2: likely class 1\n",
    "targets = torch.tensor([0, 1])  # True classes\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, targets)\n",
    "print(f\"CrossEntropyLoss: {loss.item():.4f}\")\n",
    "\n",
    "# Understanding what happens internally:\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "log_probs = log_softmax(logits)\n",
    "print(f\"Log probabilities:\\n{log_probs}\")\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "loss_manual = nll_loss(log_probs, targets)\n",
    "print(f\"Manual NLLLoss: {loss_manual.item():.4f}\")  # Same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c205aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE with Logits: 0.4205\n",
      "MSE Loss: 0.1667\n",
      "Per-sample loss: tensor([0.2500, 0.0000, 0.2500])\n",
      "Sum loss: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# OTHER LOSS FUNCTIONS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Binary Cross Entropy with Logits (for binary/multi-label)\n",
    "logits_binary = torch.tensor([0.5, -0.5, 1.0])\n",
    "targets_binary = torch.tensor([1.0, 0.0, 1.0])\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "print(f\"BCE with Logits: {bce(logits_binary, targets_binary).item():.4f}\")\n",
    "\n",
    "# Mean Squared Error (regression)\n",
    "predictions = torch.tensor([1.0, 2.0, 3.0])\n",
    "targets_reg = torch.tensor([1.5, 2.0, 2.5])\n",
    "mse = nn.MSELoss()\n",
    "print(f\"MSE Loss: {mse(predictions, targets_reg).item():.4f}\")\n",
    "\n",
    "# Reduction modes\n",
    "mse_none = nn.MSELoss(reduction='none')\n",
    "mse_sum = nn.MSELoss(reduction='sum')\n",
    "print(f\"Per-sample loss: {mse_none(predictions, targets_reg)}\")\n",
    "print(f\"Sum loss: {mse_sum(predictions, targets_reg).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer state keys: dict_keys(['state', 'param_groups'])\n",
      "Param groups: 1\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# OPTIMIZERS - SGD, Adam, AdamW\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# NOTE: optimizers are algorithms used to update the weights of a neural network during training in order to minimize the loss function. they determine how the model learns from the data by adjusting the parameters based on the computed gradients. different optimizers use different strategies for updating the weights, which can affect the speed and quality of learning.\n",
    "# - the Lr (learning rate) is the most important hyperparameter to tune for optimizers. it controls how big of a step we take during each update. too high can cause divergence, too low can make training very slow.\n",
    "\n",
    "model = SimpleNet(10, 20, 5)\n",
    "\n",
    "# SGD - Basic gradient descent\n",
    "optimizer_sgd = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adam - Adaptive learning rates (most popular)\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# AdamW - Adam with decoupled weight decay (use for transformers!)\n",
    "optimizer_adamw = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Weight decay comparison:\n",
    "# Adam:  weight_decay is L2 regularization (added to loss)\n",
    "# AdamW: weight_decay is true weight decay (decoupled from loss)\n",
    "# For transformers and large models, AdamW is almost always better\n",
    "\n",
    "print(\"Optimizer state keys:\", optimizer_adam.state_dict().keys())\n",
    "print(\"Param groups:\", len(optimizer_adam.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f586bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haidermalik/Documents/Code/CS_NOTES/Machine_Learning/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nin this EX:\\n- Initial learning rate is 0.001.\\n- StepLR uses step_size=10.\\n- Gamma is 0.1.\\n\\nWhat actually happens.\\n- For epochs 1 to 10.\\n- Learning rate stays 0.001.\\n- At epoch 10.\\n- Learning rate becomes 0.0001.\\n- At epoch 20.\\n- Learning rate becomes 0.00001.\\n\\nRule.\\n- LR_new = LR_old x gamma.\\n- This happens every step_size epochs.\\n- Not every batch or train step.\\n\\nIf you wanted per step changes.\\n- You would call scheduler.step() per batch.\\n- Or use schedulers designed for that.\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Learning Rate Shedulers\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\"\"\" \n",
    "Learning rate schedulers control how the learning rate changes during training.\n",
    "\n",
    "What they do.\n",
    "- Adjust the learning rate over time.\n",
    "- Usually decrease it as training progresses.\n",
    "- Improve convergence and stability.\n",
    "\n",
    "Why you need them.\n",
    "- High learning rate helps early learning.\n",
    "- Lower learning rate helps fine tuning.\n",
    "- Fixed learning rates often stall or overshoot.\n",
    "\n",
    "How they work in PyTorch.\n",
    "- They wrap an optimizer.\n",
    "- You call scheduler.step().\n",
    "- They update optimizer.param_groups learning rates.\n",
    "\n",
    "Common schedulers.\n",
    "- StepLR. Drops LR every N epochs.\n",
    "- MultiStepLR. Drops LR at specific epochs.\n",
    "- ExponentialLR. Decays LR continuously.\n",
    "- CosineAnnealingLR. Smooth cosine decay.\n",
    "- ReduceLROnPlateau. Lowers LR when loss stops improving.\n",
    "\n",
    "Key tradeoff.\n",
    "- Too fast decay slows learning.\n",
    "- Too slow decay causes instability.\n",
    "\n",
    "Rule of thumb.\n",
    "- Start without a scheduler.\n",
    "- Add one when training plateaus or oscillates.\n",
    "\"\"\"\n",
    "\n",
    "# EX:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=10,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "# usage\n",
    "for epoch in range(epochs):\n",
    "    # train()\n",
    "    scheduler.step()\n",
    "\n",
    "\"\"\" \n",
    "in this EX:\n",
    "- Initial learning rate is 0.001.\n",
    "- StepLR uses step_size=10.\n",
    "- Gamma is 0.1.\n",
    "\n",
    "What actually happens.\n",
    "- For epochs 1 to 10.\n",
    "- Learning rate stays 0.001.\n",
    "- At epoch 10.\n",
    "- Learning rate becomes 0.0001.\n",
    "- At epoch 20.\n",
    "- Learning rate becomes 0.00001.\n",
    "\n",
    "Rule.\n",
    "- LR_new = LR_old x gamma.\n",
    "- This happens every step_size epochs.\n",
    "- Not every batch or train step.\n",
    "\n",
    "If you wanted per step changes.\n",
    "- You would call scheduler.step() per batch.\n",
    "- Or use schedulers designed for that.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891321d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Training Mechanics\n",
    "\n",
    "*This is where PyTorch starts to feel powerful. This replaces TF's `model.fit()` entirely.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be914a5b",
   "metadata": {},
   "source": [
    "### 2.1 Training Loop Anatomy\n",
    "\n",
    "The canonical PyTorch training loop follows this pattern:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 2. Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 3. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. Zero gradients (CRITICAL!)\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "**Why this order matters:**\n",
    "- `backward()` computes gradients based on the current loss\n",
    "- `step()` updates weights using those gradients\n",
    "- `zero_grad()` clears gradients for next iteration (they accumulate by default!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906af53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.4061,  1.8123,  0.6576,  0.4971,  0.4414, -1.0050,  1.2208,  0.6445,\n",
      "         1.5288, -0.7583]), tensor(1))\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32])\n",
      "Epoch 1/3, Loss: 1.6625\n",
      "Epoch 2/3, Loss: 1.6398\n",
      "Epoch 3/3, Loss: 1.6246\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMPLETE TRAINING LOOP EXAMPLE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Create synthetic data\n",
    "X = torch.randn(1000, 10) # data: feature size 10 1000 samples\n",
    "y = torch.randint(0, 5, (1000,))  # lables: 5 random classes to simulate true lables 1 -> 4 1000 samples\n",
    "# Create dataset and dataloader (covered in next section)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(X, y) # combine features and labels into a dataset for easy batching after this the form is: (features, labels)\n",
    "print(dataset[0]) # access the first sample (features, label) both are tensors\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # takes the data tuples and creates batches of size 32 and shuffles them every epoch thes bacthes are in form (batch_X, batch_y) where batch_X is the data 32 samples with 10 features each sample and batch_y is the true lables for that batch since thre are 32 examples in each batch i.e len(bacth_X) = 32 there are 32 lables in bacth y one for each corrisponding batch X exampel\n",
    "print(next(iter(dataloader))[0].shape) # batch X: 32 tensors each with 10 features i.e 10 dataset[0:33] (32 inclusive) and 10 features as each dataset[n] has 10 features\n",
    "print(next(iter(dataloader))[1].shape) # batch y: 32 tensors each with 1 label i.e 1 dataset[0:33] (32 inclusive) and 1 label as each dataset[n] has 1 label\n",
    "\n",
    "# * next is a function that returns the next item in the iterator iter is a function that returns an iterator a iterator is an object that implements the __iter__ and __next__ methods. the dataloader is an iterable that returns batches of data when iterated over. the next function is used to get the first batch of data from the dataloader.\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = SimpleNet(10, 20, 5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in dataloader: # batch x is the data, batch y is the true lable \n",
    "        # 1. Forward pass (pass in batch_X i.e the data to make a prediction usign the model) \n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        # 2. Compute loss (compare outputs to true labels batch_y i.e true outputs to see hwo off we were)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # 3. Backward pass (computes gradients for each parameter with respect to loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Update weights (apply gradients to the model parameters)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. Zero gradients for next iteration (important to prevent accumulation i.e each training step we start fresh)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b5a1e",
   "metadata": {},
   "source": [
    "### 2.2 Datasets and DataLoader\n",
    "\n",
    "PyTorch's data loading is more explicit and Pythonic than TensorFlow's `tf.data`.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| `Dataset` | Abstract class defining `__getitem__` and `__len__` |\n",
    "| `DataLoader` | Wraps dataset for batching, shuffling, parallel loading |\n",
    "| `collate_fn` | Custom function to combine samples into batches |\n",
    "| `num_workers` | Number of parallel data loading processes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f998a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n",
      "Sample shape: torch.Size([10]), Label: 1\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CUSTOM DATASET CLASS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset must implement:\n",
    "    - __init__: Setup (load data, store paths, etc.)\n",
    "    - __len__: Return total number of samples\n",
    "    - __getitem__: Return one sample by index\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label\n",
    "\n",
    "# Create synthetic data\n",
    "X = torch.randn(1000, 10)  # 1000 samples, 10 features\n",
    "y = torch.randint(0, 5, (1000,))  # 5 classes\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(X, y)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "sample, label = dataset[0]\n",
    "print(f\"Sample shape: {sample.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd263a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: X shape = torch.Size([16, 10]), y shape = torch.Size([16])\n",
      "Batch 1: X shape = torch.Size([16, 10]), y shape = torch.Size([16])\n",
      "Batch 2: X shape = torch.Size([16, 10]), y shape = torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haidermalik/Documents/Code/CS_NOTES/Machine_Learning/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DATALOADER - Batching, shuffling, parallel loading\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Basic DataLoader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,           # Shuffle at each epoch\n",
    "    num_workers=0,          # Number of parallel loading processes (0 = main process)\n",
    "    drop_last=False,        # Drop incomplete last batch?\n",
    "    pin_memory=True,        # Faster GPU transfer (use with CUDA)\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, (batch_x, batch_y) in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}: X shape = {batch_x.shape}, y shape = {batch_y.shape}\")\n",
    "    if batch_idx >= 2:\n",
    "        break\n",
    "\n",
    "# TensorDataset - Quick dataset from tensors\n",
    "from torch.utils.data import TensorDataset\n",
    "quick_dataset = TensorDataset(X, y)\n",
    "quick_loader = DataLoader(quick_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce3d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom collate batch: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COLLATE_FN - Custom batching logic\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# collate_fn is called to combine samples into a batch\n",
    "# Useful for variable-length sequences, padding, etc.\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"Custom collate function that pads sequences to max length in batch.\"\"\"\n",
    "    # batch is a list of (data, label) tuples\n",
    "    data = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Stack into tensors\n",
    "    data = torch.stack(data)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    # You could add padding here for variable-length sequences\n",
    "    return data, labels\n",
    "\n",
    "loader_with_collate = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=custom_collate\n",
    ")\n",
    "\n",
    "batch_x, batch_y = next(iter(loader_with_collate))\n",
    "print(f\"Custom collate batch: {batch_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea1c61",
   "metadata": {},
   "source": [
    "### 2.3 Device Management\n",
    "\n",
    "LLMs will fail instantly if you don't understand device placement. All tensors and models must be on the same device.\n",
    "\n",
    "**Common Device Mismatch Errors:**\n",
    "```\n",
    "RuntimeError: Expected all tensors to be on the same device, \n",
    "but found at least two devices, cuda:0 and cpu!\n",
    "```\n",
    "\n",
    "**The Pattern:**\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b215a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model device: mps:0\n",
      "Output device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEVICE MANAGEMENT - The essential pattern\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Set device once at the start\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() \n",
    "    else 'mps' if torch.backends.mps.is_available()  # Apple Silicon\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = SimpleNet(10, 20, 5)\n",
    "model = model.to(device)  # Returns model, also modifies in-place\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Move data to device (must do this for every batch!)\n",
    "X = torch.randn(32, 10)\n",
    "y = torch.randint(0, 5, (32,))\n",
    "\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# Now forward pass works\n",
    "output = model(X)\n",
    "print(f\"Output device: {output.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "729ab8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMPLETE TRAINING LOOP WITH DEVICE MANAGEMENT\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # Set to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Move batch to device\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient tracking for evaluation\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "# Example usage\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffd7e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch Building Blocks for Deep Learning\n",
    "\n",
    "*This maps closely to what you already know from TensorFlow, but in PyTorch style.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b2a4d",
   "metadata": {},
   "source": [
    "### 3.1 Layers and Activations\n",
    "\n",
    "**Important:** You will manually control `train()` and `eval()` modes. This affects layers like Dropout and BatchNorm.\n",
    "\n",
    "**Common Layers:**\n",
    "\n",
    "| Layer | Description |\n",
    "|-------|-------------|\n",
    "| `nn.Linear` | Fully connected layer (y = xW^T + b) |\n",
    "| `nn.Conv1d/2d` | Convolutional layers |\n",
    "| `nn.Embedding` | Lookup table for embeddings |\n",
    "| `nn.Dropout` | Randomly zeros elements (only in train mode!) |\n",
    "\n",
    "**Activations:**\n",
    "\n",
    "| Activation | Formula | Use Case |\n",
    "|------------|---------|----------|\n",
    "| ReLU | max(0, x) | Default choice |\n",
    "| GELU | x · Φ(x) | Transformers (BERT, GPT) |\n",
    "| SiLU/Swish | x · σ(x) | Vision, newer models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a00f6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear: torch.Size([32, 10]) -> torch.Size([32, 5])\n",
      "  Weight: torch.Size([5, 10]), Bias: torch.Size([5])\n",
      "Conv2d: torch.Size([32, 3, 224, 224]) -> torch.Size([32, 16, 224, 224])\n",
      "Embedding: torch.Size([32, 50]) -> torch.Size([32, 50, 256])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMMON LAYERS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Linear (Dense) layer\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "x = torch.randn(32, 10)  # batch of 32\n",
    "out = linear(x)\n",
    "print(f\"Linear: {x.shape} -> {out.shape}\")\n",
    "print(f\"  Weight: {linear.weight.shape}, Bias: {linear.bias.shape}\")\n",
    "\n",
    "# Conv2d layer\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "img = torch.randn(32, 3, 224, 224)  # batch of 32 RGB images\n",
    "out = conv(img)\n",
    "print(f\"Conv2d: {img.shape} -> {out.shape}\")\n",
    "\n",
    "# Embedding layer (for NLP)\n",
    "embedding = nn.Embedding(num_embeddings=10000, embedding_dim=256)\n",
    "tokens = torch.randint(0, 10000, (32, 50))  # batch of 32, sequence length 50\n",
    "out = embedding(tokens)\n",
    "print(f\"Embedding: {tokens.shape} -> {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "062ee915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations:\n",
      "  ReLU:   tensor([1.9683, 0.0000, 0.0000, 0.0000, 0.9355])\n",
      "  GELU:   tensor([ 1.9200, -0.1698, -0.1447, -0.0323,  0.7720])\n",
      "  SiLU:   tensor([ 1.7270, -0.2362, -0.2763, -0.2220,  0.6719])\n",
      "  Sigmoid: tensor([0.8774, 0.3268, 0.2420, 0.1021, 0.7182])\n",
      "\n",
      "Train mode: tensor([0., 0., 0., 2., 2., 2., 2., 0., 0., 0.])\n",
      "Eval mode:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ACTIVATIONS AND DROPOUT (train vs eval mode)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "x = torch.randn(5)\n",
    "\n",
    "# Activations\n",
    "print(\"Activations:\")\n",
    "print(f\"  ReLU:   {torch.relu(x)}\")\n",
    "print(f\"  GELU:   {torch.nn.functional.gelu(x)}\")\n",
    "print(f\"  SiLU:   {torch.nn.functional.silu(x)}\")\n",
    "print(f\"  Sigmoid: {torch.sigmoid(x)}\")\n",
    "\n",
    "# Dropout - CRITICAL: behavior changes between train/eval\n",
    "class DropoutDemo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)\n",
    "\n",
    "model = DropoutDemo()\n",
    "x = torch.ones(10)\n",
    "\n",
    "model.train()  # Training mode - dropout active\n",
    "print(f\"\\nTrain mode: {model(x)}\")  # Some zeros\n",
    "\n",
    "model.eval()   # Eval mode - dropout disabled\n",
    "print(f\"Eval mode:  {model(x)}\")  # All ones (scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d3bee",
   "metadata": {},
   "source": [
    "### 3.2 Normalization\n",
    "\n",
    "**Why LayerNorm dominates in transformers:**\n",
    "- BatchNorm normalizes across the batch dimension → requires large batches\n",
    "- LayerNorm normalizes across the feature dimension → works with any batch size\n",
    "- For sequence models where batch size varies, LayerNorm is essential\n",
    "\n",
    "| Normalization | Normalizes Over | Use Case |\n",
    "|---------------|-----------------|----------|\n",
    "| BatchNorm | Batch dimension | CNNs, fixed batch sizes |\n",
    "| LayerNorm | Feature dimension | Transformers, RNNs |\n",
    "| RMSNorm | Feature (no mean) | LLaMA, efficient transformers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef6aa587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d: torch.Size([32, 10]) -> torch.Size([32, 10])\n",
      "  Mean ≈ 0: tensor([ 0.0000e+00,  7.4506e-09, -7.4506e-09], grad_fn=<SliceBackward0>)\n",
      "\n",
      "LayerNorm: torch.Size([32, 50, 10]) -> torch.Size([32, 50, 10])\n",
      "  Mean ≈ 0: -0.000000\n",
      "\n",
      "RMSNorm: torch.Size([32, 50, 10])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NORMALIZATION LAYERS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# BatchNorm - normalizes over batch dimension\n",
    "# Input: (N, C, *) where N=batch, C=channels\n",
    "batch_norm = nn.BatchNorm1d(num_features=10)\n",
    "x = torch.randn(32, 10)  # batch of 32, 10 features\n",
    "out = batch_norm(x)\n",
    "print(f\"BatchNorm1d: {x.shape} -> {out.shape}\")\n",
    "print(f\"  Mean ≈ 0: {out.mean(dim=0)[:3]}\")  # Mean across batch\n",
    "\n",
    "# LayerNorm - normalizes over feature dimensions\n",
    "# Input: (*, normalized_shape)\n",
    "layer_norm = nn.LayerNorm(normalized_shape=10)\n",
    "x = torch.randn(32, 50, 10)  # batch=32, seq_len=50, features=10\n",
    "out = layer_norm(x)\n",
    "print(f\"\\nLayerNorm: {x.shape} -> {out.shape}\")\n",
    "print(f\"  Mean ≈ 0: {out[0, 0].mean():.6f}\")  # Mean across features\n",
    "\n",
    "# RMSNorm (common in LLaMA, needs custom implementation)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # RMS = sqrt(mean(x^2))\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.weight\n",
    "\n",
    "rms_norm = RMSNorm(10)\n",
    "out = rms_norm(torch.randn(32, 50, 10))\n",
    "print(f\"\\nRMSNorm: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba8023",
   "metadata": {},
   "source": [
    "### 3.3 Initialization\n",
    "\n",
    "**This matters more than beginners realize.** Transformers rely on careful initialization for stable training.\n",
    "\n",
    "**Default PyTorch Initialization:**\n",
    "- `nn.Linear`: Kaiming uniform (good for ReLU)\n",
    "- `nn.Embedding`: Normal(0, 1)\n",
    "\n",
    "**Why careful init matters for transformers:**\n",
    "- Too large: exploding activations/gradients\n",
    "- Too small: vanishing signals\n",
    "- Common practice: scale by $\\frac{1}{\\sqrt{d_{model}}}$ or $\\frac{1}{\\sqrt{n_{layers}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a09072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Linear init:\n",
      "  Weight std: 0.0255\n",
      "  Bias mean:  -0.0007\n",
      "\n",
      "Xavier uniform std: 0.0442\n",
      "Kaiming uniform std: 0.0625\n",
      "Normal(0, 0.02) std: 0.0200\n",
      "Zeros bias: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# WEIGHT INITIALIZATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Check default initialization\n",
    "linear = nn.Linear(512, 512)\n",
    "print(f\"Default Linear init:\")\n",
    "print(f\"  Weight std: {linear.weight.std():.4f}\")\n",
    "print(f\"  Bias mean:  {linear.bias.mean():.4f}\")\n",
    "\n",
    "# Common initialization methods\n",
    "layer = nn.Linear(512, 512)\n",
    "\n",
    "# Xavier/Glorot (good for tanh/sigmoid)\n",
    "nn.init.xavier_uniform_(layer.weight)\n",
    "print(f\"\\nXavier uniform std: {layer.weight.std():.4f}\")\n",
    "\n",
    "# Kaiming/He (good for ReLU)\n",
    "nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "print(f\"Kaiming uniform std: {layer.weight.std():.4f}\")\n",
    "\n",
    "# Normal initialization with specific std\n",
    "nn.init.normal_(layer.weight, mean=0, std=0.02)  # GPT-2 style\n",
    "print(f\"Normal(0, 0.02) std: {layer.weight.std():.4f}\")\n",
    "\n",
    "# Zero initialization (for biases, residual connections)\n",
    "nn.init.zeros_(layer.bias)\n",
    "print(f\"Zeros bias: {layer.bias.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5a1a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After custom initialization:\n",
      "  fc1.weight: std = 0.0209\n",
      "  fc2.weight: std = 0.0189\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CUSTOM INITIALIZATION FOR A MODEL\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def init_weights(module):\n",
    "    \"\"\"Custom weight initialization function.\"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        nn.init.ones_(module.weight)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "# Apply to model\n",
    "model = SimpleNet(10, 20, 5)\n",
    "model.apply(init_weights)  # Recursively applies to all modules\n",
    "\n",
    "print(\"After custom initialization:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"  {name}: std = {param.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12be900",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. PyTorch for NLP and Sequence Models\n",
    "\n",
    "*This is where PyTorch becomes your main LLM tool.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e72ba",
   "metadata": {},
   "source": [
    "### 4.1 Embeddings and Vocab\n",
    "\n",
    "`nn.Embedding` is a lookup table that maps integer indices to dense vectors.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `num_embeddings` | Vocabulary size |\n",
    "| `embedding_dim` | Dimension of embedding vectors |\n",
    "| `padding_idx` | Index to zero out (for padding tokens) |\n",
    "\n",
    "**Vocab size effects:**\n",
    "- Larger vocab = more parameters = more memory\n",
    "- GPT-2: 50,257 tokens × 768 dims = ~39M parameters just for embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd1f47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5])\n",
      "Output shape: torch.Size([2, 5, 256])\n",
      "\n",
      "Padding embedding (should be zeros): 0.0\n",
      "Non-padding embedding: -3.7477\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# EMBEDDING LAYER INTERNALS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "vocab_size = 10000\n",
    "embed_dim = 256\n",
    "pad_idx = 0  # Token 0 is padding\n",
    "\n",
    "# Create embedding layer\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=embed_dim,\n",
    "    padding_idx=pad_idx  # This index will always output zeros\n",
    ")\n",
    "\n",
    "# Input: token indices [batch_size, seq_len]\n",
    "tokens = torch.tensor([\n",
    "    [5, 23, 456, 0, 0],    # Sentence 1 (padded)\n",
    "    [12, 34, 56, 78, 0]    # Sentence 2 (padded)\n",
    "])\n",
    "\n",
    "# Output: embeddings [batch_size, seq_len, embed_dim]\n",
    "embedded = embedding(tokens)\n",
    "print(f\"Input shape: {tokens.shape}\")\n",
    "print(f\"Output shape: {embedded.shape}\")\n",
    "\n",
    "# Verify padding is zeroed\n",
    "print(f\"\\nPadding embedding (should be zeros): {embedded[0, 3].sum().item()}\")\n",
    "print(f\"Non-padding embedding: {embedded[0, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edc1694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights tied: True\n",
      "Parameters with tying: 2,822,912\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# WEIGHT TYING (Input/Output embeddings)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# In language models, we often tie input embeddings with output projection\n",
    "# This reduces parameters and improves performance\n",
    "\n",
    "class TiedEmbeddingLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, embed_dim)\n",
    "        # Output projection to vocab (tied with embedding)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # TIE THE WEIGHTS\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # [B, S] -> [B, S, E]\n",
    "        x = self.hidden(x)              # [B, S, E] -> [B, S, H]\n",
    "        x = torch.relu(x)\n",
    "        x = self.output(x)              # [B, S, H] -> [B, S, E]\n",
    "        logits = self.lm_head(x)        # [B, S, E] -> [B, S, V]\n",
    "        return logits\n",
    "\n",
    "model = TiedEmbeddingLM(10000, 256, 512)\n",
    "\n",
    "# Verify weights are the same object\n",
    "print(f\"Weights tied: {model.lm_head.weight is model.embedding.weight}\")\n",
    "\n",
    "# Parameter count reduction\n",
    "params_with_tie = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameters with tying: {params_with_tie:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf8cf4",
   "metadata": {},
   "source": [
    "### 4.2 Attention from Scratch\n",
    "\n",
    "**Important:** Even though PyTorch has `nn.MultiheadAttention`, you should implement it yourself once to understand LLMs.\n",
    "\n",
    "**Attention Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Q (Query):** What we're looking for\n",
    "- **K (Key):** What we match against\n",
    "- **V (Value):** What we retrieve\n",
    "- **Scaling by $\\sqrt{d_k}$:** Prevents softmax saturation with large dot products\n",
    "- **Causal mask:** Prevents attending to future tokens (for autoregressive models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5562d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 8])\n",
      "Attention weights shape: torch.Size([2, 4, 4])\n",
      "Attention weights sum (should be 1): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# SCALED DOT-PRODUCT ATTENTION FROM SCRATCH\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query: [batch, seq_len, d_k]\n",
    "        key:   [batch, seq_len, d_k]\n",
    "        value: [batch, seq_len, d_v]\n",
    "        mask:  [seq_len, seq_len] or None\n",
    "    Returns:\n",
    "        output: [batch, seq_len, d_v]\n",
    "        attention_weights: [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # scores: [batch, seq_len, seq_len]\n",
    "    \n",
    "    # Apply mask (for causal attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test it\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Attention weights sum (should be 1): {weights[0, 0].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a478bccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask (1=attend, 0=block):\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "Causal attention weights for position 2:\n",
      "  tensor([0.1933, 0.1789, 0.6278, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CAUSAL MASK FOR AUTOREGRESSIVE MODELS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal (lower triangular) mask.\"\"\"\n",
    "    # 1s in lower triangle (including diagonal), 0s above\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Visualize the mask\n",
    "seq_len = 5\n",
    "mask = create_causal_mask(seq_len)\n",
    "print(\"Causal mask (1=attend, 0=block):\")\n",
    "print(mask)\n",
    "\n",
    "# Apply causal attention\n",
    "Q = torch.randn(1, seq_len, 8)\n",
    "K = torch.randn(1, seq_len, 8)\n",
    "V = torch.randn(1, seq_len, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "print(f\"\\nCausal attention weights for position 2:\")\n",
    "print(f\"  {weights[0, 2]}\")  # Can only attend to positions 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bb45624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention output: torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# MULTI-HEAD ATTENTION FROM SCRATCH\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # [B, S, D]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head: [B, S, D] -> [B, S, H, D/H] -> [B, H, S, D/H]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention for each head\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, V)  # [B, H, S, D/H]\n",
    "        \n",
    "        # Concatenate heads: [B, H, S, D/H] -> [B, S, H, D/H] -> [B, S, D]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test it\n",
    "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
    "x = torch.randn(2, 10, 64)  # [batch=2, seq_len=10, d_model=64]\n",
    "output = mha(x)\n",
    "print(f\"Multi-head attention output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6fa7f",
   "metadata": {},
   "source": [
    "### 4.3 Transformer Blocks\n",
    "\n",
    "This is the core of GPT, LLaMA, Mistral, and all modern LLMs.\n",
    "\n",
    "**Transformer Block Structure:**\n",
    "```\n",
    "Input\n",
    "  │\n",
    "  ├── Multi-Head Attention ──┐\n",
    "  │                          │ (residual connection)\n",
    "  └──────────────────────────┼──> Add & Norm\n",
    "                             │\n",
    "  ├── Feed-Forward Network ──┐\n",
    "  │                          │ (residual connection)\n",
    "  └──────────────────────────┼──> Add & Norm\n",
    "                             │\n",
    "                           Output\n",
    "```\n",
    "\n",
    "**Pre-norm vs Post-norm:**\n",
    "- **Post-norm (original):** `x + Sublayer(LayerNorm(x))` - harder to train deep models\n",
    "- **Pre-norm (modern):** `x + LayerNorm(Sublayer(x))` - more stable, used in GPT-2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2889c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer block: torch.Size([2, 10, 64]) -> torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TRANSFORMER BLOCK (Pre-norm, GPT-style)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm attention with residual\n",
    "        attn_out = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # Pre-norm FFN with residual\n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output = block(x)\n",
    "print(f\"Transformer block: {x.shape} -> {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8850d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGPT: torch.Size([2, 32]) -> torch.Size([2, 32, 1000])\n",
      "Total parameters: 272,256\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# STACKING TRANSFORMER BLOCKS (Mini GPT)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token + positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm (pre-norm style)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output head\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(T, T, device=idx.device))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create a small GPT model\n",
    "model = MiniGPT(\n",
    "    vocab_size=1000,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "tokens = torch.randint(0, 1000, (2, 32))\n",
    "logits = model(tokens)\n",
    "print(f\"MiniGPT: {tokens.shape} -> {logits.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95764a8f",
   "metadata": {},
   "source": [
    "### 4.4 Tokenization Ecosystem\n",
    "\n",
    "PyTorch relies heavily on Hugging Face tokenizers. Understanding this is essential for working with any pretrained model.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| BPE (Byte-Pair Encoding) | Subword tokenization (GPT-2, GPT-3) |\n",
    "| SentencePiece | Unigram or BPE, handles whitespace (LLaMA, T5) |\n",
    "| `padding` | Add tokens to reach fixed length |\n",
    "| `truncation` | Cut sequences to max length |\n",
    "| `attention_mask` | 1 for real tokens, 0 for padding |\n",
    "\n",
    "**Common Token IDs:**\n",
    "- `[PAD]` or `<pad>`: Padding token\n",
    "- `[CLS]` or `<s>`: Start of sequence (BERT style)\n",
    "- `[SEP]` or `</s>`: End of sequence\n",
    "- `[UNK]` or `<unk>`: Unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "425a6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization example (simulated):\n",
      "Text: 'Hello, how are you?'\n",
      "Tokens: ['Hello', ',', ' how', ' are', ' you', '?']\n",
      "Token IDs: [15496, 11, 703, 389, 345, 30]\n",
      "With padding (max_length=10): [15496, 11, 703, 389, 345, 30, 50256, 50256, 50256, 50256]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# HUGGING FACE TOKENIZERS (Example with GPT-2)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Note: Requires `pip install transformers`\n",
    "# Uncomment to run:\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Basic tokenization\n",
    "# text = \"Hello, how are you?\"\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# # Get token IDs\n",
    "# token_ids = tokenizer.encode(text)\n",
    "# print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# # Full encoding with attention mask\n",
    "# encoded = tokenizer(\n",
    "#     text,\n",
    "#     padding=\"max_length\",      # Pad to max_length\n",
    "#     max_length=10,             # Maximum sequence length\n",
    "#     truncation=True,           # Truncate if longer\n",
    "#     return_tensors=\"pt\"        # Return PyTorch tensors\n",
    "# )\n",
    "# print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "# print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# Simulated example for demonstration:\n",
    "print(\"Tokenization example (simulated):\")\n",
    "print(\"Text: 'Hello, how are you?'\")\n",
    "print(\"Tokens: ['Hello', ',', ' how', ' are', ' you', '?']\")\n",
    "print(\"Token IDs: [15496, 11, 703, 389, 345, 30]\")\n",
    "print(\"With padding (max_length=10): [15496, 11, 703, 389, 345, 30, 50256, 50256, 50256, 50256]\")\n",
    "print(\"Attention mask: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c785d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training Language Models\n",
    "\n",
    "*Now everything converges. This is identical in theory to TensorFlow, but loop-driven.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a265d",
   "metadata": {},
   "source": [
    "### 5.1 Autoregressive Training\n",
    "\n",
    "**Next-token prediction** is the core of language model training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Shifting labels:** Input `[A, B, C, D]` → Targets `[B, C, D, E]`\n",
    "- **Teacher forcing:** Use ground truth as input during training\n",
    "- **Masking padding:** Don't compute loss on padding tokens\n",
    "\n",
    "```\n",
    "Input:   [START, The, cat, sat]\n",
    "Target:  [The,   cat, sat, END]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc27bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full sequence: tensor([ 1, 45, 23, 67, 89,  2,  0,  0])\n",
      "Input:         tensor([ 1, 45, 23, 67, 89,  2,  0])\n",
      "Target:        tensor([45, 23, 67, 89,  2,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# AUTOREGRESSIVE TRAINING SETUP\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def prepare_lm_batch(token_ids, pad_id=-100):\n",
    "    \"\"\"\n",
    "    Prepare input and target for language model training.\n",
    "    \n",
    "    Args:\n",
    "        token_ids: [batch, seq_len] - full sequences including BOS/EOS\n",
    "        pad_id: Value to ignore in loss computation (-100 for CrossEntropyLoss)\n",
    "    \n",
    "    Returns:\n",
    "        inputs: [batch, seq_len-1] - everything except last token\n",
    "        targets: [batch, seq_len-1] - everything except first token\n",
    "    \"\"\"\n",
    "    inputs = token_ids[:, :-1]   # All except last\n",
    "    targets = token_ids[:, 1:]   # All except first (shifted by 1)\n",
    "    return inputs, targets\n",
    "\n",
    "# Example\n",
    "# Full sequence: [BOS, The, cat, sat, on, EOS, PAD, PAD]\n",
    "sequence = torch.tensor([\n",
    "    [1, 45, 23, 67, 89, 2, 0, 0],  # Sentence 1\n",
    "    [1, 12, 34, 56, 2, 0, 0, 0],   # Sentence 2 (shorter)\n",
    "])\n",
    "\n",
    "inputs, targets = prepare_lm_batch(sequence)\n",
    "print(f\"Full sequence: {sequence[0]}\")\n",
    "print(f\"Input:         {inputs[0]}\")\n",
    "print(f\"Target:        {targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "817b6b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined - would train on real data\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMPLETE LM TRAINING LOOP\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def train_language_model(model, dataloader, optimizer, device, num_epochs=3, pad_id=0):\n",
    "    \"\"\"\n",
    "    Complete training loop for a language model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # Ignore padding in loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Prepare batch\n",
    "            token_ids = batch[0].to(device)\n",
    "            inputs, targets = prepare_lm_batch(token_ids)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)  # [B, S, V]\n",
    "            \n",
    "            # Reshape for loss: [B*S, V] and [B*S]\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "print(\"Training function defined - would train on real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5b1bd",
   "metadata": {},
   "source": [
    "### 5.2 Mixed Precision and Performance\n",
    "\n",
    "**This is mandatory for LLMs.** Mixed precision uses FP16 for most operations while keeping FP32 for stability.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `autocast` | Automatically casts operations to FP16 |\n",
    "| `GradScaler` | Scales gradients to prevent underflow |\n",
    "| FP16 | Half precision (16-bit) - 2x memory savings |\n",
    "| FP32 | Full precision (32-bit) - more stable |\n",
    "\n",
    "**When FP16 breaks:**\n",
    "- Very small gradients underflow to zero\n",
    "- Solution: GradScaler multiplies loss, then unscales gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b6261db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision training pattern defined\n",
      "Memory savings: ~50% VRAM reduction\n",
      "Speed improvement: 2-3x on modern GPUs with Tensor Cores\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# MIXED PRECISION TRAINING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_with_mixed_precision(model, dataloader, optimizer, device, num_epochs=1):\n",
    "    \"\"\"\n",
    "    Training loop with automatic mixed precision (AMP).\n",
    "    \"\"\"\n",
    "    scaler = GradScaler()  # Gradient scaler for stability\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast (FP16 where safe)\n",
    "            with autocast():\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass with scaled gradients\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Unscale and step\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Note: autocast works best on CUDA devices\n",
    "# On CPU/MPS, it may fall back to FP32\n",
    "print(\"Mixed precision training pattern defined\")\n",
    "print(\"Memory savings: ~50% VRAM reduction\")\n",
    "print(\"Speed improvement: 2-3x on modern GPUs with Tensor Cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8dfc0",
   "metadata": {},
   "source": [
    "### 5.3 Gradient Control\n",
    "\n",
    "**Critical for stability** - without proper gradient control, LLM training will explode or vanish.\n",
    "\n",
    "**Key Techniques:**\n",
    "\n",
    "| Technique | When to Use |\n",
    "|-----------|-------------|\n",
    "| Gradient Clipping | Always for transformers |\n",
    "| Gradient Accumulation | When batch size limited by memory |\n",
    "| Learning Rate Warmup | First few thousand steps |\n",
    "\n",
    "**Common Clip Values:**\n",
    "- `max_norm=1.0` - standard for transformers\n",
    "- `max_norm=0.5` - more conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34ac8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm before clipping: 0.4729\n",
      "Gradient norm after clipping:  0.4729\n",
      "Max norm allowed: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# GRADIENT CLIPPING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "model = SimpleNet(10, 20, 5)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate a forward-backward pass\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randint(0, 5, (32,))\n",
    "loss = criterion(model(x), y)\n",
    "loss.backward()\n",
    "\n",
    "# Check gradient norms before clipping\n",
    "total_norm_before = torch.sqrt(\n",
    "    sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None)\n",
    ")\n",
    "print(f\"Gradient norm before clipping: {total_norm_before:.4f}\")\n",
    "\n",
    "# Gradient clipping (MUST be called after backward, before step)\n",
    "max_norm = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "# Check after clipping\n",
    "total_norm_after = torch.sqrt(\n",
    "    sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None)\n",
    ")\n",
    "print(f\"Gradient norm after clipping:  {total_norm_after:.4f}\")\n",
    "print(f\"Max norm allowed: {max_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82bd9928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation pattern defined\n",
      "Use when: GPU memory limits batch size, but you need larger effective batches\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# GRADIENT ACCUMULATION (for limited GPU memory)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def train_with_gradient_accumulation(\n",
    "    model, dataloader, optimizer, device,\n",
    "    accumulation_steps=4  # Effective batch = real_batch * accumulation_steps\n",
    "):\n",
    "    \"\"\"\n",
    "    Gradient accumulation allows larger effective batch sizes.\n",
    "    \n",
    "    If batch_size=8 and accumulation_steps=4, effective batch = 32\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer.zero_grad()  # Zero once at start\n",
    "    \n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Backward (gradients accumulate automatically!)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Only step every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # Optional: gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            print(f\"Step {(i+1) // accumulation_steps}, Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "\n",
    "print(\"Gradient accumulation pattern defined\")\n",
    "print(\"Use when: GPU memory limits batch size, but you need larger effective batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb4600",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Generation and Inference\n",
    "\n",
    "*This is where models become useful.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8bcdf",
   "metadata": {},
   "source": [
    "### 6.1 Decoding Mechanics\n",
    "\n",
    "All decoding methods start with logits and use softmax to get probabilities.\n",
    "\n",
    "**Decoding Strategies:**\n",
    "\n",
    "| Strategy | Description | Temperature |\n",
    "|----------|-------------|-------------|\n",
    "| Greedy | Always pick highest probability | N/A |\n",
    "| Temperature | Scale logits before softmax | 0.1-2.0 |\n",
    "| Top-k | Sample from top k tokens | Usually with temp |\n",
    "| Top-p (nucleus) | Sample from smallest set with cumulative prob ≥ p | Usually with temp |\n",
    "| Repetition penalty | Reduce probability of already-generated tokens | N/A |\n",
    "\n",
    "**Key Formula:**\n",
    "$$P(x_i) = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}$$\n",
    "\n",
    "Where $T$ is temperature:\n",
    "- $T < 1$: More confident (sharper distribution)\n",
    "- $T > 1$: More random (flatter distribution)\n",
    "- $T = 1$: Original distribution\n",
    "\n",
    "**Stopping Conditions:**\n",
    "- `max_length`: Hard limit on total sequence length\n",
    "- `max_new_tokens`: Limit on generated tokens only\n",
    "- `eos_token_id`: Stop when EOS token is generated\n",
    "- `min_length`: Don't stop before minimum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7419e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.0000,  1.5000,  0.5000,  0.1000, -1.0000]])\n",
      "Greedy:      0\n",
      "Temp=0.5:    1\n",
      "Temp=2.0:    1\n",
      "Top-k (k=2): 1\n",
      "Top-p (p=0.9): 0\n",
      "\n",
      "Original logits: tensor([[ 2.0000,  1.5000,  0.5000,  0.1000, -1.0000]])\n",
      "After rep penalty (tokens 0,1 seen): tensor([[ 1.3333,  1.0000,  0.5000,  0.1000, -1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DECODING STRATEGIES\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def greedy_decode(logits):\n",
    "    \"\"\"Always pick the highest probability token.\"\"\"\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def temperature_sample(logits, temperature=1.0):\n",
    "    \"\"\"Sample with temperature scaling.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = F.softmax(scaled_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "def top_k_sample(logits, k=50, temperature=1.0):\n",
    "    \"\"\"Sample from top-k tokens only.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Zero out everything except top-k\n",
    "    top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n",
    "    \n",
    "    # Sample from top-k\n",
    "    probs = F.softmax(top_k_logits, dim=-1)\n",
    "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    # Map back to vocabulary indices\n",
    "    return top_k_indices.gather(-1, sampled_idx).squeeze(-1)\n",
    "\n",
    "def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"Sample from smallest set with cumulative probability >= p.\"\"\"\n",
    "    scaled_logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(scaled_logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative prob > p\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = False\n",
    "    \n",
    "    # Set removed tokens to -inf\n",
    "    sorted_logits[sorted_indices_to_remove] = float('-inf')\n",
    "    \n",
    "    # Sample\n",
    "    probs = F.softmax(sorted_logits, dim=-1)\n",
    "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return sorted_indices.gather(-1, sampled_idx).squeeze(-1)\n",
    "\n",
    "def apply_repetition_penalty(logits, generated_ids, penalty=1.2):\n",
    "    \"\"\"\n",
    "    Apply repetition penalty to already-generated tokens.\n",
    "    penalty > 1.0: discourage repetition\n",
    "    penalty < 1.0: encourage repetition (rarely used)\n",
    "    \"\"\"\n",
    "    for token_id in set(generated_ids.tolist()):\n",
    "        # If logit is positive, divide by penalty (reduce)\n",
    "        # If logit is negative, multiply by penalty (make more negative)\n",
    "        if logits[0, token_id] > 0:\n",
    "            logits[0, token_id] /= penalty\n",
    "        else:\n",
    "            logits[0, token_id] *= penalty\n",
    "    return logits\n",
    "\n",
    "# Demonstrate with example logits\n",
    "logits = torch.tensor([[2.0, 1.5, 0.5, 0.1, -1.0]])  # 5 tokens\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(f\"Greedy:      {greedy_decode(logits).item()}\")\n",
    "print(f\"Temp=0.5:    {temperature_sample(logits, 0.5).item()}\")\n",
    "print(f\"Temp=2.0:    {temperature_sample(logits, 2.0).item()}\")\n",
    "print(f\"Top-k (k=2): {top_k_sample(logits, k=2).item()}\")\n",
    "print(f\"Top-p (p=0.9): {top_p_sample(logits, p=0.9).item()}\")\n",
    "\n",
    "# Demonstrate repetition penalty\n",
    "generated = torch.tensor([0, 0, 1])  # Token 0 appeared twice\n",
    "logits_copy = logits.clone()\n",
    "penalized = apply_repetition_penalty(logits_copy, generated, penalty=1.5)\n",
    "print(f\"\\nOriginal logits: {logits}\")\n",
    "print(f\"After rep penalty (tokens 0,1 seen): {penalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d69bd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function defined\n",
      "Usage: generate(model, prompt_ids, max_new_tokens=100, temperature=0.8, top_p=0.9)\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMPLETE GENERATION FUNCTION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    prompt_ids,          # [batch, prompt_len]\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    eos_token_id=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressive generation with various sampling strategies.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = prompt_ids.clone()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get logits for last position only\n",
    "        logits = model(generated)[:, -1, :]  # [batch, vocab]\n",
    "        \n",
    "        # Apply sampling strategy\n",
    "        if top_k is not None:\n",
    "            next_token = top_k_sample(logits, k=top_k, temperature=temperature)\n",
    "        elif top_p is not None:\n",
    "            next_token = top_p_sample(logits, p=top_p, temperature=temperature)\n",
    "        elif temperature != 1.0:\n",
    "            next_token = temperature_sample(logits, temperature=temperature)\n",
    "        else:\n",
    "            next_token = greedy_decode(logits)\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "        # Stop if EOS\n",
    "        if eos_token_id is not None and (next_token == eos_token_id).all():\n",
    "            break\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"Generation function defined\")\n",
    "print(\"Usage: generate(model, prompt_ids, max_new_tokens=100, temperature=0.8, top_p=0.9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613c886",
   "metadata": {},
   "source": [
    "### 6.2 KV Caching\n",
    "\n",
    "**This separates toy LMs from real ones.** Without KV caching, generation is O(n²) in sequence length.\n",
    "\n",
    "**The Problem:**\n",
    "- Without cache: At each step, recompute attention for ALL previous tokens\n",
    "- With cache: Store K and V from previous steps, only compute for new token\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Step 1: Compute K₁, V₁ for token 1 → Store in cache\n",
    "Step 2: Compute K₂, V₂ for token 2 → Append to cache, attend to [K₁,K₂], [V₁,V₂]\n",
    "Step 3: Compute K₃, V₃ for token 3 → Append to cache, attend to [K₁,K₂,K₃], [V₁,V₂,V₃]\n",
    "```\n",
    "\n",
    "**Speed improvement:** From O(n²) to O(n) for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28d7a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: input torch.Size([1, 10, 64]), cache K shape: torch.Size([1, 4, 10, 16])\n",
      "After 1 token: cache K shape: torch.Size([1, 4, 11, 16])\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# KV CACHE IMPLEMENTATION\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class CachedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with KV cache for efficient generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, S, D] - if cached, S=1 (only new token)\n",
    "            kv_cache: tuple of (cached_k, cached_v) or None\n",
    "            use_cache: whether to return updated cache\n",
    "        \"\"\"\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for new tokens\n",
    "        Q = self.W_q(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, S, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate with cache if exists\n",
    "        if kv_cache is not None:\n",
    "            cached_k, cached_v = kv_cache\n",
    "            K = torch.cat([cached_k, K], dim=2)  # [B, H, S_cached+S, D]\n",
    "            V = torch.cat([cached_v, V], dim=2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, self.d_model)\n",
    "        out = self.W_o(out)\n",
    "        \n",
    "        if use_cache:\n",
    "            return out, (K, V)  # Return updated cache\n",
    "        return out\n",
    "\n",
    "# Demonstrate cache shapes\n",
    "attn = CachedMultiHeadAttention(64, 4)\n",
    "x = torch.randn(1, 10, 64)  # Initial prompt\n",
    "\n",
    "# First forward (no cache)\n",
    "out, cache = attn(x, kv_cache=None, use_cache=True)\n",
    "print(f\"Initial: input {x.shape}, cache K shape: {cache[0].shape}\")\n",
    "\n",
    "# Second forward (with cache, only 1 new token)\n",
    "new_token = torch.randn(1, 1, 64)\n",
    "out, cache = attn(new_token, kv_cache=cache, use_cache=True)\n",
    "print(f\"After 1 token: cache K shape: {cache[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb432b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ecosystem and Real-world PyTorch\n",
    "\n",
    "*This is why PyTorch dominates LLMs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854ca22",
   "metadata": {},
   "source": [
    "### 7.1 Hugging Face Transformers\n",
    "\n",
    "**Do not treat this as magic. Read the source.**\n",
    "\n",
    "Hugging Face provides pretrained models, tokenizers, and training utilities that are the standard in NLP.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| `AutoModel` | Automatically loads the right model architecture |\n",
    "| `AutoTokenizer` | Automatically loads the right tokenizer |\n",
    "| `AutoConfig` | Model configuration (hidden size, layers, etc.) |\n",
    "| `Trainer` | High-level training API |\n",
    "\n",
    "**Model Loading Pattern:**\n",
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af69ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face patterns defined as template\n",
      "Install with: pip install transformers\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# HUGGING FACE TRANSFORMERS PATTERNS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Note: Requires `pip install transformers`\n",
    "# This is pseudocode/template - uncomment to run with transformers installed\n",
    "\n",
    "\"\"\"\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ─── BASIC LOADING ───\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ─── TOKENIZATION ───\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# Returns: {'input_ids': tensor, 'attention_mask': tensor}\n",
    "\n",
    "# ─── INFERENCE ───\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # outputs.logits: [batch, seq_len, vocab_size]\n",
    "    # outputs.past_key_values: KV cache for generation\n",
    "\n",
    "# ─── GENERATION ───\n",
    "generated = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "print(tokenizer.decode(generated[0]))\n",
    "\n",
    "# ─── FINE-TUNING ───\n",
    "# Just use normal PyTorch training loop!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# ... training loop ...\n",
    "\n",
    "# ─── ACCESS CONFIG ───\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Num heads: {model.config.num_attention_heads}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Hugging Face patterns defined as template\")\n",
    "print(\"Install with: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42693d17",
   "metadata": {},
   "source": [
    "### 7.2 Model Saving and Loading\n",
    "\n",
    "This replaces TensorFlow's SavedModel.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Method | What it saves | Use Case |\n",
    "|--------|---------------|----------|\n",
    "| `state_dict` | Only weights | Recommended for most cases |\n",
    "| `torch.save(model)` | Entire model | Quick prototyping (not portable) |\n",
    "| Checkpoints | Weights + optimizer + epoch | Resume training |\n",
    "\n",
    "**Device-agnostic Loading:**\n",
    "Always use `map_location` when loading to avoid device mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99fcf912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dict loaded successfully\n",
      "Device-agnostic loading successful\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# MODEL SAVING AND LOADING\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "model = SimpleNet(10, 20, 5)\n",
    "\n",
    "# ─── METHOD 1: Save state_dict (RECOMMENDED) ───\n",
    "# Save\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load\n",
    "model_new = SimpleNet(10, 20, 5)  # Must create model first\n",
    "model_new.load_state_dict(torch.load('model_weights.pth'))\n",
    "print(\"State dict loaded successfully\")\n",
    "\n",
    "# ─── METHOD 2: Device-agnostic loading ───\n",
    "# Save on GPU, load on CPU (or vice versa)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load to specific device\n",
    "device = torch.device('cpu')\n",
    "model_new.load_state_dict(\n",
    "    torch.load('model_weights.pth', map_location=device)\n",
    ")\n",
    "print(\"Device-agnostic loading successful\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45268fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved: epoch 5, loss 0.2340\n",
      "Resumed from epoch 5, loss 0.2340\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# FULL CHECKPOINT (for resuming training)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    \"\"\"Save complete training state.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved: epoch {epoch}, loss {loss:.4f}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, path, device='cpu'):\n",
    "    \"\"\"Load complete training state.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Example usage\n",
    "model = SimpleNet(10, 20, 5)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Save\n",
    "save_checkpoint(model, optimizer, epoch=5, loss=0.234, path='checkpoint.pth')\n",
    "\n",
    "# Load\n",
    "loaded_epoch, loaded_loss = load_checkpoint(model, optimizer, 'checkpoint.pth')\n",
    "print(f\"Resumed from epoch {loaded_epoch}, loss {loaded_loss:.4f}\")\n",
    "\n",
    "# Clean up\n",
    "os.remove('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca13b12",
   "metadata": {},
   "source": [
    "### 7.3 Distributed Training Concepts\n",
    "\n",
    "**You don't need to implement this yet, just understand it.**\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "| Approach | Description | Use Case |\n",
    "|----------|-------------|----------|\n",
    "| `DataParallel` (DP) | Split batch across GPUs, single process | Quick prototyping |\n",
    "| `DistributedDataParallel` (DDP) | One process per GPU, synchronized | Production training |\n",
    "| FSDP | Shard model + gradients + optimizer | Very large models |\n",
    "\n",
    "**Why DDP is preferred over DP:**\n",
    "- DP has GIL bottleneck (Python's Global Interpreter Lock)\n",
    "- DDP has better GPU utilization\n",
    "- DDP scales better to multiple nodes\n",
    "\n",
    "**Gradient Synchronization:**\n",
    "- After backward pass, gradients are averaged across all processes\n",
    "- All processes have identical gradients before optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9caad665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed training concepts:\n",
      "• DataParallel: Easy but limited (GIL bottleneck)\n",
      "• DDP: Production standard (one process per GPU)\n",
      "• FSDP: For models larger than single GPU memory\n",
      "\n",
      "Launch DDP training with: torchrun --nproc_per_node=N train.py\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DISTRIBUTED TRAINING PATTERNS (Conceptual)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# These are templates - actual distributed training requires proper setup\n",
    "\n",
    "\"\"\"\n",
    "# ─── DATA PARALLEL (Simple, but limited) ───\n",
    "model = nn.DataParallel(model)\n",
    "# Automatically splits batch across available GPUs\n",
    "# But: single process, GIL bottleneck\n",
    "\n",
    "# ─── DISTRIBUTED DATA PARALLEL (Production) ───\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Initialize process group\n",
    "dist.init_process_group(backend='nccl')\n",
    "\n",
    "# Wrap model\n",
    "local_rank = int(os.environ['LOCAL_RANK'])\n",
    "model = model.to(local_rank)\n",
    "model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "# Use DistributedSampler for data\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "sampler = DistributedSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler)\n",
    "\n",
    "# Training loop is the same!\n",
    "# DDP automatically synchronizes gradients\n",
    "\n",
    "# Launch with:\n",
    "# torchrun --nproc_per_node=4 train.py\n",
    "\n",
    "# ─── FSDP (For very large models) ───\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "model = FSDP(model)\n",
    "# Shards model parameters, gradients, and optimizer states\n",
    "# Enables training models that don't fit on single GPU\n",
    "\"\"\"\n",
    "\n",
    "print(\"Distributed training concepts:\")\n",
    "print(\"• DataParallel: Easy but limited (GIL bottleneck)\")\n",
    "print(\"• DDP: Production standard (one process per GPU)\")\n",
    "print(\"• FSDP: For models larger than single GPU memory\")\n",
    "print(\"\\nLaunch DDP training with: torchrun --nproc_per_node=N train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4d55f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Notes\n",
    "\n",
    "### 8.1 Common Questions & Answers\n",
    "\n",
    "**Q: How does PyTorch build graphs compared to TensorFlow?**\n",
    "\n",
    "PyTorch uses **dynamic graphs (define-by-run)** — the computation graph is built on-the-fly during execution. Each forward pass creates a new graph, which is immediately used for backprop and then discarded. TensorFlow (1.x) used **static graphs (define-then-run)** — you build the entire graph first, then execute it in a session. TF 2.x added eager mode (like PyTorch), but `@tf.function` still compiles to static graphs for performance. PyTorch's approach is more Pythonic and easier to debug; TensorFlow's static graphs enable more optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why do LLMs use decoder-only transformers?**\n",
    "\n",
    "Because language modeling is **autoregressive** — predicting the next token given all previous tokens. Decoder-only architectures use **causal masking** to prevent attending to future tokens, which is exactly what you need for generation. Encoder-decoder (like T5) is for sequence-to-sequence tasks (translation, summarization) where you have a complete input. Encoder-only (like BERT) is for understanding tasks (classification, NER) where you need bidirectional context. For pure generation (GPT, LLaMA, Claude), decoder-only is simpler and scales better.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why are logits passed to loss functions (not softmax)?**\n",
    "\n",
    "**Numerical stability.** `CrossEntropyLoss` internally computes `log_softmax(logits)`, which uses the log-sum-exp trick to avoid overflow/underflow:\n",
    "\n",
    "$$\\log(\\text{softmax}(x_i)) = x_i - \\log\\sum_j e^{x_j}$$\n",
    "\n",
    "If you pass probabilities (after softmax), taking `log` of very small values causes numerical issues. Also, computing softmax then log is redundant work — `log_softmax` fuses these operations efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why is LayerNorm used instead of BatchNorm in transformers?**\n",
    "\n",
    "**BatchNorm** normalizes across the batch dimension — it computes mean/variance over all samples for each feature. This breaks with:\n",
    "- **Variable sequence lengths** (different positions shouldn't share statistics)\n",
    "- **Small batches** (unstable statistics)\n",
    "- **Inference** (needs running statistics from training)\n",
    "\n",
    "**LayerNorm** normalizes across the feature dimension for each sample independently. Each token gets normalized by its own statistics, making it batch-size agnostic and suitable for autoregressive generation where batch size is often 1.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why does mixed precision require scaling?**\n",
    "\n",
    "**FP16 has limited range** — max value ~65,504, min positive ~6e-8. Gradients during backprop can easily underflow to zero (vanishing gradients) or overflow to inf. The **GradScaler** multiplies the loss by a large factor (e.g., 1024) before `.backward()`, which scales up all gradients proportionally. After backward, it unscales before `optimizer.step()`. If gradients overflow (inf/nan), it skips the update and reduces the scale factor. This keeps gradients in FP16's representable range while maintaining numerical accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why is attention O(n²)?**\n",
    "\n",
    "The attention formula computes **every query against every key**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "For sequence length $n$:\n",
    "- $QK^T$ is $[n \\times d] \\cdot [d \\times n] = [n \\times n]$ — that's $n^2$ dot products\n",
    "- Softmax over $n \\times n$ matrix\n",
    "- Multiply by $V$: $[n \\times n] \\cdot [n \\times d]$\n",
    "\n",
    "Both compute and memory are $O(n^2)$. This is why long-context models need optimizations like FlashAttention (memory-efficient), sliding window attention (sparse), or linear attention approximations.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2 Quick Reference Card\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        PYTORCH ESSENTIALS CHEATSHEET                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ TENSORS                                                                     │\n",
    "│   torch.tensor([1,2,3])           # Create tensor                          │\n",
    "│   x.to(device)                    # Move to device                         │\n",
    "│   x.requires_grad_(True)          # Enable gradients                       │\n",
    "│   x.detach()                      # Remove from graph                      │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ TRAINING LOOP                                                               │\n",
    "│   outputs = model(inputs)         # Forward                                │\n",
    "│   loss = criterion(outputs, y)    # Loss                                   │\n",
    "│   loss.backward()                 # Backward                               │\n",
    "│   optimizer.step()                # Update                                 │\n",
    "│   optimizer.zero_grad()           # Zero grads                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ MODEL MODES                                                                 │\n",
    "│   model.train()                   # Training (dropout ON)                  │\n",
    "│   model.eval()                    # Evaluation (dropout OFF)               │\n",
    "│   with torch.no_grad():           # No gradient tracking                   │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ SAVING/LOADING                                                              │\n",
    "│   torch.save(model.state_dict(), 'model.pth')                              │\n",
    "│   model.load_state_dict(torch.load('model.pth', map_location=device))      │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ MIXED PRECISION                                                             │\n",
    "│   scaler = GradScaler()                                                    │\n",
    "│   with autocast():                                                         │\n",
    "│       loss = model(x)                                                      │\n",
    "│   scaler.scale(loss).backward()                                            │\n",
    "│   scaler.step(optimizer)                                                   │\n",
    "│   scaler.update()                                                          │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│ GRADIENT CONTROL                                                            │\n",
    "│   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)         │\n",
    "│   loss = loss / accumulation_steps  # For gradient accumulation            │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813b19e",
   "metadata": {},
   "source": [
    "## 9. PyTorch Examples\n",
    "\n",
    "*Practical examples putting everything together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1063a2",
   "metadata": {},
   "source": [
    "### 9.1 Basic Neural Network Training\n",
    "\n",
    "A complete example building and training a simple fully-connected neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dda4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.409788131713867\n",
      "Epoch 2/10, Loss: 1.7556540966033936\n",
      "Epoch 3/10, Loss: 1.3171563148498535\n",
      "Epoch 4/10, Loss: 0.7755150198936462\n",
      "Epoch 5/10, Loss: 0.4362756907939911\n",
      "Epoch 6/10, Loss: 0.1316525787115097\n",
      "Epoch 7/10, Loss: 0.07957600057125092\n",
      "Epoch 8/10, Loss: 0.05105067417025566\n",
      "Epoch 9/10, Loss: 0.05781383067369461\n",
      "Epoch 10/10, Loss: 0.03630366548895836\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\" \n",
    "Neural Networks with torch.nn\n",
    "The torch.nn module provides a set of tools to build and train neural networks.\n",
    "\n",
    "Building a Model (Class-based Approach)\n",
    "\n",
    "You define a neural network by subclassing torch.nn.Module.\n",
    "\n",
    "what is a FULLY CONNECTED LAYER?: a fully connected layer is a layer in a neural network where each neuron in the layer is connected to every neuron in the previous layer.\n",
    "from tensorflow we know that the dense layer is a fully connected layer.\n",
    "\n",
    "Forward Pass: The forward pass is the process of passing the input data through the layers of the neural network and computing the output.\n",
    "- NOTE a backward pass is the process of computing the gradients of the loss function with respect to the model parameters. not the same as the forward pass.\n",
    "\n",
    "The RELU is a non-linear activation function that is used to introduce non-linearity into the network. see TF notes for more details.\n",
    "\n",
    "EX:\n",
    "in our example we have 2 layers, the first layer has 784 input neurons and 128 output neurons, and the second layer has 128 input neurons and 10 output neurons.\n",
    "in the forward pass we pass the input data through the first layer, apply the ReLU activation function what relu dose in our case is lets us activate the neurons based on the input, \n",
    "and then pass the output through the second layer and return that as the output of the network.\n",
    "\n",
    "ReLU (Rectified Linear Unit) sets all negative values to zero and keeps positive values unchanged.\n",
    "In a simple NN, it adds non-linearity, helping the model learn complex patterns instead of just straight lines. another one is signmoid (see tf)\n",
    "it basically tells the NN what input values to keep and what to ignore. i.e what is important and what is not.\n",
    "Mathematically:\n",
    "ReLU(x)=max⁡(0,x)\n",
    "ReLU(x)=max(0,x)\n",
    "✅ Keeps positives → same\n",
    "✅ Turns negatives → 0\n",
    "\"\"\"\n",
    "import torch.nn as nn # for neural networks\n",
    "import torch.optim as optim # for optimizers\n",
    "\n",
    "class SimpleNN(nn.Module): # inherit from nn.Module\n",
    "    def __init__(self): \n",
    "        super(SimpleNN, self).__init__() # call the parent class constructor to initialize the module\n",
    "        self.fc1 = nn.Linear(784, 128)  # Fully connected layer (input 784, output 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Fully connected layer (input 128, output 10)\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = torch.relu(self.fc1(x))  # ReLU activation function \n",
    "        x = self.fc2(x) # output layer takes input from previous layer\n",
    "        return x # return the output of the network\n",
    "\n",
    "# Model instantiation\n",
    "model = SimpleNN()\n",
    "\n",
    "\"\"\" \n",
    "Training a Model\n",
    "\n",
    "Training a model involves the following steps:\n",
    "\n",
    "Define the loss function (e.g., nn.CrossEntropyLoss). this is used for multi-class classification problems i.e predicting numbers from 0 to 9 for ex the loss function is used to compute the difference between the predicted output and the target output help us measure how well the model is performing.\n",
    "Define an optimizer (e.g., optim.SGD or optim.Adam). this is used to update the model parameters based on the gradients computed in the backward pass. basically we use the optimizer to update the weights and biases of the network in the direction that reduces the loss function.\n",
    "Perform the forward pass and compute the loss. this is used to compute the difference between the predicted output and the target output help us measure how well the model is performing.\n",
    "Backpropagate to compute gradients. this is used to compute the gradients of the loss function with respect to the model parameters. after we compute the gradients we can update the weights and biases of the network in the direction that reduces the loss function.\n",
    "Update weights using the optimizer. this is used to update the model parameters based on the gradients computed in the backward pass. basically we use the optimizer to update the weights and biases of the network in the direction that reduces the loss function.\n",
    "\n",
    "Backwards pass vs Backpropagation: \n",
    "Backpropagation = the method (the algorithm for calculating gradients).\n",
    "Backward pass = the action (the model doing the gradient calculation step during training).\n",
    "\n",
    "Simple Timeline in Training:\n",
    "Forward pass → compute output.\n",
    "Compute loss.\n",
    "Backward pass → run backpropagation → compute gradients.\n",
    "Optimizer step → update weights.\n",
    "\n",
    "# NOTE: a epoch is one complete pass through the entire training dataset so here we have 10 epochs meaning we will pass through the entire training dataset (X, y) 10 times.\n",
    "\"\"\"\n",
    "# NOTE: you random data maches the model meaning they are the same shape, so you can use any data you want, but in a real world example you would use real data.\n",
    "# Create 1000 samples, each with 10 features a feature is a column in the input data that represents a specific characteristic of the data like e.g. height, weight, age, etc.\n",
    "X = torch.randn(1000, 784)  # inputs (features)\n",
    "y = torch.randint(0, 10, (1000,))  # outputs (labels) - for binary classification (0 or 1)\n",
    "# Wrap into a dataset\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "# Create a trainloader\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs): # for each epoch\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(data)  # Forward pass using the model above\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update weights\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")  # Print loss for each epoch\n",
    "# we should see the loss decrease as we train the model\n",
    "\n",
    "\"\"\" \n",
    "Datasets and DataLoader\n",
    "PyTorch provides utilities for loading and batching data using torch.utils.data.Dataset and DataLoader.\n",
    "\n",
    "Creating a Custom Dataset\n",
    "- A dataset is a class that inherits from torch.utils.data.Dataset and implements the __len__ and __getitem__ methods.\n",
    "- in torch a dataset is a class that represents a collection of data samples. it is used to load and preprocess the data for training and testing the model.\n",
    "\n",
    "### what we achove here is we can load the data in batches of 64 samples and shuffle the data so that we dont get the same data every time we train the model.\n",
    "\n",
    "- you can also use Predefined Datasets (e.g., MNIST, CIFAR)\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_data = torch.randn(1000, 784)  # Random training data with 1000 samples and 784 features\n",
    "train_labels = torch.randint(0, 10, (1000,))  # Random training labels numbers from 0 to 9\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Example usage\n",
    "train_dataset = CustomDataset(train_data, train_labels) # this is the dataset we created above\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # this is the dataloader that loads the dataset in batches of 64 samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e045349",
   "metadata": {},
   "source": [
    "### 9.2 Transformer Language Model on MPS\n",
    "\n",
    "Training a transformer-based language model on Apple Silicon using MPS acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f7610a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to mps\n",
      "\n",
      "Training for 20 iterations to test performance:\n",
      "Iteration 1, Loss: 9.3868\n",
      "Iteration 2, Loss: 9.0019\n",
      "Iteration 3, Loss: 8.4839\n",
      "Iteration 4, Loss: 8.0962\n",
      "Iteration 5, Loss: 7.6600\n",
      "Iteration 6, Loss: 9.2323\n",
      "Iteration 7, Loss: 7.6815\n",
      "Iteration 8, Loss: 7.6208\n",
      "Iteration 9, Loss: 7.5674\n",
      "Iteration 10, Loss: 7.5357\n",
      "Iteration 11, Loss: 7.5157\n",
      "Iteration 12, Loss: 7.4886\n",
      "Iteration 13, Loss: 7.4116\n",
      "Iteration 14, Loss: 7.1883\n",
      "Iteration 15, Loss: 7.3648\n",
      "Iteration 16, Loss: 7.2120\n",
      "Iteration 17, Loss: 6.7272\n",
      "Iteration 18, Loss: 7.0580\n",
      "Iteration 19, Loss: 6.4870\n",
      "Iteration 20, Loss: 6.4689\n",
      "Training time: 6.48 seconds\n",
      "\n",
      "Generated token sequence:\n",
      "[[8382 2019 8302 9113 4589 3610 6330 9380 4788 5181 8065 4707 8917 6371\n",
      "   516 5624  178 7638 1814 8643 7851 1157 6889 1783  581]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # for cpu do device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a simple transformer-based language model\n",
    "class SimpleTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=512, nhead=8, \n",
    "                 num_layers=6, dim_feedforward=2048):\n",
    "        super(SimpleTransformerLM, self).__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional encoding (simplified for this example)\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len]\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output(x)  # [batch_size, seq_len, vocab_size]\n",
    "        return x\n",
    "\n",
    "# Create model and move to MPS device\n",
    "vocab_size = 10000  # Vocabulary size\n",
    "seq_length = 128    # Sequence length\n",
    "batch_size = 16     # Batch size\n",
    "\n",
    "model = SimpleTransformerLM(vocab_size=vocab_size)\n",
    "model.to(device)  # Move model to GPU\n",
    "print(f\"Model moved to {device}\")\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "target_data = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model for a few steps to demonstrate GPU usage\n",
    "def train_step(model, inputs, targets):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    # Reshape for cross entropy\n",
    "    outputs = outputs.view(-1, vocab_size)\n",
    "    targets = targets.view(-1)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Test training performance\n",
    "print(\"\\nTraining for 20 iterations to test performance:\")\n",
    "start_time = time.time()\n",
    "for i in range(20):\n",
    "    loss = train_step(model, input_data, target_data)\n",
    "    print(f\"Iteration {i+1}, Loss: {loss:.4f}\")\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\") # 2.80 sec on GPU, 13.35 sec on CPU (MacBook Pro M2 16GB)\n",
    "\n",
    "# Generate predictions with the model\n",
    "def generate_text(model, start_tokens, max_length=20):\n",
    "    model.eval()\n",
    "    current_tokens = start_tokens.clone()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions for the next token\n",
    "            logits = model(current_tokens)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append new token to sequence\n",
    "            current_tokens = torch.cat([current_tokens, next_token], dim=1)\n",
    "    \n",
    "    return current_tokens\n",
    "\n",
    "# Try generating some \"text\" (just token IDs in this example)\n",
    "start_seq = torch.randint(0, vocab_size, (1, 5)).to(device)\n",
    "generated = generate_text(model, start_seq)\n",
    "print(\"\\nGenerated token sequence:\")\n",
    "print(generated.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
