{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc60672",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis - Complete Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for training, testing, and comparing sentiment analysis models on the IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow tensorflow-datasets scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35eb7e",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc372e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create model_data directory if it doesn't exist\n",
    "MODEL_DIR = \"model_data\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be9b45",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset and Visualize Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4340336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB reviews dataset\n",
    "train_ds, test_ds = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE REVIEW FROM DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text, label in train_ds.take(1):\n",
    "    sample_text = text.numpy().decode('utf-8')\n",
    "    sample_label = label.numpy()\n",
    "    print(f\"\\nReview Text:\\n{sample_text}\\n\")\n",
    "    print(f\"Label: {sample_label} ({'Positive' if sample_label == 1 else 'Negative'})\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7030c5e",
   "metadata": {},
   "source": [
    "## Step 3: Create Text Vectorization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for text processing\n",
    "vocab_size = 10000  # Top 10,000 most frequent words\n",
    "seq_length = 250    # Maximum review length in words\n",
    "\n",
    "# Create TextVectorization layer\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length\n",
    ")\n",
    "\n",
    "# Learn vocabulary from training data\n",
    "train_text = train_ds.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Vocabulary learned from {len(list(train_text))} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2dec7",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text, label):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, label\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Prepare training dataset with shuffling, batching, and prefetching\n",
    "train_ds_prepared = (\n",
    "    train_ds\n",
    "    .shuffle(10000)\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Prepare test dataset (no shuffling needed for testing)\n",
    "test_ds_prepared = (\n",
    "    test_ds\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(\"Data pipeline prepared successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1b340",
   "metadata": {},
   "source": [
    "## Step 5: Configure Model Name (Set this before training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== CONTROL PARAMETERS - CHANGE THIS VARIABLE BEFORE TRAINING ======\n",
    "USE_EXISTING_MODEL = False  # Set to True to load an existing model, False to create new\n",
    "MODEL_NAME = \"baseline_model\"  # Change this to describe your model\n",
    "EXISTING_MODEL_PATH = \"model_data/baseline_model_20251122_151705.h5\"  # Path to existing model (start with model_data/)\n",
    "# ==================================================================\n",
    "\n",
    "if not USE_EXISTING_MODEL:\n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"{MODEL_NAME}_{timestamp}\"\n",
    "    model_path = os.path.join(MODEL_DIR, model_filename)\n",
    "\n",
    "    print(f\"Model will be saved as: {model_filename}\")\n",
    "    print(f\"Full path: {model_path}\")\n",
    "else:\n",
    "    # Validate existing model path\n",
    "    if os.path.exists(EXISTING_MODEL_PATH):\n",
    "        print(f\"âœ“ Model file found: {EXISTING_MODEL_PATH}\")\n",
    "    else:\n",
    "        print(f\"âŒ ERROR: Model file not found at {EXISTING_MODEL_PATH}\")\n",
    "        print(\"Please check the path and try again.\")\n",
    "        raise FileNotFoundError(f\"Model file not found: {EXISTING_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb1981",
   "metadata": {},
   "source": [
    "## Step 6: Build or Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2543a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_EXISTING_MODEL:\n",
    "    # Load existing model (path already validated in Step 5)\n",
    "    print(f\"Loading existing model from: {EXISTING_MODEL_PATH}\")\n",
    "    model = tf.keras.models.load_model(EXISTING_MODEL_PATH)\n",
    "    print(\"âœ“ Model loaded successfully!\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(\"Model Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Try to load corresponding metrics file\n",
    "    metrics_path = EXISTING_MODEL_PATH.replace('.h5', '.json')\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            model_config = json.load(f)\n",
    "        print(f\"âœ“ Loaded existing model configuration\")\n",
    "    else:\n",
    "        # Create minimal config for existing model\n",
    "        model_config = {\n",
    "            \"model_name\": \"existing_model\",\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            \"note\": \"Loaded from existing file\"\n",
    "        }\n",
    "        print(\"âš  No configuration file found, using minimal config\")\n",
    "else:\n",
    "    # Build new model\n",
    "    print(\"Creating new model...\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    embedding_dim = 64\n",
    "    hidden_units = 64\n",
    "    epochs = 3\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(hidden_units, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Display model architecture\n",
    "    model.build(input_shape=(None, seq_length))\n",
    "    model.summary()\n",
    "    \n",
    "    # Extract model architecture dynamically\n",
    "    layers_info = []\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        layer_config = layer.get_config()\n",
    "        layer_info = {\n",
    "            \"layer_type\": layer.__class__.__name__,\n",
    "            \"layer_index\": idx,\n",
    "            \"layer_name\": layer.name,\n",
    "            \"config\": layer_config,\n",
    "            \"trainable\": layer.trainable\n",
    "        }\n",
    "        # Add output shape if available\n",
    "        try:\n",
    "            layer_info[\"output_shape\"] = str(layer.output_shape)\n",
    "        except:\n",
    "            pass\n",
    "        layers_info.append(layer_info)\n",
    "    \n",
    "    # Extract optimizer configuration\n",
    "    optimizer_config = model.optimizer.get_config()\n",
    "    optimizer_name = model.optimizer.__class__.__name__\n",
    "    \n",
    "    # Extract loss function\n",
    "    loss_fn = model.loss\n",
    "    if hasattr(loss_fn, '__name__'):\n",
    "        loss_name = loss_fn.__name__\n",
    "    elif hasattr(loss_fn, 'name'):\n",
    "        loss_name = loss_fn.name\n",
    "    else:\n",
    "        loss_name = str(loss_fn)\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics_list = []\n",
    "    for metric in model.metrics:\n",
    "        if hasattr(metric, 'name'):\n",
    "            metrics_list.append(metric.name)\n",
    "        else:\n",
    "            metrics_list.append(str(metric))\n",
    "    \n",
    "    # Store comprehensive model configuration\n",
    "    model_config = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_type\": model.__class__.__name__,\n",
    "        \"hyperparameters\": {\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"seq_length\": seq_length,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"hidden_units\": hidden_units,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs\n",
    "        },\n",
    "        \"model_architecture\": {\n",
    "            \"layers\": layers_info,\n",
    "            \"total_layers\": len(model.layers),\n",
    "            \"total_params\": int(model.count_params()),\n",
    "            \"trainable_params\": int(sum([tf.size(w).numpy() for w in model.trainable_weights])),\n",
    "            \"non_trainable_params\": int(sum([tf.size(w).numpy() for w in model.non_trainable_weights]))\n",
    "        },\n",
    "        \"compilation\": {\n",
    "            \"optimizer\": {\n",
    "                \"class_name\": optimizer_name,\n",
    "                \"config\": optimizer_config\n",
    "            },\n",
    "            \"loss_function\": loss_name,\n",
    "            \"metrics\": metrics_list\n",
    "        }\n",
    "    }\n",
    "    print(\"âœ“ New model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c51b8",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c26331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only train if we created a new model (skip training for existing models)\n",
    "if not USE_EXISTING_MODEL:\n",
    "    print(f\"Training {MODEL_NAME}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds_prepared,\n",
    "        epochs=epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store training history\n",
    "    model_config[\"training_history\"] = {\n",
    "        \"loss\": [float(val) for val in history.history['loss']],\n",
    "        \"accuracy\": [float(val) for val in history.history['accuracy']],\n",
    "        \"epochs_completed\": len(history.history['loss'])\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training complete!\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SKIPPED TRAINING (Using existing model)\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7640e",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a035d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_ds_prepared, verbose=1)\n",
    "\n",
    "# Get predictions for all test data\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_true = []\n",
    "y_pred_probs = []\n",
    "\n",
    "for texts, labels in test_ds_prepared:\n",
    "    predictions = model.predict(texts, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred_probs.extend(predictions.flatten())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred_probs = np.array(y_pred_probs)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision:      {precision:.4f}\")\n",
    "print(f\"Recall:         {recall:.4f}\")\n",
    "print(f\"F1 Score:       {f1:.4f}\")\n",
    "print(f\"ROC AUC Score:  {roc_auc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted Negative  Predicted Positive\")\n",
    "print(f\"Actual Negative        {cm[0,0]:5d}              {cm[0,1]:5d}\")\n",
    "print(f\"Actual Positive        {cm[1,0]:5d}              {cm[1,1]:5d}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "\n",
    "# True/False Positives/Negatives\n",
    "tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "total_predictions = tn + fp + fn + tp\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "# False Positive Rate\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# False Negative Rate\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "# Matthews Correlation Coefficient\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Store comprehensive metrics in config\n",
    "model_config[\"evaluation_metrics\"] = {\n",
    "    \"test_loss\": float(test_loss),\n",
    "    \"test_accuracy\": float(test_accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"roc_auc_score\": float(roc_auc),\n",
    "    \"specificity\": float(specificity),\n",
    "    \"matthews_corrcoef\": float(mcc),\n",
    "    \"confusion_matrix\": {\n",
    "        \"matrix\": cm.tolist(),\n",
    "        \"true_negatives\": int(tn),\n",
    "        \"false_positives\": int(fp),\n",
    "        \"false_negatives\": int(fn),\n",
    "        \"true_positives\": int(tp)\n",
    "    },\n",
    "    \"error_rates\": {\n",
    "        \"false_positive_rate\": float(fpr),\n",
    "        \"false_negative_rate\": float(fnr)\n",
    "    },\n",
    "    \"total_predictions\": int(total_predictions),\n",
    "    \"correct_predictions\": int(tp + tn),\n",
    "    \"incorrect_predictions\": int(fp + fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a67c2",
   "metadata": {},
   "source": [
    "## Step 9: Test Single Example (Detailed Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8078ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one example from test set for detailed analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"SINGLE TEST EXAMPLE - DETAILED PREDICTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for text, label in test_ds.take(1):\n",
    "    # Get the original text\n",
    "    original_text = text.numpy().decode('utf-8')\n",
    "    true_label = label.numpy()\n",
    "    \n",
    "    # Preprocess and predict\n",
    "    preprocessed = vectorize_layer(tf.expand_dims(text, 0))\n",
    "    prediction_prob = model.predict(preprocessed, verbose=0)[0][0]\n",
    "    predicted_label = 1 if prediction_prob > 0.5 else 0\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nReview Text:\\n{original_text}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"True Label:           {true_label} ({'Positive' if true_label == 1 else 'Negative'})\")\n",
    "    print(f\"Predicted Label:      {predicted_label} ({'Positive' if predicted_label == 1 else 'Negative'})\")\n",
    "    print(f\"Prediction Confidence: {prediction_prob:.4f} ({prediction_prob*100:.2f}%)\")\n",
    "    print(f\"Correct Prediction:   {'âœ“ YES' if predicted_label == true_label else 'âœ— NO'}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a3ed6",
   "metadata": {},
   "source": [
    "## Step 10: Save Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only save if we created a new model (not loading existing)\n",
    "if not USE_EXISTING_MODEL:\n",
    "    # Save the model\n",
    "    model_file = f\"{model_path}.h5\"\n",
    "    model.save(model_file)\n",
    "    print(f\"âœ“ Model saved to: {model_file}\")\n",
    "    \n",
    "    # Save the metrics and configuration\n",
    "    metrics_file = f\"{model_path}.json\"\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(model_config, f, indent=4)\n",
    "    print(f\"âœ“ Metrics saved to: {metrics_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL AND METRICS SAVED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SKIPPED SAVING (Using existing model)\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e80ed6a",
   "metadata": {},
   "source": [
    "---\n",
    "## MODEL COMPARISON AND RANKING SYSTEM\n",
    "\n",
    "Run the cells below to compare all trained models and see rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597d2dc",
   "metadata": {},
   "source": [
    "## Step 11: Load and Rank All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Load all model metrics from the model_data directory\n",
    "def load_all_models():\n",
    "    \"\"\"Load metrics for all trained models\"\"\"\n",
    "    json_files = glob.glob(os.path.join(MODEL_DIR, \"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No trained models found in model_data directory.\")\n",
    "        print(\"Train a model first by running the cells above!\")\n",
    "        return []\n",
    "    \n",
    "    models_data = []\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # Add filename for reference\n",
    "                data['filename'] = os.path.basename(json_file).replace('.json', '')\n",
    "                models_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return models_data\n",
    "\n",
    "# Load all models\n",
    "all_models = load_all_models()\n",
    "print(f\"Found {len(all_models)} trained model(s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d621023",
   "metadata": {},
   "source": [
    "## Step 12: Rank Models by Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranking_score(model_data):\n",
    "    \"\"\"\n",
    "    Calculate a composite ranking score based on multiple metrics.\n",
    "    Higher is better. Weighted combination of:\n",
    "    - F1 Score (40%)\n",
    "    - ROC AUC (30%)\n",
    "    - Accuracy (20%)\n",
    "    - Precision (10%)\n",
    "    \"\"\"\n",
    "    # Support both old and new metric format\n",
    "    metrics = model_data.get('evaluation_metrics', model_data.get('metrics', {}))\n",
    "    \n",
    "    f1 = metrics.get('f1_score', 0)\n",
    "    roc_auc = metrics.get('roc_auc_score', metrics.get('roc_auc', 0))\n",
    "    accuracy = metrics.get('test_accuracy', 0)\n",
    "    precision = metrics.get('precision', 0)\n",
    "    \n",
    "    # Weighted score\n",
    "    score = (f1 * 0.4) + (roc_auc * 0.3) + (accuracy * 0.2) + (precision * 0.1)\n",
    "    return score\n",
    "\n",
    "# Rank models\n",
    "if all_models:\n",
    "    for model in all_models:\n",
    "        model['ranking_score'] = calculate_ranking_score(model)\n",
    "    \n",
    "    # Sort by ranking score (best first)\n",
    "    ranked_models = sorted(all_models, key=lambda x: x['ranking_score'], reverse=True)\n",
    "    \n",
    "    # Display rankings\n",
    "    print(\"=\" * 100)\n",
    "    print(\"MODEL RANKINGS (Best to Worst)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Rank':<6} {'Model Name':<35} {'F1':<8} {'ROC AUC':<8} {'Accuracy':<8} {'Score':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for rank, model in enumerate(ranked_models, 1):\n",
    "        name = model['model_name']\n",
    "        timestamp = model['timestamp']\n",
    "        metrics = model.get('evaluation_metrics', model.get('metrics', {}))\n",
    "        \n",
    "        f1 = metrics.get('f1_score', 0)\n",
    "        roc_auc = metrics.get('roc_auc_score', metrics.get('roc_auc', 0))\n",
    "        accuracy = metrics.get('test_accuracy', 0)\n",
    "        score = model['ranking_score']\n",
    "        \n",
    "        display_name = f\"{name} ({timestamp})\"\n",
    "        print(f\"{rank:<6} {display_name:<35} {f1:<8.4f} {roc_auc:<8.4f} {accuracy:<8.4f} {score:<8.4f}\")\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "else:\n",
    "    print(\"No models to rank. Train some models first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cb894",
   "metadata": {},
   "source": [
    "## Step 13: Display Best Model in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9487a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_models and ranked_models:\n",
    "    best_model = ranked_models[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"ðŸ† BEST MODEL - DETAILED INFORMATION\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ MODEL IDENTIFICATION\")\n",
    "    print(f\"   Name:           {best_model['model_name']}\")\n",
    "    print(f\"   Timestamp:      {best_model['timestamp']}\")\n",
    "    print(f\"   Filename:       {best_model['filename']}\")\n",
    "    print(f\"   Ranking Score:  {best_model['ranking_score']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ—ï¸  MODEL ARCHITECTURE\")\n",
    "    arch = best_model.get('model_architecture', best_model.get('architecture', {}))\n",
    "    hyperparams = best_model.get('hyperparameters', {})\n",
    "    print(f\"   Vocabulary Size:    {hyperparams.get('vocab_size', arch.get('vocab_size', 'N/A'))}\")\n",
    "    print(f\"   Sequence Length:    {hyperparams.get('seq_length', arch.get('seq_length', 'N/A'))}\")\n",
    "    print(f\"   Embedding Dim:      {hyperparams.get('embedding_dim', arch.get('embedding_dim', 'N/A'))}\")\n",
    "    print(f\"   Hidden Units:       {hyperparams.get('hidden_units', arch.get('hidden_units', 'N/A'))}\")\n",
    "    \n",
    "    compilation = best_model.get('compilation', {})\n",
    "    print(f\"   Optimizer:          {compilation.get('optimizer', arch.get('optimizer', 'N/A'))}\")\n",
    "    print(f\"   Loss Function:      {compilation.get('loss_function', arch.get('loss', 'N/A'))}\")\n",
    "    \n",
    "    print(f\"\\nâš™ï¸  TRAINING PARAMETERS\")\n",
    "    print(f\"   Batch Size:         {hyperparams.get('batch_size', 'N/A')}\")\n",
    "    print(f\"   Epochs:             {hyperparams.get('epochs', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š PERFORMANCE METRICS\")\n",
    "    metrics = best_model.get('evaluation_metrics', best_model.get('metrics', {}))\n",
    "    print(f\"   Test Accuracy:      {metrics.get('test_accuracy', 0):.4f} ({metrics.get('test_accuracy', 0)*100:.2f}%)\")\n",
    "    print(f\"   Test Loss:          {metrics.get('test_loss', 0):.4f}\")\n",
    "    print(f\"   Precision:          {metrics.get('precision', 0):.4f}\")\n",
    "    print(f\"   Recall:             {metrics.get('recall', 0):.4f}\")\n",
    "    print(f\"   F1 Score:           {metrics.get('f1_score', 0):.4f}\")\n",
    "    print(f\"   ROC AUC Score:      {metrics.get('roc_auc_score', metrics.get('roc_auc', 0)):.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ CONFUSION MATRIX\")\n",
    "    cm_data = metrics.get('confusion_matrix', {})\n",
    "    cm = cm_data.get('matrix', [[0, 0], [0, 0]]) if isinstance(cm_data, dict) else cm_data\n",
    "    print(f\"                      Predicted Negative    Predicted Positive\")\n",
    "    print(f\"   Actual Negative          {cm[0][0]:6d}                {cm[0][1]:6d}\")\n",
    "    print(f\"   Actual Positive          {cm[1][0]:6d}                {cm[1][1]:6d}\")\n",
    "    \n",
    "    # Calculate additional insights\n",
    "    total = sum(sum(row) for row in cm)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ ADDITIONAL INSIGHTS\")\n",
    "    print(f\"   True Negatives:     {tn:6d} ({tn/total*100:.2f}%)\")\n",
    "    print(f\"   False Positives:    {fp:6d} ({fp/total*100:.2f}%)\")\n",
    "    print(f\"   False Negatives:    {fn:6d} ({fn/total*100:.2f}%)\")\n",
    "    print(f\"   True Positives:     {tp:6d} ({tp/total*100:.2f}%)\")\n",
    "    print(f\"   Total Predictions:  {total:6d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "else:\n",
    "    print(\"No models available for detailed display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2ab3c",
   "metadata": {},
   "source": [
    "## Step 14: Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_models and len(ranked_models) > 0:\n",
    "    # Prepare data for visualization\n",
    "    model_names = [f\"{m['model_name']}\\n{m['timestamp']}\" for m in ranked_models]\n",
    "    # Support both old and new format\n",
    "    accuracies = [m.get('evaluation_metrics', m.get('metrics', {})).get('test_accuracy', 0) for m in ranked_models]\n",
    "    f1_scores = [m.get('evaluation_metrics', m.get('metrics', {})).get('f1_score', 0) for m in ranked_models]\n",
    "    roc_aucs = [m.get('evaluation_metrics', m.get('metrics', {})).get('roc_auc_score', m.get('evaluation_metrics', m.get('metrics', {})).get('roc_auc', 0)) for m in ranked_models]\n",
    "    precisions = [m.get('evaluation_metrics', m.get('metrics', {})).get('precision', 0) for m in ranked_models]\n",
    "    recalls = [m.get('evaluation_metrics', m.get('metrics', {})).get('recall', 0) for m in ranked_models]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    axes[0, 0].barh(model_names, accuracies, color='steelblue')\n",
    "    axes[0, 0].set_xlabel('Accuracy')\n",
    "    axes[0, 0].set_title('Test Accuracy')\n",
    "    axes[0, 0].set_xlim(0, 1)\n",
    "    for i, v in enumerate(accuracies):\n",
    "        axes[0, 0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    # Plot 2: F1 Score comparison\n",
    "    axes[0, 1].barh(model_names, f1_scores, color='coral')\n",
    "    axes[0, 1].set_xlabel('F1 Score')\n",
    "    axes[0, 1].set_title('F1 Score')\n",
    "    axes[0, 1].set_xlim(0, 1)\n",
    "    for i, v in enumerate(f1_scores):\n",
    "        axes[0, 1].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    # Plot 3: ROC AUC comparison\n",
    "    axes[1, 0].barh(model_names, roc_aucs, color='mediumseagreen')\n",
    "    axes[1, 0].set_xlabel('ROC AUC')\n",
    "    axes[1, 0].set_title('ROC AUC Score')\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    for i, v in enumerate(roc_aucs):\n",
    "        axes[1, 0].text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "    \n",
    "    # Plot 4: Precision vs Recall\n",
    "    axes[1, 1].scatter(recalls, precisions, s=200, alpha=0.6, c=range(len(model_names)), cmap='viridis')\n",
    "    for i, name in enumerate(model_names):\n",
    "        axes[1, 1].annotate(f'#{i+1}', (recalls[i], precisions[i]), \n",
    "                           ha='center', va='center', fontweight='bold', color='white')\n",
    "    axes[1, 1].set_xlabel('Recall')\n",
    "    axes[1, 1].set_ylabel('Precision')\n",
    "    axes[1, 1].set_title('Precision vs Recall')\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Visualization complete!\")\n",
    "else:\n",
    "    print(\"Need at least one model to visualize. Train some models first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
