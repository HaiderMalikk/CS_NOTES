{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba17df",
   "metadata": {},
   "source": [
    "# Data Pipelines in TensorFlow\n",
    "\n",
    "### What is a “data pipeline” in machine learning?\n",
    "A data pipeline is the system that feeds data into your model during training and evaluation.\n",
    "Think of it like a kitchen conveyor belt:\n",
    "- At one end, you load in raw ingredients (text files, CSVs, TFRecords, images, etc.).\n",
    "- Along the belt, you wash, cut, and prepare them (decode, normalize, tokenize, batch).\n",
    "- At the other end, your model gets perfectly prepared “mini-meals” (tensors ready for training).\n",
    "If this belt is slow, your model waits idle, wasting GPU power.\n",
    "If it’s too fast, you waste memory.\n",
    "So ML engineers tune it carefully for balance and throughput.\n",
    "\n",
    "### Why TensorFlow needs pipelines\n",
    "TensorFlow models are trained in graphs — computations that run on CPUs, GPUs, or TPUs.\n",
    "Those devices are fast, but the bottleneck is usually the data loading step:\n",
    "reading from disk, decoding files, augmenting images, or tokenizing text.\n",
    "\n",
    "To fix this, TensorFlow provides the tf.data API — a high-performance, graph-integrated data pipeline system.\n",
    "It lets you:\n",
    "- Stream data efficiently from disk or memory\n",
    "- Parallelize operations across CPU cores\n",
    "- Prefetch batches so the GPU never waits\n",
    "- Cache preprocessed data to avoid recomputation\n",
    "- Compose your data transformations like a chain\n",
    "  \n",
    "So when we say “TensorFlow pipeline,” we really mean:\n",
    "A tf.data.Dataset object that describes how to load, process, and batch your data efficiently, often entirely inside the TensorFlow graph.\n",
    "\n",
    "### What actually happens inside a pipeline\n",
    "Let’s walk through a typical training example:\n",
    "\n",
    "```py\n",
    "dataset = (\n",
    "    tf.data.TextLineDataset(\"reviews.txt\")     # 1️⃣ Read from file(s)\n",
    "    .map(parse_line)                           # 2️⃣ Parse or tokenize text\n",
    "    .shuffle(buffer_size=10000)                # 3️⃣ Shuffle samples for randomness\n",
    "    .batch(32)                                 # 4️⃣ Group into batches\n",
    "    .prefetch(tf.data.AUTOTUNE)                # 5️⃣ Prepare next batch while GPU trains\n",
    ")\n",
    "```\n",
    "Let’s break it down:\n",
    "\n",
    "| Step   | Function                          | What It Does                                                   | Why It Matters                                |\n",
    "|--------|-----------------------------------|----------------------------------------------------------------|----------------------------------------------|\n",
    "| 1. Read | TextLineDataset, TFRecordDataset, etc. | Loads data efficiently from disk, streaming, or memory.         | Prevents “file I/O bottlenecks”.             |\n",
    "| 2. Map  | .map(func)                       | Applies a transformation to each element (like parsing JSON, tokenizing text, decoding images). | Lets you preprocess inside TF (parallelizable). |\n",
    "| 3. Shuffle | .shuffle(buffer_size)          | Randomizes sample order each epoch.                            | Improves generalization, prevents overfitting. |\n",
    "| 4. Batch | .batch(batch_size)              | Groups samples into mini-batches for training.                 | Allows vectorized GPU operations.            |\n",
    "| 5. Prefetch | .prefetch(tf.data.AUTOTUNE)   | Loads next batch while GPU is training on current batch.       | Maximizes GPU utilization.                   |\n",
    "\n",
    "That’s a complete data pipeline — from reading → preprocessing → batching → feeding.\n",
    "\n",
    "### What makes TensorFlow pipelines special\n",
    "TensorFlow’s tf.data pipelines are not just loops — they are part of the graph, meaning:\n",
    "- They can run asynchronously from the model.\n",
    "- They can overlap CPU preprocessing and GPU training.\n",
    "- They can automatically tune performance using tf.data.AUTOTUNE.\n",
    "- They can scale across devices (e.g., multiple GPUs or TPUs).\n",
    "This is what makes them much faster and cleaner than writing your own Python loop like:\n",
    "``` py\n",
    "for x, y in dataset:\n",
    "    model.train_on_batch(x, y)\n",
    "```\n",
    "That’s fine for small demos — but real-world ML needs throughput and reproducibility, which tf.data gives you.\n",
    "\n",
    "### TF pipeline in context of NLP\n",
    "When your data is text, the pipeline also does:\n",
    "1. Reading raw text (from files, CSVs, TFRecords).\n",
    "2. Tokenizing (turning words → integers).\n",
    "3. Padding/truncating sequences.\n",
    "4. Building attention masks or features.\n",
    "5. Batching for model input.\n",
    "\n",
    "Example:\n",
    "```py\n",
    "dataset = (\n",
    "    tf.data.TextLineDataset(\"data.txt\")\n",
    "    .map(tokenize_and_pad, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "```\n",
    "\n",
    "The tokenize_and_pad function might call:\n",
    "- tf.strings.split() (basic whitespace)\n",
    "- or a pretrained tokenizer like keras_nlp.tokenizers.WordPieceTokenizer\n",
    "- or tf.py_function wrapping a Python tokenizer\n",
    "That’s what “integrating a tokenizer into the pipeline” means — it becomes part of this conveyor belt.\n",
    "\n",
    "### Why we care about things like cache(), prefetch(), AUTOTUNE\n",
    "These are performance tuning knobs for your data loader:\n",
    "\n",
    "| Function                  | Description                                           | Common Use                                      |\n",
    "|---------------------------|-------------------------------------------------------|------------------------------------------------|\n",
    "| cache()                  | Stores the preprocessed dataset in memory or on disk after first epoch. | When dataset fits in RAM or is small.          |\n",
    "| prefetch()               | Loads the next batch while the model trains on the current one. | Always use with AUTOTUNE.                      |\n",
    "| AUTOTUNE                 | Lets TF automatically pick parallelism/prefetch settings. | Default best practice.                         |\n",
    "| map(num_parallel_calls=AUTOTUNE) | Runs preprocessing functions in parallel threads. | Speeds up CPU-bound steps like decoding/tokenizing. |\n",
    "\n",
    "Together, these let TensorFlow stream data continuously to your GPU.\n",
    "\n",
    "### How it connects to what you’ll build later\n",
    "Eventually, your model training loop will look like this:\n",
    "``` py\n",
    "for x_batch, y_batch in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = loss_fn(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "The dataset in this loop is what the pipeline built.\n",
    "It keeps producing ready-to-train batches infinitely or per epoch.\n",
    "\n",
    "That’s why every TensorFlow engineer must master tf.data — it’s how you feed your models at scale.\n",
    "\n",
    "Summary — “What are TensorFlow pipelines?”\n",
    "| Concept         | Intuition                          | Analogy                                      |\n",
    "|------------------|------------------------------------|----------------------------------------------|\n",
    "| Pipeline         | The complete data flow from disk → ready tensors | A kitchen conveyor belt for data            |\n",
    "| tf.data.Dataset  | TensorFlow object representing a pipeline | Recipe for data preparation                 |\n",
    "| map()            | Transform each data sample        | Chop vegetables on the belt                 |\n",
    "| batch()          | Group samples together            | Pack boxes of meals                         |\n",
    "| prefetch()       | Get the next batch ready          | Chef preps next dish while plating current one |\n",
    "| cache()          | Save processed data               | Store pre-chopped ingredients               |\n",
    "| AUTOTUNE         | Auto-optimizes performance        | Smart chef who adjusts speed                |\n",
    "\n",
    "NOTE: in scikit-learn, pipelines chain processing + modeling steps, while in TensorFlow, pipelines mainly handle data loading, preprocessing, and feeding efficiently to the model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983675a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Example: High-Level Text Classification Pipeline\n",
    "Let’s build a clean pipeline for a text classification task — say, classifying IMDB movie reviews as positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfcdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Load dataset\n",
    "# TensorFlow Datasets (TFDS) gives us ready-to-use data\n",
    "train_ds, test_ds = tfds.load(\n",
    "    \"imdb_reviews\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,  # returns (text, label) for both train and test sets\n",
    ")\n",
    "\n",
    "# 2. Tokenization and TextVectorization layer, tokenizing is to convert text to numbers (vectors) and textVectorization is a layer that helps with that\n",
    "# This is a built-in Keras preprocessing layer for text\n",
    "vocab_size = 10000 # Limit vocabulary size to top 10,000 words to save memory this is 10000 words from the dataset\n",
    "seq_length = 250 # Limit each review to 250 words\n",
    "\n",
    "# a vectorization layer is created to handle the tokenization and vectorization of text data it converts text into sequences of integers using a fixed vocabulary size and sequence length the word vocab for the layer is learned from the training data\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=seq_length\n",
    ")\n",
    "\n",
    "# You must \"adapt\" the layer to learn the vocabulary from the text\n",
    "train_text = train_ds.map(lambda text, label: text) # Extract only the text from the training dataset\n",
    "vectorize_layer.adapt(train_text) # once we learn the vocab we can use the layer to convert text to integer sequences basically the text embeddings willwe based on this vocab\n",
    "\n",
    "# 3. Preprocessing pipeline function\n",
    "def preprocess_text(text, label):\n",
    "    text = vectorize_layer(text)  # Apply the TextVectorization layer to the text this will convert the text to integer sequences from out learned vocabulary \n",
    "    return text, label # Return the processed text and label as a tuple\n",
    "\n",
    "# 4. Apply preprocessing, shuffle, batch, and prefetch\n",
    "batch_size = 32\n",
    "\n",
    "# Apply preprocessing to the datasets and optimize them for performance (my using autotune for number of parallel calls and prefetching we let tensorflow decide the optimal number of batches to prefetch and number of parallel calls)\n",
    "train_ds = (\n",
    "    train_ds \n",
    "    .shuffle(10000) # Shuffle the dataset with a buffer size of 10,000 to ensure randomness\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE) # Apply the preprocessing function in parallel\n",
    "    .batch(batch_size) # Batch the data\n",
    "    .prefetch(tf.data.AUTOTUNE) # Prefetch data for better performance AUTOTUNE lets TensorFlow decide the optimal number of batches to prefetch\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the test dataset (by using autotune for number of parallel calls and prefetching we let tensorflow decide the optimal number of batches to prefetch and number of parallel calls)\n",
    "test_ds = (\n",
    "    test_ds\n",
    "    .map(preprocess_text, num_parallel_calls=tf.data.AUTOTUNE) # Apply the preprocessing function in parallel\n",
    "    .batch(batch_size) # Batch the data\n",
    "    .prefetch(tf.data.AUTOTUNE) # Prefetch data for better performance\n",
    ")\n",
    "\n",
    "# 5. Build a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64, input_length=seq_length), # Embedding layer to convert integer sequences to dense vectors of fixed size why because neural networks work better with dense vectors (here we say each word will be represented by a 64-dimensional vector we specify the input length to be seq_length which is 250 words and give vocab size to the function as well)\n",
    "    tf.keras.layers.GlobalAveragePooling1D(), # Global average pooling to reduce the sequence dimension pooling is a way to downsample the data means taking the average of all the elements in the sequence and reducing the dimensionality of the data here we reduce the dimentions of the sequence to a single vector our sequence is now represented by a single vector (our sequence here was 250 words long now its just one vector) this vector represents the entire review the embedding process is done by pooling function\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"), # A dense hidden layer with ReLU activation (takes in the 64 neurons from the embedding layer i.e our input in a vector of size 64 and applies ReLU activation to it)\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\") # Output layer for binary classification (positive/negative review) (here we have one neuron with sigmoid activation to output a probability between 0 and 1 indicating the sentiment of the review)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\", # Adam optimizer is an efficient optimization algorithm that adjusts the learning rate during training we use the optimizer to apply gradients to the model's weights based on the loss function\n",
    "    loss=\"binary_crossentropy\", # Binary crossentropy loss function for binary classification tasks\n",
    "    metrics=[\"accuracy\"] # Track accuracy during training\n",
    ")\n",
    "\n",
    "# 6. Train the model — the dataset is already optimized\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=3)\n",
    "\n",
    "# In this code example we built a complete TensorFlow data pipeline for text data using the IMDB reviews dataset we loaded the data, tokenized and vectorized the text using a TextVectorization layer, applied preprocessing, shuffling, batching, and prefetching to optimize performance finally we built and trained a simple neural network model for sentiment analysis on the preprocessed data\n",
    "# what we mean by a data pipeline is a series of steps that process and prepare data for training machine learning models these steps typically include loading the data, preprocessing it (like tokenization and vectorization for text data), batching it into manageable sizes, and optimizing the data flow for performance during training\n",
    "# in our case that part (the data pipeline part) was: loading the dataset, applying the TextVectorization layer, shuffling, batching, and prefetching the data\n",
    "# this ensures that the data is in the right format and is efficiently fed into the model during training\n",
    "\n",
    "# EX output \n",
    "\"\"\" \n",
    "Epoch 1/3\n",
    "782/782 ━━━━━━━━━━━━━━━━━━━━ 13s 16ms/step - accuracy: 0.6535 - loss: 0.5926 - val_accuracy: 0.8550 - val_loss: 0.3496\n",
    "Epoch 2/3\n",
    "782/782 ━━━━━━━━━━━━━━━━━━━━ 12s 16ms/step - accuracy: 0.8736 - loss: 0.3008 - val_accuracy: 0.8558 - val_loss: 0.3340\n",
    "Epoch 3/3\n",
    "782/782 ━━━━━━━━━━━━━━━━━━━━ 12s 16ms/step - accuracy: 0.9040 - loss: 0.2412 - val_accuracy: 0.8660 - val_loss: 0.3235\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4444c",
   "metadata": {},
   "source": [
    "## Efficient tf.data Pipelines\n",
    "\n",
    "**Goal**: Keep the accelerators fed. The CPU pipeline should produce batches faster than the device consumes them.\n",
    "\n",
    "## Map vs map with num_parallel_calls\n",
    "\n",
    "- `map(func)` applies `func` sequentially. Slow if `func` is CPU-heavy.\n",
    "- `map(func, num_parallel_calls=N)` runs up to N calls concurrently on CPU threads.\n",
    "- `num_parallel_calls=tf.data.AUTOTUNE` lets TF choose.\n",
    "- **Always use parallel map for CPU-bound transforms** (tokenization, augmentation).\n",
    "```python\n",
    "ds = ds.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "If your map function uses Python-only tokenizers via `tf.py_function`, you still benefit from `num_parallel_calls`, but pyfunctions have extra overhead.\n",
    "\n",
    "## batch() and prefetch()\n",
    "\n",
    "- `batch(batch_size)` groups items into tensors shape `[batch_size, ...]`. Use `padded_batch` for variable-length sequences.\n",
    "- `prefetch(buffer_size)` overlaps data production and model consumption.\n",
    "- Use `AUTOTUNE` for prefetch to let TF decide.\n",
    "```python\n",
    "ds = ds.padded_batch(batch_size, padded_shapes=(...), drop_remainder=True)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "- `drop_remainder=True` is helpful for multi-device training (synchronous replicas expect equal sized per-replica batches).\n",
    "- For LLM pretraining using sequence lengths, `padded_batch` with truncation/padding is typical.\n",
    "\n",
    "## shuffle and buffer size\n",
    "\n",
    "- `shuffle(buffer_size)` maintains a reservoir of `buffer_size` samples; draws uniformly from it.\n",
    "- Larger `buffer_size` => better mixing but more memory and startup delay.\n",
    "\n",
    "**Common strategies:**\n",
    "- For huge datasets use `buffer_size = 1000000` or a multiple of per-worker dataset size when memory allows.\n",
    "- For very large corpora where full-shuffle is impossible, use file-level shuffle (shuffle list of files) + intra-file partial shuffle.\n",
    "\n",
    "**Pattern:**\n",
    "```python\n",
    "files = tf.data.Dataset.list_files(\"data/train-*.tfrecord\")\n",
    "files = files.shuffle(buffer_size=1000)\n",
    "ds = files.interleave(lambda f: tf.data.TFRecordDataset(f), \n",
    "                      cycle_length=16, \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds = ds.shuffle(buffer_size=10000)  # local shuffle\n",
    "```\n",
    "\n",
    "`interleave` + file shuffle is effective: it mixes records from different shards early.\n",
    "\n",
    "## cache() — when and when not to use\n",
    "\n",
    "- `cache()` stores the results of previous transforms to speed subsequent epochs.\n",
    "- Use when dataset fits RAM or you can use disk-backed cache (`ds.cache(filename)`).\n",
    "- **Do not cache huge corpora in memory** — it will OOM and slow system.\n",
    "\n",
    "**Common pattern:**\n",
    "- If you have a small validation set, `val_ds = val_ds.cache()` for fast evaluation.\n",
    "- For pretraining on very large corpora, do not cache entire dataset. Instead rely on sharding, interleave, and parallel map.\n",
    "\n",
    "## Sliding window for sequence generation (critical for LLM pretraining)\n",
    "\n",
    "LLM training often converts long token streams into many overlapping context windows.\n",
    "\n",
    "**Two approaches:**\n",
    "1. Pre-chunk offline into fixed windows and save as TFRecords.\n",
    "2. Use `tf.data` sliding windows on token streams.\n",
    "\n",
    "**Sliding window example:**\n",
    "```python\n",
    "tokens = tf.data.Dataset.from_tensor_slices(token_ids)  # long 1-D stream\n",
    "\n",
    "seq_length = 2048\n",
    "# window dataset: create overlapping windows with shift\n",
    "windows = tokens.window(size=seq_length, shift=seq_length, drop_remainder=True)\n",
    "# For overlapping with stride < seq_length use shift < seq_length\n",
    "\n",
    "# Convert windows (Dataset of Datasets) to batched tensors\n",
    "windows = windows.flat_map(lambda w: w.batch(seq_length))\n",
    "# now windows yields tensors shape (seq_length,)\n",
    "```\n",
    "\n",
    "For overlapping windows with step stride:\n",
    "```python\n",
    "windows = tokens.window(size=seq_length, shift=stride, drop_remainder=True)\n",
    "windows = windows.flat_map(lambda w: w.batch(seq_length))\n",
    "```\n",
    "\n",
    "**Alternative:** using `tf.signal.frame` (works on tensors, not datasets) or vectorized ops: convert long arrays to 2D matrix of frames.\n",
    "\n",
    "**Important:** sliding windows produce many overlapping examples — this is expected for causal LM training but increases dataset size. Precompute and shard offline if possible for efficiency.\n",
    "\n",
    "## Example pipeline putting it all together (text -> tokenization -> batching)\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "files = tf.io.gfile.glob(\"gs://bucket/train-*.tfrecord\")\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(files).shuffle(len(files))\n",
    "ds = files_ds.interleave(lambda f: tf.data.TFRecordDataset(f),\n",
    "                         cycle_length=16, \n",
    "                         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "ds = ds.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# tokenization: prefer TF-native tokenizers (tensorflow_text or KerasNLP) inside graph\n",
    "# if using Python tokenizer use tf.py_function wrapped carefully\n",
    "def tokenize_map(text, label):\n",
    "    # assume tokenizer_fn is pure-TF or tf.py_function wrapper\n",
    "    input_ids = tokenizer_fn(text)  # returns 1D int tensor\n",
    "    return input_ids, label\n",
    "\n",
    "ds = ds.map(tokenize_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# pack into sequences or use padded_batch\n",
    "ds = ds.padded_batch(batch_size, padded_shapes=([max_len], []), drop_remainder=True)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c7ed7",
   "metadata": {},
   "source": [
    "# Sharding Basics\n",
    "\n",
    "Scaling across devices and workers needs correct sharding to avoid duplicated training data and maximize parallelism.\n",
    "\n",
    "## Automatic sharding with tf.distribute\n",
    "\n",
    "When you use `tf.distribute.Strategy` (`MirroredStrategy`, `MultiWorkerMirroredStrategy`), TensorFlow can perform automatic dataset sharding if you pass a dataset created for distribution.\n",
    "\n",
    "- For `model.fit(dataset, ...)` with a Strategy active, Keras expects that the dataset yields global batches (size = `batch_per_replica * num_replicas_in_sync`) or that the dataset is already sharded per worker.\n",
    "- With `MultiWorkerMirroredStrategy`, set environment variable `TF_CONFIG` and rely on automatic sharding when using `dataset = strategy.experimental_distribute_dataset(dataset)`.\n",
    "\n",
    "**Important:** automatic sharding works only when dataset is shaped properly and you use `experimental_distribute_dataset` or `model.fit` under the strategy.\n",
    "\n",
    "## How dataset.batch interacts with strategy\n",
    "\n",
    "If you call `dataset.batch(global_batch_size)` and then pass the dataset to `strategy.run` or `model.fit` inside a `MirroredStrategy` context, TensorFlow will split each batch across replicas. \n",
    "\n",
    "**Example:**\n",
    "- `global_batch_size = per_replica_batch * num_replicas`.\n",
    "- `dataset.batch(global_batch_size)` yields `[global_batch_size, ...]` tensors; TF runtime splits into per-replica sub-batches.\n",
    "\n",
    "If you instead call `dataset.batch(per_replica_batch)` and then use `strategy.experimental_distribute_dataset`, TF will automatically concatenate per-replica batches across workers which may cause duplication. \n",
    "\n",
    "**Best practice:** always batch with a global batch size.\n",
    "\n",
    "## Why each worker should get unique shards\n",
    "\n",
    "If all workers read the same sequence of files without sharding, they will train on exactly the same samples, effectively multiplying gradient steps on the same data — breaks randomness and slows convergence.\n",
    "\n",
    "Unique sharding per worker ensures independent samples and correct effective epoch size.\n",
    "\n",
    "### How to shard:\n",
    "\n",
    "- Use `tf.data.Dataset.shard(num_shards, index)` when you have manual worker id and worker count:\n",
    "```python\n",
    "ds = ds.shard(num_shards=num_workers, index=worker_id)\n",
    "```\n",
    "\n",
    "- Or use **file-level sharding**: on worker i, read `files[i::num_workers]` only.\n",
    "- With `tf.distribute.experimental.MultiWorkerMirroredStrategy`, prefer letting TF handle sharding: create dataset of filenames and use `tf.data.experimental.AutoShardPolicy` (default) or `dataset = dataset.shard(num_shards, task_index)` before interleave.\n",
    "\n",
    "## Practical multi-worker pipeline template\n",
    "```python\n",
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "def make_dataset(file_pattern, batch_size, worker_id, num_workers):\n",
    "    files = tf.io.gfile.glob(file_pattern)\n",
    "    # file-level sharding: each worker reads a subset of files\n",
    "    files = sorted(files)\n",
    "    files = files[worker_id::num_workers]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = ds.interleave(lambda f: tf.data.TFRecordDataset(f),\n",
    "                       cycle_length=16, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    model.compile(...)\n",
    "    dataset = make_dataset(\"gs://bucket/train-*.tfrecord\", global_batch_size, worker_id, num_workers)\n",
    "    model.fit(dataset, epochs=..., steps_per_epoch=...)\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "- `worker_id` and `num_workers` typically come from the cluster manager or `TF_CONFIG`.\n",
    "- `global_batch_size` equals `per_replica_batch * num_replicas_in_sync`.\n",
    "\n",
    "## Practical tips, diagnostics, and benchmarks\n",
    "\n",
    "### Measure throughput\n",
    "\n",
    "Simple: iterate N batches on dataset and time it.\n",
    "```python\n",
    "import time\n",
    "it = iter(dataset)\n",
    "t0 = time.time()\n",
    "num_samples = 0\n",
    "for i in range(100):\n",
    "    x, y = next(it)\n",
    "    num_samples += x.shape[0]\n",
    "elapsed = time.time() - t0\n",
    "print(\"samples/sec:\", num_samples / elapsed)\n",
    "```\n",
    "\n",
    "If `samples/sec < GPU utilization capacity`, pipeline is bottlenecked.\n",
    "\n",
    "### Profiling data pipeline\n",
    "\n",
    "Use TensorBoard profiler and look at CPU activity, file I/O, and thread usage.\n",
    "\n",
    "If CPU bound, increase `num_parallel_calls` and the number of `interleave cycle_length`.\n",
    "\n",
    "### Tokenizer placement\n",
    "\n",
    "Prefer TF-native tokenizers (`tensorflow_text`, `KerasNLP`) to avoid Python overhead. When using Python tokenizers, pre-tokenize to TFRecords or use `tf.py_function` sparingly with parallel map.\n",
    "\n",
    "## Performance checklist\n",
    "\n",
    "- Use sharded TFRecord files.\n",
    "- Use `tf.data.TFRecordDataset(files, num_parallel_reads=AUTOTUNE)`.\n",
    "- Use `interleave` with `cycle_length` tuned (16 or 32 for large datasets).\n",
    "- Use `map(..., num_parallel_calls=AUTOTUNE)`.\n",
    "- Use file-level shuffle plus local sample-level shuffle.\n",
    "- Use `padded_batch` with `drop_remainder=True` for multi-GPU.\n",
    "- Use `prefetch(AUTOTUNE)`.\n",
    "- If tokenization is heavy, pre-tokenize into TFRecords with `input_ids`.\n",
    "- Monitor samples/sec and system CPU/GPU utilization.\n",
    "\n",
    "## Common pitfalls and how to fix them\n",
    "\n",
    "- **Low throughput:** Often due to tokenization in Python. Fix by using TF tokenizers or pre-tokenize, increase `num_parallel_calls`, or add more `interleave` parallelism.\n",
    "- **OOM during cache:** Do not use `ds.cache()` on datasets that do not fit memory. Use disk cache or avoid caching.\n",
    "- **Data duplication across workers:** Happens when files are not sharded per worker. Use file-level sharding or `Dataset.shard`.\n",
    "- **Incorrect batch sizes for distributed training:** Always reason in global batch size and compute per-replica batch accordingly.\n",
    "- **Retracing or slow startup:** Avoid complicated Python logic inside map functions that cause retracing; provide static shapes where possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
