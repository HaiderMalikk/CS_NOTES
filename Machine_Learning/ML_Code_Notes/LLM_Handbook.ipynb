{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd61ec",
   "metadata": {},
   "source": [
    "# Complete LLM Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3429ce4",
   "metadata": {},
   "source": [
    "Install Dependencies to run code blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras keras_nlp matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f5fca",
   "metadata": {},
   "source": [
    "## Sections \n",
    "\n",
    "### [1 Tokenization, vocab, sequence formatting](#1-Tokenization,-vocab,-sequence-formatting)\n",
    "- [1.1 Byte level words vs Subword tokenization](#1.1-Byte-level-words-vs-Subword-tokenization)\n",
    "  - [1.1.1 Byte level tokenization](#1.1.1-Byte-level-tokenization)\n",
    "  - [1.1.2 Subword tokenization (BPE, WordPiece, SentencePiece)](#1.1.2-Subword-tokenization-(BPE,-WordPiece,-SentencePiece))\n",
    "- [1.2 Token Ids and Vocabulary Size](#1.2-Token-Ids-and-Vocabulary-Size)\n",
    "- [1.3 Padding and Masking](#1.3-Padding-and-Masking)\n",
    "- [1.4 Special Tokens](#1.4-Special-Tokens)\n",
    "- [1.5 Sequence Packing and Contiguous Streams](#1.5-Sequence-Packing-and-Contiguous-Streams)\n",
    "- [1.6 Sliding Window Chunking](#1.6-Sliding-Window-Chunking)\n",
    "- [1.7 Complete Example: Combining All Tokenization Steps](#1.7-Complete-Example:-Combining-All-Tokenization-Steps)\n",
    "\n",
    "### [2 Embedding and Unembedding](#2-Embedding-and-Unembedding)\n",
    "- [2.1 Word embedding lookup tables](#2.1-Word-embedding-lookup-tables)\n",
    "- [2.2 Unembedding and tied embeddings](#2.2-Unembedding-and-tied-embeddings)\n",
    "  - [2.2.1 Unembedding](#2.2.1-Unembedding)\n",
    "  - [2.2.2 Tied Embeddings](#2.2.2-Tied-Embeddings)\n",
    "- [2.3 Why positional representations are required](#2.3-Why-positional-representations-are-required)\n",
    "- [2.4 Positional encoding types](#2.4-Positional-encoding-types)\n",
    "  - [2.4.1 Sinusoidal positional encoding](#2.4.1-Sinusodal-positional-encoding)\n",
    "  - [2.4.2 Learned positional embeddings](#2.4.2-Learned-positional-embeddings)\n",
    "  - [2.4.3 Rotary Position Embeddings (RoPE)](#2.4.3-Rotary-Position-Embeddings-(RoPE))\n",
    "  - [2.4.4 ALiBi](#2.4.4-ALiBi)\n",
    "- [2.5 How positional encoding interacts with attention](#2.5-How-positional-encoding-interacts-with-attention)\n",
    "- [2.6 Embedding scaling by sqrt(d_model)](#2.6-Embedding-scaling-by-sqrt(d_model))\n",
    "- [2.7 Complete Example: Combining All Embedding Steps](#2.7-Complete-Example:-Combining-All-Embedding-Steps)\n",
    "\n",
    "### [3 Attention](#3-Attention)\n",
    "- [3.1 Query, Key, Value fundamentals](#3.1-Query,-Key,-Value-fundamentals)\n",
    "- [3.2 Dot product attention](#3.2-Dot-product-attention)\n",
    "- [3.3 Why divide by âˆšdk](#3.3-Why-divide-by-âˆšdk)\n",
    "- [3.4 Causal masking and autoregressive behavior](#3.4-Causal-masking-and-autoregressive-behavior)\n",
    "- [3.5 Softmax details and numerical stability](#3.5-Softmax-details-and-numerical-stability)\n",
    "- [3.6 Multi-head attention](#3.6-Multi-head-attention)\n",
    "- [3.7 Attention complexity](#3.7-Attention-complexity)\n",
    "- [3.8 Memory layout and tensor shapes](#3.8-Memory-layout-and-tensor-shapes)\n",
    "- [3.9 Flash Attention (conceptual)](#3.9-Flash-Attention-(conceptual))\n",
    "- [3.10 Self attention vs cross attention](#3.10-Self-attention-vs-cross-attention)\n",
    "- [3.11 Key-value caching for generation](#3.11-Key-value-caching-for-generation)\n",
    "- [3.12 TensorFlow Examples](#3.12-Tensorflow-Example-(no-raw-kernal-high-level-usage))\n",
    "  - [3.12.1 Single-head attention](#3.12.1-Single-head-attention-(conceptual-TF))\n",
    "  - [3.12.2 Multi-head attention](#3.12.2-Multi-head-attention-(high-level))\n",
    "  - [3.12.3 Causal mask](#3.12.3-Causal-mask)\n",
    "  - [3.12.4 Multi-head attention demo](#3.12.4-Multi-Head-Attention-in-Action:-Example)\n",
    "  - [3.12.5 What transformers do: context integration](#3.12.5-What-Transformers-Do-and-what-do-(context-integration))\n",
    "\n",
    "### [4 Feed Forward Networks (MLP block)](#4-Feed-Forward-Networks-(MLP-block))\n",
    "- [4.1 What the FFN is really doing](#4.1-What-the-FFN-is-really-doing)\n",
    "- [4.2 Two-layer MLP structure](#4.2-Two-layer-MLP-structure)\n",
    "- [4.3 Activation functions and why GELU matters](#4.3-Activation-functions-and-why-GELU-matters)\n",
    "  - [4.3.1 ReLU](#4.3.1-ReLU)\n",
    "  - [4.3.2 GELU (Gaussian Error Linear Unit)](#4.3.2-GELU-(Gaussian-Error-Linear-Unit))\n",
    "- [4.4 Modern variants: SwiGLU](#4.4-Modern-variants:-SwiGLU)\n",
    "- [4.5 Dropout and residual scaling](#4.5-Dropout-and-residual-scaling)\n",
    "- [4.6 TensorFlow: Configurable FFN block (high level)](#4.6-TensorFlow:-Configurable-FFN-block-(high-level))\n",
    "\n",
    "### [5 Normalization and Stabilization in Transformers](#5.-Normalization-and-Stabilization-in-Transformers)\n",
    "- [5.1 Why normalization is critical in deep transformers](#5.1-Why-normalization-is-critical-in-deep-transformers)\n",
    "- [5.2 LayerNorm inside transformers](#5.2-LayerNorm-inside-transformers)\n",
    "- [5.3 Post-Norm vs Pre-Norm](#5.3-Post-Norm-vs-Pre-Norm)\n",
    "  - [5.3.1 Post-Norm (original Transformer)](#5.3.1-Post-Norm-(original-Transformer))\n",
    "  - [5.3.2 Pre-Norm (modern standard)](#5.3.2-Pre-Norm-(modern-standard))\n",
    "- [5.4 RMSNorm](#5.4-RMSNorm)\n",
    "- [5.5 Why normalization stabilizes attention and FFNs](#5.5-Why-normalization-stabilizes-attention-and-FFNs)\n",
    "- [5.6 Initialization strategies for stability](#5.6-Initialization-strategies-for-stability)\n",
    "- [5.7 Residual connection depth scaling](#5.7-Residual-connection-depth-scaling)\n",
    "- [5.8 TensorFlow: Pre-Norm transformer components (high level)](#5.8-TensorFlow:-Pre-Norm-transformer-components-(high-level))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5011d5",
   "metadata": {},
   "source": [
    "## 1 Tokenization, vocab, sequence formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36007de8",
   "metadata": {},
   "source": [
    "### 1.1 Byte level words vs Subword tokenization \n",
    "\n",
    "**Why**: Transformers cannot process raw text, text must be converted into numbers. The way we break text into tokens affects efficency, generalization and memory usage\n",
    "\n",
    "#### 1.1.1 Byte level tokenization\n",
    "\n",
    "- works at the byte level (0-255)\n",
    "- Real world usage: GPT-2 uses byte pair encoding (BPE) at byte level\n",
    "- Pros:\n",
    "  - Handels any charecter, any language, emojis, symbol\n",
    "  - no OOV (out of vocab) tokens\n",
    "- Cons:\n",
    "  - Toekn sequences can be longer -> means more compute \n",
    "- Example: \"hello ðŸ‘‹\" â€“> [104, 101, 108, 108, 111, 32, 240, 159, 145, 139] (in token ids where range is 0 -> vocab_size)\n",
    "\n",
    "#### 1.1.2 Subword tokenization (BPE, WordPeice, SentencePiece)\n",
    "\n",
    "-  Breaks Text into frequent subwords insted of characters or words. \n",
    "-  Example: \n",
    "   -  \"unhappiness\" -> [\"un\", \"happi\", \"ness\"] -> [217, 9812, 403] # in token ids (range is 0 -> vocab size)\n",
    "- Pros: \n",
    "  - Shorter sequences than byte\n",
    "  - Can handle rare words via subword decomposition (breaking unknown words into known smaller parts)\n",
    "- Cons:\n",
    "  - some complexity in building vocab and handling edge cases\n",
    "  \n",
    "**NOTE:** LLM's often use subword BPE (BPE applied at the subword level) it iteratively merges the most frequent character or subword pairs to build a vocabulary, balancing between character-level and word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce1ea",
   "metadata": {},
   "source": [
    "### 1.2 Token Ids and Vocabulary Size\n",
    "- After Tokenization, each token is mapped to a integer ID using a vocabulary\n",
    "- Vocabulary size (V) is very important\n",
    "  - Larger V -> model must have a bigger embedding matrix (page 50 in written notes) -> more parameters (hence a larger model)\n",
    "  - Smaller V -> more subword splitting (words broken into more pieces) -> longer sequences -> slower training (but smaller model size)\n",
    "- Typical LLM vocab sizes: 30K-100K for english models \n",
    "- Example: In TensorFlow, keras_nlp.tokenizers handles both mapping tokens â†’ IDs and IDs â†’ tokens.\n",
    "\n",
    "``` py\n",
    "from keras_nlp.tokenizers import BytePairTokenizer\n",
    "\n",
    "tokenizer = BytePairTokenizer(vocabulary=[\"hello\", \"world\", \"un\", \"happi\", \"ness\", \"<PAD>\", \"<BOS>\", \"<EOS>\"])\n",
    "tokens = tokenizer.tokenize([\"hello world\", \"unhappiness\"])\n",
    "token_ids = tokenizer(tokens)\n",
    "print(token_ids)\n",
    "\n",
    "```\n",
    "\n",
    "**How Keras NLP Tokenizers Handle Token â†” ID Mapping** Under the hood, Keras NLP tokenizers maintain two key data structures (`token_to_id` and `id_to_token`) for bidirectional mapping. When you call `tokenizer.tokenize(text)`, it returns tokens as strings; `tokenizer(text)` returns token IDs; and `tokenizer.detokenize(ids)` converts IDs back to text. The vocabulary is built during training or loaded from a pre-trained model, with special tokens (PAD, UNK, BOS, EOS) typically assigned fixed IDs at the beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5d852",
   "metadata": {},
   "source": [
    "### 1.3 Padding and Masking \n",
    "1. Padding: Short sequences are extended with PAD tokens to match the longest sequence in a batch, enabling efficient parallel processing (e.g., `[5, 10, 15]` â†’ `[5, 10, 15, <PAD>, <PAD>]`)\n",
    "\n",
    "2. Attention Masking: Tells the transformer which positions to ignore during attention.\n",
    "- **No Mask (Bidirectional)**: All tokens attend to all tokens; used in BERT for full context understanding\n",
    "- **Causal Mask (Autoregressive)**: Each token only attends to previous tokens; used in GPT to prevent future information leakage during training\n",
    "- **Padding Mask**: Masks PAD tokens so they don't affect attention scores; combined with other masks in most models\n",
    "\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example: batch of token IDs (here each array of token ids in a batch is a sqeuence i.e one example, by spliting in batches we can proccess in parallel)\n",
    "batch = tf.ragged.constant([\n",
    "    [1, 2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "padded = batch.to_tensor(default_value=0) # Output: [[1, 2, 3], [4, 5, 0]]  <- 0 is the PAD token ID (these are the new tokens)\n",
    "mask = tf.cast(padded != 0, tf.int32) # Output: [[1, 1, 1], [1, 1, 0]]  <- tells attention to ignore the last position in sequence 2 (this is the attention scores not token values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47c9fa",
   "metadata": {},
   "source": [
    "### 1.4 Special Tokens (see ML notes page 121)\n",
    "- `<BOS>`: Beginning of sequence (marks where a sequence starts)\n",
    "- `<EOS>`: End of sequence (marks where a sequence ends)\n",
    "- `<PAD>`: Padding (fills shorter sequences to match batch length)\n",
    "- `<UNK>`: Unknown/ out of vocab token (represents words not in vocabulary)\n",
    "- etc\n",
    "\n",
    "**usage in training**\n",
    "``` text\n",
    "Input:  <BOS> hello world <EOS> <PAD> <PAD>    # BOS is fed as a conditioning token ((a special input token that provides initial context/prompt for the model; the model conditions its next-token predictions on it but is not trained to predict it)) EOS is included so the model learns to predict sequence end PADs fill to uniform length\n",
    "\n",
    "Target: hello world <EOS> <PAD> <PAD> <PAD>   # Target = input shifted left (model predicts the next token at each step, including EOS); PADs fill to uniform length\n",
    "\n",
    "Mask:   1 1 1 1 0 0 0                          # Mask=1 for positions to compute loss (we compute loss for real tokens and EOS), 0 for PADs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde25d45",
   "metadata": {},
   "source": [
    "### 1.5 Sequence Packing and Contiguous Streams \n",
    "- Why: LLM training is compute-heavy, to use memory efficiently, multiple short examples can be concatenated into a single long sequence and then chunked\n",
    "- Benefits: \n",
    "  - Reduces wasted padding\n",
    "  - keepinh sequences dense for attention\n",
    "  \n",
    "**Example (pseudo)**\n",
    "```text\n",
    "Examples: [\"hello\", \"world\"], [\"goodbye\", \"moon\"]\n",
    "Packed sequence: \"hello world goodbye moon\"\n",
    "```\n",
    "- Then split into fixed length chunks (ex: 8 tokens per chunk) for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2eaec",
   "metadata": {},
   "source": [
    "### 1.6 Sliding Window Chunking \n",
    "- When text is too long to fit in memory, we create overlapping windown to preserve context\n",
    "- why: prevents cutting off dependencies between sequences.\n",
    "- Example Sequence length = 6, chunk size = 4, stride = 2\n",
    "``` text\n",
    "Sequence: [A B C D E F] (len = 6)\n",
    "\n",
    "Chunk 1: Start at position 0 â†’ [A B C D] (len = 4 beacuse chunk size = 4)\n",
    "Chunk 2: Start at position 0 (position) + 2(stride) = 2 â†’ [C D E F] (move the window by 2, keeps last 2 tokens of last chink in new chunk)\n",
    "\n",
    "Chunks:  [A B C D], [C D E F]\n",
    "\n",
    "Result Sequence: [A B C D], [C D E F]\n",
    "                      â†‘overlapâ†‘\n",
    "```\n",
    "- Edge case: If the final window doesn't have enough tokens (e.g., only 3 tokens left for chunk_size=4), you either pad it with `<PAD>` tokens or discard it depending on your training strategy.\n",
    "- overlapping ensures context continuity for training \n",
    "- common in LLM pretraining \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff521f58",
   "metadata": {},
   "source": [
    "### 1.7 Complete Example: Combining All Tokenization Steps\n",
    "\n",
    "This example demonstrates the entire pipeline from raw text to training-ready sequences, incorporating all concepts from 1.1-1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2957877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer (handles vocab, special tokens, BPE subword)\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Create dataset pipeline: tokenization â†’ chunking â†’ padding\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices([\"Hello world\", \"Goodbye moon\"])\n",
    "    .map(tokenizer)  # Tokenize: text â†’ IDs\n",
    "    .unbatch()  # Flatten to token stream (packing)\n",
    "    .batch(8, drop_remainder=False)  # Chunk into sequences of 8 tokens\n",
    "    .map(lambda x: (x[:-1], x[1:]))  # Create (input, target) pairs\n",
    "    .padded_batch(2, padded_shapes=([None], [None]))  # Pad and batch\n",
    ")\n",
    "\n",
    "# Usage:\n",
    "inputs, targets = next(iter(dataset))\n",
    "print(tokenizer.detokenize(inputs[0]))\n",
    "print(tokenizer.detokenize(targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14482b70",
   "metadata": {},
   "source": [
    "**OUTPUT:**\n",
    "```\n",
    "Hello worldGoodbye\n",
    "worldGoodbye moon\n",
    "```\n",
    "\n",
    "**EXPLANATION:**\n",
    "\n",
    "The dataset variable is a `tf.data.Dataset` pipeline that transforms raw text into training-ready (input, target) pairs. It's a lazy iterator (doesn't process until called).\n",
    "\n",
    "**Pipeline steps:**\n",
    "1. **from_tensor_slices**: Creates dataset from list of strings\n",
    "2. **map(tokenizer)**: Converts each text â†’ token IDs (e.g., \"Hello\" â†’ [15496, 995])\n",
    "3. **unbatch()**: Flattens all sequences into one continuous token stream (sequence packing)\n",
    "4. **batch(8)**: Groups tokens into chunks of 8 (creates fixed-length sequences)\n",
    "5. **map(lambda)**: Splits each chunk into (input, target) where target = input shifted left\n",
    "6. **padded_batch(2)**: Groups 2 sequences into a batch, pads shorter ones to match length\n",
    "\n",
    "**How next-token prediction works:**\n",
    "\n",
    "The model predicts the NEXT token at EACH position, not just the last one:\n",
    "- Position 0: Given \"Hello\" â†’ predict \"world\"\n",
    "- Position 1: Given \"Hello world\" â†’ predict \"Goodbye\"  \n",
    "- Position 2: Given \"Hello worldGoodbye\" â†’ predict \"moon\"\n",
    "\n",
    "So the target sequence shows what should be predicted at each step. The entire target = input shifted left by 1 token (each target is the next token).\n",
    "\n",
    "**Chunking Strategy Comparison:**\n",
    "\n",
    "Token stream: `[A, B, C, D, E, F, G, H, I, J]`\n",
    "\n",
    "**Fixed batch** (current): `.batch(4)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[E, F, G, H]` (tokens 4-7)\n",
    "- Chunk 3: `[I, J]` (tokens 8-9)\n",
    "- â†’ No overlap, each token appears once\n",
    "\n",
    "**Sliding window**: `.window(size=4, shift=2, drop_remainder=True)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[C, D, E, F]` (tokens 2-5, overlaps last 2 from chunk 1)\n",
    "- Chunk 3: `[E, F, G, H]` (tokens 4-7, overlaps last 2 from chunk 2)\n",
    "- â†’ Overlap preserves context across chunks, useful for long documents\n",
    "\n",
    "**IMPORTANT: Both strategies train on next-token prediction at EVERY position!**\n",
    "\n",
    "**Fixed batch:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[E,F,G,H]` â†’ trains: (Eâ†’F), (E,Fâ†’G), (E,F,Gâ†’H)\n",
    "- Each token appears ONCE\n",
    "\n",
    "**Sliding window:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[C,D,E,F]` â†’ trains: (Câ†’D), (C,Dâ†’E), (C,D,Eâ†’F)\n",
    "- Tokens C and D appear TWICE (extra training for better context)\n",
    "\n",
    "**Key Takeaway:** The chunking method only affects which tokens are grouped together, not how training works. Sliding window gives overlapping tokens extra exposure for better long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d86c5a",
   "metadata": {},
   "source": [
    "## 2 Embedding and Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7187be",
   "metadata": {},
   "source": [
    "### 2.1 Word embedding lookup tables\n",
    "\n",
    "**What a Embedding is**\n",
    "\n",
    "- An embedding is just a trainable lookup table:\n",
    "  - Shape: (vocab_size, d_model)\n",
    "  - Input: token ID's of shape (batch, seq_len)\n",
    "  - Output: vectors of shape (batch, seq_len, d_model)\n",
    "\n",
    "Each token ID indexes one row, the learning happens because gradients update those rows based on prediction error\n",
    "\n",
    "**Traning Proccess**: How Embeddings Are Learned\n",
    "\n",
    "1. **Random Initialization**: Embedding table starts with random values (shape: `vocab_size Ã— d_model`)\n",
    "\n",
    "2. **Forward Pass**: \n",
    "    - Token IDs â†’ Look up embeddings â†’ Feed through transformer â†’ Predict next token\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare prediction to actual next token\n",
    "    - Compute cross-entropy loss\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back through the entire model\n",
    "    - Embedding table rows get gradient updates based on which tokens were used\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    embedding[token_id] -= learning_rate Ã— gradient[token_id]\n",
    "    ```\n",
    "\n",
    "**Key Insight**: Tokens that appear in similar contexts will develop similar embeddings because they receive similar gradient updates. For example:\n",
    "- \"cat\" and \"dog\" â†’ often surrounded by words like \"pet\", \"animal\" â†’ embeddings become similar\n",
    "- \"king\" and \"queen\" â†’ share contexts like \"royal\", \"throne\" â†’ learn related representations\n",
    "\n",
    "The model learns embeddings **jointly** with all other parameters (attention weights, feedforward layers) to minimize prediction error across the entire training corpus.\n",
    "\n",
    "**Example Matrix:**\n",
    "``` text\n",
    "vocab_size = 50000  # Total unique tokens in vocabulary\n",
    "d_model = 512       # Each token â†’ 512-dimensional vector\n",
    "embedding_table = tf.Variable(shape=(50000, 512))\n",
    "# If token ID = 42, its embedding is embedding_table[42] (a 512-length vector)\n",
    "```\n",
    "\n",
    "In short the embedding is the models repersentation of a token, attention uses these repersentations to operate\n",
    "\n",
    "**TensorFlow Example**\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenEmbedding(tf.keras.layers.Layer): # inherit tf Layer Class\n",
    "    def __init__(self, vocab_size, d_model): # init model\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding( # create embedding layer\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=d_model\n",
    "        )\n",
    "\n",
    "    # override call function with out custom embeddign layer\n",
    "    def call(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "```\n",
    "\n",
    "Example IO:\n",
    "```text\n",
    "input:  [12,   431,   98] # token ID's\n",
    "\n",
    "# each token gets a embedding returned (output[i] = E[token_ids[i]]) where 'E' is teh embedding matrix\n",
    "output: [\n",
    "  E[12],     # âˆˆ R^d_model here 'R' is (real numbers) and 'd_model' is the number of dimentions in the model its a hyperparameter you chose (like 64, 128, 512, etc) for ex R^4 = [x1, x2, x3, x4] x can be any real number\n",
    "  E[431],    # âˆˆ R^d_model\n",
    "  E[98]      # âˆˆ R^d_model\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032ee2e",
   "metadata": {},
   "source": [
    "### 2.2 Unembedding and tied embeddings\n",
    "\n",
    "#### 2.2.1 Unembedding\n",
    "\n",
    "Unembedding converts each transformer output vector into a score for every token in the vocabulary so the model can predict which token comes next. we need this at the output of the transformer\n",
    "```\n",
    "Vocabulary: [\"hello\", \"world\", \"cat\", \"dog\", ...] (ex 50000 words)\n",
    "                  â†“       â†“      â†“     â†“\n",
    "Logits:          [2.1,   0.5,  -1.2,  3.4, ...]  â† Higher score = more likely next token (50000 scores one for each token)\n",
    "```\n",
    "this is done with linear projection.\n",
    "\n",
    "- Input: (batch, seq_len, d_model)\n",
    "- Weight: (d_model, vocab_size)\n",
    "- Output: (batch, seq_len, vocab_size)\n",
    "\n",
    "**NOTE:** this layer is also called the unembedding or LM head\n",
    "\n",
    "**NOTE:** The converstion of the token ID back to a final word is the tokenizers job do not mistake unebedding for that\n",
    "\n",
    "\n",
    "**Training Process**: How Unembedding Weights Are Learned\n",
    "\n",
    "1. **Random Initialization**: Unembedding matrix starts with random values (shape: `d_model Ã— vocab_size`)\n",
    "\n",
    "2. **Forward Pass**:\n",
    "    - Transformer outputs â†’ (batch, seq_len, d_model)\n",
    "    - Matrix multiply with unembedding weights â†’ (batch, seq_len, vocab_size)\n",
    "    - Apply softmax â†’ probability distribution over vocabulary\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare predicted probabilities to actual next token (one-hot encoded)\n",
    "    - Compute cross-entropy loss: `loss = -log(P(correct_token))`\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back from loss through softmax and unembedding layer\n",
    "    - Each row of the unembedding matrix (corresponding to one output dimension) gets updated based on prediction errors\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    unembedding_weights -= learning_rate Ã— gradient\n",
    "    ```\n",
    "\n",
    "**Key Insight**: The unembedding layer learns which transformer output patterns correspond to which tokens. If the model frequently outputs vectors in a certain direction when \"cat\" should be next, those weights get strengthened to produce higher logits for \"cat\".\n",
    "\n",
    "**Example:**\n",
    "``` text\n",
    "d_model = 512\n",
    "vocab_size = 50000\n",
    "unembedding_matrix = tf.Variable(shape=(512, 50000))\n",
    "\n",
    "# Transformer output: (batch=1, seq_len=1, d_model=512)\n",
    "# After matmul: (batch=1, seq_len=1, vocab_size=50000)\n",
    "# Each position gets 50000 scores (one per possible next token)\n",
    "\n",
    "# Mathamatically\n",
    "\n",
    "# Hidden vector h âˆˆ R^d_model # from tansfromer \n",
    "# Unembedding matrix W âˆˆ R^(d_model Ã— vocab_size) # learned matrix\n",
    "# Logits l = h Â· W   â†’ l âˆˆ R^vocab_size # 50000 logits as a result\n",
    "```\n",
    "\n",
    "**The flow so far is as follows**: Token ID â†’ Embedding â†’ Transformer â†’ Hidden Vector â†’ Unembedding â†’ Logits â†’ Token ID â†’ Word (where we pick the highest logit as next word)\n",
    "\n",
    "**Tensorflow Example**\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenUnembedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        # Linear layer without bias: projects hidden vectors to vocab logits\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # hidden_states: (batch, seq_len, d_model)\n",
    "        # logits: (batch, seq_len, vocab_size)\n",
    "        logits = self.dense(hidden_states)\n",
    "        return logits\n",
    "\n",
    "```\n",
    "\n",
    "#### 2.2.2 Tied Embeddings\n",
    "\n",
    "Modern LLMs tie the input embedding matrix and output projection weights \n",
    "\n",
    "- Why it works:\n",
    "  - the same geometric space is used for reading and writing tokens \n",
    "  - reduces parameters \n",
    "  - improves sample efficiency and stability\n",
    "- Mathamatically\n",
    "  - input embedding: E[token_id]\n",
    "  - output logits: h Â· Eáµ€ (h is the hidden vector)\n",
    "  \n",
    "This enforces symmetry between encoding and decoding\n",
    "\n",
    "**Tensorflow example**\n",
    "``` py\n",
    "class TiedOutputProjection(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        embedding_matrix = self.embedding_layer.embedding.embeddings\n",
    "        logits = tf.einsum(\"btd,vd->btv\", hidden_states, embedding_matrix)\n",
    "        return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a76fb",
   "metadata": {},
   "source": [
    "### 2.3 Why positional representations are required (ML notes pg 112)\n",
    "\n",
    "**Self attention is permutation-invariant.**\n",
    "\n",
    "That means:\n",
    "- â€œcat sat matâ€ and â€œmat sat catâ€ look identical without position.\n",
    "- Order must be injected explicitly.\n",
    "\n",
    "Positions noting sequence index are added or applied to embeddings before attention.\n",
    "\n",
    "**Key idea:**\n",
    "Token meaning + position meaning = input representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b4c2d",
   "metadata": {},
   "source": [
    "### 2.4 Positional encoding types\n",
    "\n",
    "#### 2.4.1 Sinusodal positional encoding (ML Notes pg 112)\n",
    "\n",
    "**Concept**\n",
    "- Deterministic, non-trainable\n",
    "- Uses sine and cosine at different frequencies\n",
    "- Allows extrapolation to longer sequences\n",
    "\n",
    "**Formulas:**\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "**Why it works:**\n",
    "- Relative positions can be inferred from linear combinations\n",
    "- Attention can compute distance relationships\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "import math\n",
    "\n",
    "def sinusoidal_position_encoding(seq_len, d_model):\n",
    "    position = tf.range(seq_len, dtype=tf.float32)[:, None]\n",
    "    div_term = tf.exp(\n",
    "        tf.range(0, d_model, 2, dtype=tf.float32) *\n",
    "        -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    pe = tf.zeros((seq_len, d_model))\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(0, d_model, 2)[:, None],\n",
    "        updates=tf.sin(position * div_term)\n",
    "    )\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(1, d_model, 2)[:, None],\n",
    "        updates=tf.cos(position * div_term)\n",
    "    )\n",
    "    return pe\n",
    "```\n",
    "\n",
    "**added directly to token embeddings**\n",
    "\n",
    "#### 2.4.2 Learned positional embeddings (ML notes pg 112)\n",
    "**Concept**\n",
    "- Position IDs get their own embedding table\n",
    "- Fully learned\n",
    "- Used in GPT-2, BERT\n",
    "\n",
    "**Pros:**\n",
    "- Flexible\n",
    "- Often better on fixed context lengths\n",
    "\n",
    "**Cons:**\n",
    "- Cannot extrapolate beyond max length trained\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "class LearnedPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
    "\n",
    "    def call(self, seq_len):\n",
    "        positions = tf.range(seq_len)\n",
    "        return self.embedding(positions)\n",
    "```\n",
    "\n",
    "#### 2.4.3 Rotary Position Embeddings (RoPE)\n",
    "This is where modern LLMs diverge from early transformers.\n",
    "\n",
    "**Core idea:** RoPE rotates query and key vectors in embedding space based on position.\n",
    "\n",
    "**Key properties:**\n",
    "- Position information is applied inside attention\n",
    "- Enables relative position reasoning\n",
    "- Scales well to long contexts\n",
    "\n",
    "Instead of adding position vectors, we rotate pairs of dimensions:\n",
    "```text\n",
    "embedding vector = [x1, x2] â†’ rotation by angle Î¸( determined by tokens position)\n",
    "x1' = x1*cosÎ¸ - x2*sinÎ¸\n",
    "x2' = x1*sinÎ¸ + x2*cosÎ¸\n",
    "x_rotated = [x1', x2']\n",
    "```\n",
    "\n",
    "**Why this is powerful**\n",
    "- Dot products encode relative distance naturally when dot product of K,Q are rotated\n",
    "- No learned position embeddings\n",
    "- Better extrapolation\n",
    "\n",
    "**TensorFlow implementation**\n",
    "```py\n",
    "def rotary_embedding(x, seq_len):\n",
    "    d_model = x.shape[-1]\n",
    "    half = d_model // 2\n",
    "\n",
    "    freqs = tf.exp(\n",
    "        -tf.range(0, half, dtype=tf.float32) / half * tf.math.log(10000.0)\n",
    "    )\n",
    "    positions = tf.range(seq_len, dtype=tf.float32)\n",
    "    angles = positions[:, None] * freqs[None, :]\n",
    "\n",
    "    sin = tf.sin(angles)\n",
    "    cos = tf.cos(angles)\n",
    "\n",
    "    x1 = x[..., :half]\n",
    "    x2 = x[..., half:]\n",
    "\n",
    "    rotated = tf.concat(\n",
    "        [x1 * cos - x2 * sin,\n",
    "         x1 * sin + x2 * cos],\n",
    "        axis=-1\n",
    "    )\n",
    "    return rotated\n",
    "```\n",
    "**Applied to queries and keys only, never values.**\n",
    "\n",
    "#### 2.4.4 ALiBi\n",
    "**Concept**\n",
    "- No position embeddings at all\n",
    "- Adds a linear bias to attention scores\n",
    "- Penalizes distant tokens\n",
    "\n",
    "**Attention score becomes:**\n",
    "``` text\n",
    "QKáµ€ / sqrt(d) + bias(distance)\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Extremely simple\n",
    "- Strong extrapolation to long sequences\n",
    "- Used in MPT and others\n",
    "\n",
    "**NOTE:** ALiBi changes attention, not embeddings.\n",
    "\n",
    "**You do not add position vectors at input.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8b1e2",
   "metadata": {},
   "source": [
    "### 2.5 How positional encoding interacts with attention\n",
    "**Important clarity:**\n",
    "- Additive encodings modify token representations before attention.\n",
    "- RoPE modifies query and key geometry.\n",
    "- ALiBi modifies attention logits directly.\n",
    "\n",
    "**All three inject order, but at different stages.**\n",
    "\n",
    "**This choice affects:**\n",
    "- Long context scaling\n",
    "- Memory behavior\n",
    "- Generalization beyond training length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786ff05",
   "metadata": {},
   "source": [
    "### 2.6 Embedding scaling by sqrt(d_model)\n",
    "\n",
    "When embeddings are initialized, their variance is small.\n",
    "\n",
    "If we add positional encodings directly, they can dominate early training.\n",
    "\n",
    "**Standard fix:**\n",
    "```text\n",
    "x = embedding(token_ids) * sqrt(d_model)\n",
    "x = x + position_encoding\n",
    "```\n",
    "\n",
    "**This ensures:**\n",
    "- Token identity dominates initially\n",
    "- Position is a refinement, not the signal\n",
    "\n",
    "**TF snippet**\n",
    "```py\n",
    "x = token_embedding(token_ids)\n",
    "x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "x += position_encoding\n",
    "```\n",
    "\n",
    "**This is not cosmetic. It stabilizes training.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbf855",
   "metadata": {},
   "source": [
    "### 2.7 Complete Example: Combining All Embedding Steps\n",
    "\n",
    "This example demonstrates the entire embedding pipeline from token IDs to final logits, a code example for 2.1-2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03605a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Example text: Notice \"cat\" appears at different positions\n",
    "text = \"The cat sat on the mat. I love my cat very much.\"\n",
    "\n",
    "# Step 1: Tokenize (from section 1)\n",
    "token_ids = tokenizer(text) # Token IDs: [ 464 3797 3332  319  262 2603   13  314 1842  616 3797  845  881   13]\n",
    "\n",
    "# Step 2: Token Embedding (2.1) - Convert IDs to vectors\n",
    "token_embedding = backbone.token_embedding # Token Embeddings shape: (14, 768), Each token â†’ 768-dimensional vector\n",
    "token_embeds = token_embedding(token_ids)\n",
    "\n",
    "# Step 3: Position Embedding (2.4) - Add position information\n",
    "position_embedding = backbone.position_embedding # Position Embeddings shape: (14, 768)\n",
    "pos_embeds = position_embedding(token_embeds)\n",
    "\n",
    "# Step 4: Combine (2.5) - Token meaning + Position\n",
    "final_embeds = token_embeds + pos_embeds #  Final Embeddings (token + position): (14, 768)\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Unembedding (2.2) - Project back to vocabulary\n",
    "# ============================================================\n",
    "print(f\"\\n5. Unembedding (tied weights):\")\n",
    "# NOTE: We're using GPT-2's PRETRAINED embedding weights\n",
    "# These were learned on billions of tokens, so predictions are meaningful\n",
    "# In practice, you'd train these from scratch on your data\n",
    "embedding_matrix = token_embedding.embeddings  # Reuse same pretrained weights\n",
    "logits = tf.matmul(final_embeds, embedding_matrix, transpose_b=True) # Logits shape: (14, 50257)\n",
    "predicted_token_ids = tf.argmax(logits, axis=-1)  # Predicted Token IDs shape: (14,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebce668",
   "metadata": {},
   "source": [
    "**SUMMARY: What we covered from sections 2.1-2.6**\n",
    "\n",
    "**1. TOKEN EMBEDDING (2.1):**\n",
    "- Input: Token IDs [464, 3797, ...]\n",
    "- Output: Dense vectors [(768 dims), (768 dims), ...]\n",
    "- â†’ Each token gets a learned representation\n",
    "\n",
    "**2. POSITION EMBEDDING (2.4):**\n",
    "- Learned embeddings that encode position\n",
    "- Position 0 â‰  Position 10 (different vectors)\n",
    "- â†’ Tells model WHERE each token is\n",
    "\n",
    "**3. COMBINING (2.5):**\n",
    "- final = token_embed + pos_embed\n",
    "- â†’ Same word at different positions has different final representations\n",
    "\n",
    "**4. UNEMBEDDING (2.2):**\n",
    "- Projects embeddings â†’ vocabulary logits\n",
    "- Uses TIED WEIGHTS (same matrix as input embeddings)\n",
    "- â†’ Predicts next token distribution\n",
    "\n",
    "**IMPORTANT:** This example uses GPT-2's PRETRAINED embeddings!\n",
    "- These weights were learned on billions of tokens of web text\n",
    "- That's why predictions are meaningful (not random)\n",
    "- In your own model, you'd train these from scratch on your data\n",
    "- Training process: same backpropagation as described in 2.1\n",
    "\n",
    "**Key Insight:** Without position embeddings, \"cat sat mat\" and \"mat sat cat\" would look identical to the model. Position info is CRITICAL.\n",
    "\n",
    "**Note:** We haven't covered transformers yet - that processes these embeddings! The transformer would sit between step 4 (final embeddings) and step 5 (unembedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a45ff",
   "metadata": {},
   "source": [
    "## 3 Attention\n",
    "(ML notes pg 54-61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9acf6e",
   "metadata": {},
   "source": [
    "#### 3.1 Query, Key, Value fundamentals\n",
    "Every token embedding is projected into three different spaces:\n",
    "- Query (Q): what this token is looking for\n",
    "- Key (K): what this token offers\n",
    "- Value (V): the information this token contributes if selected\n",
    "\n",
    "All three come from the same input embedding x, but with different learned linear projections. The input token starts as the same vector x and is transformed using a learned weight matrix\n",
    "\n",
    "**Mathematically:**\n",
    "- Q = x Wq\n",
    "- K = x Wk\n",
    "- V = x Wv\n",
    "  \n",
    "Why this works:\n",
    "- Attention becomes content-addressable memory.\n",
    "- Tokens donâ€™t attend by position or index, but by similarity in meaning.\n",
    "\n",
    "Conceptual analogy:\n",
    "- Query = question\n",
    "- Key = label on a memory slot\n",
    "- Value = content inside the slot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4c411",
   "metadata": {},
   "source": [
    "### 3.2. Dot product attention\n",
    "\n",
    "The core operation:\n",
    "```math\n",
    "attention(Q, K, V) = softmax(Q Káµ€ / âˆšdk) V\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "1. Compute similarity between every query and every key.\n",
    "2. Normalize scores with softmax.\n",
    "3. Use scores to form weighted sums of values.\n",
    "\n",
    "This produces:\n",
    "- For each token, a contextualized representation.\n",
    "- Tokens can pull information from any previous token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38a4ac",
   "metadata": {},
   "source": [
    "### 3.3 Why divide by âˆšdk\n",
    "\n",
    "Without scaling:\n",
    "- Dot products grow with dimension.\n",
    "- Softmax saturates (becomes near one-hot).\n",
    "- Gradients vanish.\n",
    "\n",
    "Scaling by **sqrt(dk)** keeps variance stable.\n",
    "\n",
    "This is not optional. Training becomes unstable without it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1552",
   "metadata": {},
   "source": [
    "### 3.4 Causal masking and autoregressive behavior\n",
    "\n",
    "LLMs generate left to right. A token must not see future tokens during training.\n",
    "\n",
    "Causal mask:\n",
    "- Upper triangular matrix filled with -inf\n",
    "- Added to attention logits before softmax\n",
    "\n",
    "Effect:\n",
    "- Positions j > i get zero probability\n",
    "- Guarantees autoregressive behavior\n",
    "\n",
    "Conceptually:\n",
    "- Training simulates generation\n",
    "- Prediction at position i only depends on < i\n",
    "\n",
    "**Similarity Scores Q Káµ€ (no mask)**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 0.60 | 0.30 | 0.10 |\n",
    "| love       | 0.20 | 0.50 | 0.30 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |\n",
    "\n",
    "**Casual mask**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 1.00 | 0.00 | 0.00 |\n",
    "| love       | 0.29 | 0.71 | 0.00 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15224027",
   "metadata": {},
   "source": [
    "### 3.5 Softmax details and numerical stability\n",
    "\n",
    "Softmax is:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "Numerical issue:\n",
    "- Large logits overflow\n",
    "\n",
    "Standard trick:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max_j x_j)}{\\sum_j \\exp(x_j - \\max_j x_j)}\n",
    "$$\n",
    "This preserves probabilities but stabilizes computation.\n",
    "\n",
    "TensorFlow handles this internally, but you must remember why it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf452b3",
   "metadata": {},
   "source": [
    "### 3.6 Multi-head attention\n",
    "Single head attention has one similarity space basicaly it can only attend to one type of question. see `ML notes pg 61` for more\n",
    "\n",
    "Multi-head attention:\n",
    "- Splits channels into h heads\n",
    "- Each head attends differently\n",
    "- Results are concatenated and projected\n",
    "\n",
    "Shape intuition:\n",
    "```\n",
    "(batch, seq, d_model)\n",
    "â†’ (batch, heads, seq, d_head)\n",
    "```\n",
    "Where:\n",
    "```\n",
    "d_head = d_model / heads\n",
    "```\n",
    "\n",
    "Why this matters:\n",
    "- Different heads learn syntax, semantics, coreference, long-range dependencies\n",
    "- Heads act as independent subspaces\n",
    "\n",
    "**Example**\n",
    "Tokens: T1, T2, T3\n",
    "\n",
    "Head 1 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.1 | 0.3 | 0.6 |\n",
    "| 0.2 | 0.5 | 0.3 |\n",
    "\n",
    "Head 2 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.4 | 0.4 | 0.2 |\n",
    "| 0.3 | 0.3 | 0.4 |\n",
    "\n",
    "Concatenated (along features):\n",
    "| T1         | T2         | T3         |\n",
    "|------------|------------|------------|\n",
    "| 0.1, 0.4   | 0.3, 0.4   | 0.6, 0.2   |\n",
    "| 0.2, 0.3   | 0.5, 0.3   | 0.3, 0.4   |\n",
    "\n",
    "**Multi-Head Attention Combination**\n",
    "\n",
    "Each head computes its own attention output independently:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)\n",
    "$$\n",
    "\n",
    "Outputs of all heads are concatenated along the feature dimension:\n",
    "$$\n",
    "\\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\in \\mathbb{R}^{d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "A final learned linear projection mixes them:\n",
    "$$\n",
    "\\text{MHA\\_output} = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\cdot W_O\n",
    "$$\n",
    "\n",
    "Note: attention scores are not combined across heads; each head attends separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64eab8",
   "metadata": {},
   "source": [
    "### 3.7 Attention complexity\n",
    "\n",
    "Time and memory:\n",
    "```O(seq_lenÂ²)```\n",
    "\n",
    "This is the main scaling bottleneck in LLMs.\n",
    "\n",
    "Implications:\n",
    "- Long context is expensive\n",
    "- Motivates FlashAttention, sparse attention, sliding windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa64d3",
   "metadata": {},
   "source": [
    "### 3.8 Memory layout and tensor shapes (critical)\n",
    "\n",
    "Most bugs in attention come from shape mistakes.\n",
    "\n",
    "Typical shapes:\n",
    "- Q, K, V: (batch, heads, seq_len, d_head) # d_head = d_model / heads\n",
    "- Attention scores: (batch, heads, seq_len, seq_len)\n",
    "- Output: (batch, seq_len, d_model)\n",
    "\n",
    "Reshaping and transposing correctly is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba8c74",
   "metadata": {},
   "source": [
    "### 3.9 Flash Attention (conceptual only)\n",
    "\n",
    "Flash Attention:\n",
    "- Computes attention without materializing full seq_len Ã— seq_len matrix\n",
    "- Uses tiling and fused kernels\n",
    "- Reduces memory bandwidth bottleneck\n",
    "\n",
    "Important takeaway:\n",
    "- Same math, different execution\n",
    "- You do not change the model, only the kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd3832",
   "metadata": {},
   "source": [
    "### 3.10 Self attention vs cross attention\n",
    "\n",
    "- Self attention: Q, K, V come from same sequence\n",
    "- Cross attention: Q from decoder, K/V from encoder\n",
    "\n",
    "GPT style LLMs use only self attention.\n",
    "\n",
    "Encoderâ€“decoder models use both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5147b0",
   "metadata": {},
   "source": [
    "### 3.11 Key-value caching for generation\n",
    "\n",
    "During autoregressive generation:\n",
    "- Past K and V do not change\n",
    "- Only compute Q for new token\n",
    "- Append new K, V to cache\n",
    "\n",
    "Effect:\n",
    "- Reduces per-token cost from O(nÂ²) to O(n)\n",
    "\n",
    "\n",
    "This is essential for fast inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfc97c",
   "metadata": {},
   "source": [
    "### 3.12 Tensorflow Example (no raw kernal high level usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1f50c",
   "metadata": {},
   "source": [
    "#### 3.12.1 Single-head attention (conceptual TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def single_head_attention(x, mask=None):\n",
    "    d_model = x.shape[-1]\n",
    "\n",
    "    q = tf.keras.layers.Dense(d_model)(x)\n",
    "    k = tf.keras.layers.Dense(d_model)(x)\n",
    "    v = tf.keras.layers.Dense(d_model)(x)\n",
    "\n",
    "    scores = tf.matmul(q, k, transpose_b=True)\n",
    "    scores /= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    output = tf.matmul(weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa167c",
   "metadata": {},
   "source": [
    "#### 3.12.2 Multi-head attention (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7494795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        return self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=mask,\n",
    "            training=training\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6333c89",
   "metadata": {},
   "source": [
    "#### 3.12.3 Causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066809e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    mask = tf.linalg.band_part(\n",
    "        tf.ones((seq_len, seq_len)), -1, 0\n",
    "    )\n",
    "    return 1.0 - mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e0269",
   "metadata": {},
   "source": [
    "#### 3.12.4 Multi-Head Attention in Action: Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da922e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Interesting sentence with clear relationships\n",
    "text = \"The cat chased the mouse\"\n",
    "\n",
    "# Tokenize\n",
    "token_ids = tokenizer(text)\n",
    "tokens = [tokenizer.detokenize([tid]).numpy().decode() if isinstance(tokenizer.detokenize([tid]), tf.Tensor) \n",
    "          else tokenizer.detokenize([tid]) for tid in token_ids.numpy()]\n",
    "\n",
    "if tf.rank(token_ids) == 1:\n",
    "    token_ids = token_ids[None, :]\n",
    "\n",
    "# Get embeddings\n",
    "x_tokens = backbone.token_embedding(token_ids)\n",
    "x_pos = backbone.position_embedding(x_tokens)\n",
    "embeddings = x_tokens + x_pos\n",
    "\n",
    "# Pass through FIRST transformer layer (uses pretrained attention weights!)\n",
    "transformer_layer = backbone.transformer_layers[0]\n",
    "output_embeddings = transformer_layer(embeddings)\n",
    "\n",
    "# NEXT WORD PREDICTION (after just 1 layer)\n",
    "\n",
    "# Unembed: project back to vocabulary\n",
    "embedding_matrix = backbone.token_embedding.embeddings\n",
    "logits = tf.matmul(output_embeddings, embedding_matrix, transpose_b=True)\n",
    "\n",
    "# Get top 3 predictions for the LAST position (next word)\n",
    "last_logits = logits[0, -1, :]\n",
    "top3_ids = tf.argsort(last_logits, direction='DESCENDING')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f192907",
   "metadata": {},
   "source": [
    "#### 3.12.5 What Transformers Do and what do (context integration)\n",
    "**The transformer's job when updating embeddings:**\n",
    "\n",
    "Each transformer layer refines token embeddings by **integrating context from surrounding tokens** via attention:\n",
    "- Before: Each token's embedding is isolated (just word + position info)\n",
    "- After: Each token's embedding incorporates information from relevant context\n",
    "- Example: \"mouse\" embedding gets updated based on \"cat chased\" â†’ now encodes \"something being chased\"\n",
    "\n",
    "**Why this matters for prediction:**\n",
    "\n",
    "Updated embeddings â†’ better next-token predictions. GPT-2 stacks 12 layers because:\n",
    "- **1 layer**: Minimal context integration â†’ weak predictions\n",
    "- **12 layers**: Deep reasoning across all context â†’ strong predictions\n",
    "\n",
    "**Logit scores:** Higher number = more likely next token (passed through softmax to get probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7a163",
   "metadata": {},
   "source": [
    "## 4 Feed Forward Networks (MLP block)\n",
    "ML notes pages 62-68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82210f00",
   "metadata": {},
   "source": [
    "### 4.1 What the FFN is really doing\n",
    "\n",
    "In a transformer layer, attention answers:\n",
    "\n",
    "â€œWhich tokens should talk to each other?â€\n",
    "\n",
    "The FFN answers:\n",
    "\n",
    "â€œWhat computation should I apply to each token independently? its basically where your questions are answerd and facts are storedâ€\n",
    "\n",
    "Key property:\n",
    "- The FFN is position-wise.\n",
    "- Same MLP is applied to every token.\n",
    "- No interaction across sequence here, only across channels.\n",
    "\n",
    "Mathematically, for each token vector x:\n",
    "```math\n",
    "FFN(x) = W2 Â· Ïƒ(W1 Â· x + b1) + b2\n",
    "```\n",
    "the W2 and B2 are for projecting the linear step W1 Â· x + b1 back down to a lower dim, the lenaer step is made up of a wight matrix and bias just like a NN and just liek a NN it also has a activation function Ïƒ to add non linearity\n",
    "\n",
    "This is where:\n",
    "- Nonlinearity enters the model.\n",
    "- Feature composition happens.\n",
    "- Model capacity explodes.\n",
    "\n",
    "Most of the parameters in modern LLMs live here, not in attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5bd0d",
   "metadata": {},
   "source": [
    "### 4.2 Two-layer MLP structure\n",
    "\n",
    "Standard configuration:\n",
    "- Input dimension: d_model\n",
    "- Hidden dimension: d_ff (usually 4Ã— d_model)\n",
    "- Output dimension: d_model\n",
    "\n",
    "Example for GPT-like models:\n",
    "- d_model = 768\n",
    "- d_ff = 3072\n",
    "\n",
    "Why expand then contract:\n",
    "- Expansion creates a high-dimensional feature space.\n",
    "- Nonlinearity selects and combines features.\n",
    "- Projection back keeps residual dimensions stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a9c4f",
   "metadata": {},
   "source": [
    "### 4.3 Activation functions and why GELU matters\n",
    "#### 4.3.1 ReLU\n",
    "`ReLU(x) = max(0, x)`\n",
    "\n",
    "Pros:\n",
    "- Simple\n",
    "- Fast\n",
    "\n",
    "Cons:\n",
    "- Hard cutoff at zero\n",
    "- Less expressive for language\n",
    "\n",
    "#### 4.3.2 GELU (Gaussian Error Linear Unit)\n",
    "```\n",
    "GELU(x) = x Â· Î¦(x)\n",
    "```\n",
    "Where Î¦ is the standard normal CDF.\n",
    "\n",
    "Intuition:\n",
    "- Smooth gating\n",
    "- Small negative values partially pass\n",
    "- Better gradient flow\n",
    "\n",
    "Why LLMs prefer GELU:\n",
    "- Language is continuous, not sparse\n",
    "- Smooth activations model nuance better\n",
    "- Empirically improves perplexity\n",
    "\n",
    "GPT-2, BERT, and many successors use GELU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476b3fc",
   "metadata": {},
   "source": [
    "### 4.4 Modern variants: SwiGLU\n",
    "This is where newer LLMs (PaLM, LLaMA, Mistral) improve capacity.\n",
    "\n",
    "**Core idea**\n",
    "Instead of one activation, split the hidden layer and gate it:\n",
    "```\n",
    "FFN(x) = (W1 x âŠ™ Ïƒ(W2 x)) W3\n",
    "```\n",
    "\n",
    "For SwiGLU:\n",
    "- Ïƒ is Swish: x Â· sigmoid(x)\n",
    "- âŠ™ means element-wise multiplication\n",
    "- The gate controls information flow\n",
    "\n",
    "Why this works:\n",
    "- Explicit multiplicative interaction\n",
    "- Better feature selection\n",
    "- More expressive than plain GELU\n",
    "\n",
    "Important note:\n",
    "- Hidden dimension is often reduced (e.g. 2/3 of 4Ã—) to keep parameter count similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ee1df",
   "metadata": {},
   "source": [
    "### 4.5 Dropout and residual scaling\n",
    "\n",
    "Dropout:\n",
    "- Applied after activation or projection\n",
    "- Regularizes training\n",
    "- Less critical in massive pretraining, more in fine-tuning\n",
    "\n",
    "Residual scaling:\n",
    "- Output of FFN is added to residual stream\n",
    "- Large activations can destabilize training\n",
    "- Some models scale FFN output by a small constant early in training\n",
    "\n",
    "Core pattern:\n",
    "`x = x + dropout(FFN(LN(x)))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40016a8a",
   "metadata": {},
   "source": [
    "### 4.6 TensorFlow: Configurable FFN block (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\" \n",
    "    Feed-Forward Network with configurable activation.\n",
    "    Supports ReLU, GELU, and SwiGLU activations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        activation=\"gelu\",\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        if activation == \"swiglu\":\n",
    "            self.w1 = tf.keras.layers.Dense(d_ff)\n",
    "            self.w2 = tf.keras.layers.Dense(d_ff)\n",
    "            self.w3 = tf.keras.layers.Dense(d_model)\n",
    "        else:\n",
    "            self.dense1 = tf.keras.layers.Dense(d_ff)\n",
    "            self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if self.activation == \"relu\":\n",
    "            h = tf.nn.relu(self.dense1(x))\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.dense2(h)\n",
    "\n",
    "        if self.activation == \"gelu\":\n",
    "            h = tf.nn.gelu(self.dense1(x))\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.dense2(h)\n",
    "\n",
    "        if self.activation == \"swiglu\":\n",
    "            h = self.w1(x)\n",
    "            gate = tf.nn.swish(self.w2(x))\n",
    "            h = h * gate\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.w3(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da5292",
   "metadata": {},
   "source": [
    "using it inside the transformer layer (EX)\n",
    "```py\n",
    "ffn = FeedForward(\n",
    "    d_model=768,\n",
    "    d_ff=3072,\n",
    "    activation=\"gelu\"\n",
    ")\n",
    "\n",
    "y = ffn(x, training=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf12016",
   "metadata": {},
   "source": [
    "## 5. Normalization and Stabilization in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3c6f4",
   "metadata": {},
   "source": [
    "### 5.1 Why normalization is critical in deep transformers\n",
    "\n",
    "Transformers stack dozens to hundreds of layers. Each layer:\n",
    "- Applies attention\n",
    "- Applies an FFN\n",
    "- Adds residual connections (You add the layerâ€™s input back to its output, this way Each layer learns a small correction, not a full transformation.)\n",
    "\n",
    "```\n",
    "DEFINITION:\n",
    "Residual stream = the running hidden state that flows through the model via residual connections.\n",
    "Short definition: It is the vector being repeatedly updated as: x â† x + sublayer_output at every layer.\n",
    "```\n",
    "\n",
    "Without normalization:\n",
    "- Activations grow with depth\n",
    "- Variance drifts layer by layer\n",
    "- Gradients either explode or vanish\n",
    "- Training becomes extremely sensitive to learning rate and initialization\n",
    "\n",
    "Normalization exists to control the statistics of the residual stream so depth becomes feasible.\n",
    "\n",
    "**The residual stream must remain well-conditioned across layers.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613b0aa",
   "metadata": {},
   "source": [
    "### 5.2 LayerNorm inside transformers\n",
    "\n",
    "**What LayerNorm actually does**\n",
    "\n",
    "LayerNorm normalizes across features, not across batch.\n",
    "\n",
    "For a token vector `x âˆˆ R^d_model`:\n",
    "\n",
    "```\n",
    "LN(x) = (x âˆ’ mean(x)) / sqrt(var(x) + Îµ) * Î³ + Î²\n",
    "```\n",
    "\n",
    "Important properties:\n",
    "- Applied per token\n",
    "- Independent of batch size\n",
    "- Ideal for sequence models\n",
    "\n",
    "This is why transformers do not use BatchNorm.\n",
    "\n",
    "**Where LayerNorm is applied**\n",
    "\n",
    "In transformers, LayerNorm is applied inside each block, not globally.\n",
    "\n",
    "**NOTE:** Two main placements exist. This choice matters a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cca33",
   "metadata": {},
   "source": [
    "### 5.3 Post-Norm vs Pre-Norm\n",
    "\n",
    "#### 5.3.1 Post-Norm (original Transformer)\n",
    "Structure:\n",
    "```\n",
    "x â†’ Attention â†’ Add â†’ LayerNorm\n",
    "x â†’ FFN â†’ Add â†’ LayerNorm\n",
    "```\n",
    "Add = residual connection, It means: take the input x and add it element-wise to the sublayer output, Sublayer output = the result of the operation inside the block before the residual add.\n",
    "\n",
    "Problems:\n",
    "- Gradients must pass through many nonlinear layers before normalization\n",
    "- Deep models become unstable\n",
    "- Training requires careful warmup and small learning rates\n",
    "\n",
    "This works for shallow models but breaks at scale.\n",
    "\n",
    "#### 5.3.2 Pre-Norm (modern standard)\n",
    "Structure:\n",
    "```\n",
    "x â†’ LayerNorm â†’ Attention â†’ Add\n",
    "x â†’ LayerNorm â†’ FFN â†’ Add\n",
    "```\n",
    "\n",
    "Why this works better:\n",
    "- Residual path is clean and unnormalized\n",
    "- Gradients flow directly through identity connections\n",
    "- Much more stable for deep stacks\n",
    "\n",
    "Key insight:\n",
    "- Pre-norm turns the transformer into a well-behaved residual network.\n",
    "- Almost all modern LLMs use Pre-Norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35db75",
   "metadata": {},
   "source": [
    "### 5.4 RMSNorm\n",
    "\n",
    "**What RMSNorm is**\n",
    "\n",
    "RMSNorm removes mean-centering and only normalizes by root mean square.\n",
    "\n",
    "`RMSNorm(x) = x / rms(x) * Î³`\n",
    "\n",
    "Where:\n",
    "`rms(x) = sqrt(mean(xÂ²))`\n",
    "\n",
    "Differences from LayerNorm:\n",
    "- No subtraction of mean\n",
    "- Fewer operations\n",
    "- Slightly faster\n",
    "- Numerically simpler\n",
    "\n",
    "**Why modern LLMs prefer RMSNorm**\n",
    "\n",
    "Empirical findings:\n",
    "- Mean-centering is not strictly necessary\n",
    "- RMS scaling alone stabilizes training\n",
    "- Works well with large batch sizes and long contexts\n",
    "\n",
    "Used in:\n",
    "- LLaMA\n",
    "- Mistral\n",
    "- PaLM\n",
    "\n",
    "\n",
    "RMSNorm is not about expressiveness. It is about efficiency and stability at scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ca00f",
   "metadata": {},
   "source": [
    "### 5.5 Why normalization stabilizes attention and FFNs\n",
    "\n",
    "Two failure modes normalization prevents:\n",
    "\n",
    "1. Attention collapse\n",
    "- QKáµ€ grows too large\n",
    "- Softmax saturates\n",
    "- One token dominates\n",
    "2. FFN blow-up\n",
    "- Expansion layer produces large activations\n",
    "- Residual accumulation amplifies them\n",
    "\n",
    "Normalization ensures:\n",
    "- Attention logits stay in a reasonable range\n",
    "- FFN outputs do not dominate the residual stream\n",
    "- Each layer operates in a similar statistical regime\n",
    "\n",
    "This makes hyperparameters transferable across depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d6770",
   "metadata": {},
   "source": [
    "### 5.6 Initialization strategies for stability\n",
    "\n",
    "Initialization is tightly coupled with normalization.\n",
    "\n",
    "Key principles:\n",
    "- Linear layers should preserve variance\n",
    "- Residual paths should start near identity\n",
    "- Early training should be conservative\n",
    "\n",
    "Common strategies:\n",
    "- Xavier or Kaiming for dense layers\n",
    "- Small initialization for output projections\n",
    "- Zero or near-zero initialization for some residual branches\n",
    "\n",
    "In GPT-2:\n",
    "- Output projection of attention and FFN initialized with smaller std\n",
    "- This slows early residual accumulation\n",
    "\n",
    "The goal:\n",
    "- Let the model learn depth gradually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c476f79",
   "metadata": {},
   "source": [
    "### 5.7 Residual connection depth scaling\n",
    "\n",
    "As depth increases, residual additions accumulate.\n",
    "\n",
    "If each layer adds a signal of similar magnitude:\n",
    "`||x_L|| â‰ˆ L Â· ||Î”||`\n",
    "\n",
    "This is bad.\n",
    "\n",
    "Solutions:\n",
    "- Scale residual outputs by a constant\n",
    "- Implicit scaling via normalization\n",
    "- Careful initialization\n",
    "\n",
    "GPT-2 style scaling:\n",
    "- Attention and FFN outputs are scaled implicitly via initialization\n",
    "- Residual stream remains stable even at 48+ layers\n",
    "\n",
    "Some modern models explicitly scale residuals by **1/âˆšN.**\n",
    "\n",
    "$$\n",
    "x_{l+1} = x_l + \\frac{1}{\\sqrt{N}} \\, \\text{Sublayer}(x_l)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509a874",
   "metadata": {},
   "source": [
    "### 5.8 TensorFlow: Pre-Norm transformer components (high level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f229d5",
   "metadata": {},
   "source": [
    "LayerNorm block\n",
    "``` py\n",
    "norm = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "y = norm(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = self.add_weight(\n",
    "            shape=(d_model,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True))\n",
    "        return x / (rms + self.epsilon) * self.scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1f02",
   "metadata": {},
   "source": [
    "Pre-Norm pattern inside a block\n",
    "```py\n",
    "x = x + attention(norm1(x))\n",
    "x = x + ffn(norm2(x))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
