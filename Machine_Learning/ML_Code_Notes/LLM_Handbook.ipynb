{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd61ec",
   "metadata": {},
   "source": [
    "# Complete LLM Hankbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3429ce4",
   "metadata": {},
   "source": [
    "Install Dependencies to run code blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras keras_nlp matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f5fca",
   "metadata": {},
   "source": [
    "## Sections \n",
    "\n",
    "### 1 Tokenization, vocab, sequence formatting\n",
    "- 1.1 Byte level words vs Subword tokenization\n",
    "  - 1.1.1 Byte level tokenization\n",
    "  - 1.1.2 Subword tokenization (BPE, WordPiece, SentencePiece)\n",
    "- 1.2 Token Ids and Vocabulary Size\n",
    "- 1.3 Padding and Masking\n",
    "- 1.4 Special Tokens\n",
    "- 1.5 Sequence Packing and Contiguous Streams\n",
    "- 1.6 Sliding Window Chunking\n",
    "- 1.7 Complete Example: Combining All Tokenization Steps\n",
    "\n",
    "### 2 Embedding and Unembedding\n",
    "- 2.1 Word embedding lookup tables\n",
    "- 2.2 Unembedding and tied embeddings\n",
    "  - 2.2.1 Unembedding\n",
    "  - 2.2.2 Tied Embeddings\n",
    "- 2.3 Why positional representations are required\n",
    "- 2.4 Positional encoding types\n",
    "  - 2.4.1 Sinusoidal positional encoding\n",
    "  - 2.4.2 Learned positional embeddings\n",
    "  - 2.4.3 Rotary Position Embeddings (RoPE)\n",
    "  - 2.4.4 ALiBi\n",
    "- 2.5 How positional encoding interacts with attention\n",
    "- 2.6 Embedding scaling by sqrt(d_model)\n",
    "- 2.7 Complete Example: Combining All Embedding Steps\n",
    "\n",
    "### 3 Attention\n",
    "- 3.1 Query, Key, Value fundamentals\n",
    "- 3.2 Dot product attention\n",
    "- 3.3 Why divide by âˆšdk\n",
    "- 3.4 Causal masking and autoregressive behavior\n",
    "- 3.5 Softmax details and numerical stability\n",
    "- 3.6 Multi-head attention\n",
    "- 3.7 Attention complexity\n",
    "- 3.8 Memory layout and tensor shapes\n",
    "- 3.9 Flash Attention (conceptual)\n",
    "- 3.10 Self attention vs cross attention\n",
    "- 3.11 Key-value caching for generation\n",
    "- 3.12 TensorFlow Examples\n",
    "  - 3.12.1 Single-head attention\n",
    "  - 3.12.2 Multi-head attention\n",
    "  - 3.12.3 Causal mask\n",
    "  - 3.12.4 Multi-head attention demo\n",
    "  - 3.12.5 What transformers do: context integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5011d5",
   "metadata": {},
   "source": [
    "## 1 Tokenization, vocab, sequence formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36007de8",
   "metadata": {},
   "source": [
    "### 1.1 Byte level words vs Subword tokenization \n",
    "\n",
    "**Why**: Transformers cannot process raw text, text must be converted into numbers. The way we break text into tokens affects efficency, generalization and memory usage\n",
    "\n",
    "#### 1.1.1 Byte level tokenization\n",
    "\n",
    "- works at the byte level (0-255)\n",
    "- Real world usage: GPT-2 uses byte pair encoding (BPE) at byte level\n",
    "- Pros:\n",
    "  - Handels any charecter, any language, emojis, symbol\n",
    "  - no OOV (out of vocab) tokens\n",
    "- Cons:\n",
    "  - Toekn sequences can be longer -> means more compute \n",
    "- Example: \"hello ðŸ‘‹\" â€“> [104, 101, 108, 108, 111, 32, 240, 159, 145, 139] (in token ids where range is 0 -> vocab_size)\n",
    "\n",
    "#### 1.1.2 Subword tokenization (BPE, WordPeice, SentencePiece)\n",
    "\n",
    "-  Breaks Text into frequent subwords insted of characters or words. \n",
    "-  Example: \n",
    "   -  \"unhappiness\" -> [\"un\", \"happi\", \"ness\"] -> [217, 9812, 403] # in token ids (range is 0 -> vocab size)\n",
    "- Pros: \n",
    "  - Shorter sequences than byte\n",
    "  - Can handle rare words via subword decomposition (breaking unknown words into known smaller parts)\n",
    "- Cons:\n",
    "  - some complexity in building vocab and handling edge cases\n",
    "  \n",
    "**NOTE:** LLM's often use subword BPE (BPE applied at the subword level) it iteratively merges the most frequent character or subword pairs to build a vocabulary, balancing between character-level and word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce1ea",
   "metadata": {},
   "source": [
    "### 1.2 Token Ids and Vocabulary Size\n",
    "- After Tokenization, each token is mapped to a integer ID using a vocabulary\n",
    "- Vocabulary size (V) is very important\n",
    "  - Larger V -> model must have a bigger embedding matrix (page 50 in written notes) -> more parameters (hence a larger model)\n",
    "  - Smaller V -> more subword splitting (words broken into more pieces) -> longer sequences -> slower training (but smaller model size)\n",
    "- Typical LLM vocab sizes: 30K-100K for english models \n",
    "- Example: In TensorFlow, keras_nlp.tokenizers handles both mapping tokens â†’ IDs and IDs â†’ tokens.\n",
    "\n",
    "``` py\n",
    "from keras_nlp.tokenizers import BytePairTokenizer\n",
    "\n",
    "tokenizer = BytePairTokenizer(vocabulary=[\"hello\", \"world\", \"un\", \"happi\", \"ness\", \"<PAD>\", \"<BOS>\", \"<EOS>\"])\n",
    "tokens = tokenizer.tokenize([\"hello world\", \"unhappiness\"])\n",
    "token_ids = tokenizer(tokens)\n",
    "print(token_ids)\n",
    "\n",
    "```\n",
    "\n",
    "**How Keras NLP Tokenizers Handle Token â†” ID Mapping** Under the hood, Keras NLP tokenizers maintain two key data structures (`token_to_id` and `id_to_token`) for bidirectional mapping. When you call `tokenizer.tokenize(text)`, it returns tokens as strings; `tokenizer(text)` returns token IDs; and `tokenizer.detokenize(ids)` converts IDs back to text. The vocabulary is built during training or loaded from a pre-trained model, with special tokens (PAD, UNK, BOS, EOS) typically assigned fixed IDs at the beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5d852",
   "metadata": {},
   "source": [
    "### 1.3 Padding and Masking \n",
    "1. Padding: Short sequences are extended with PAD tokens to match the longest sequence in a batch, enabling efficient parallel processing (e.g., `[5, 10, 15]` â†’ `[5, 10, 15, <PAD>, <PAD>]`)\n",
    "\n",
    "2. Attention Masking: Tells the transformer which positions to ignore during attention.\n",
    "- **No Mask (Bidirectional)**: All tokens attend to all tokens; used in BERT for full context understanding\n",
    "- **Causal Mask (Autoregressive)**: Each token only attends to previous tokens; used in GPT to prevent future information leakage during training\n",
    "- **Padding Mask**: Masks PAD tokens so they don't affect attention scores; combined with other masks in most models\n",
    "\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example: batch of token IDs (here each array of token ids in a batch is a sqeuence i.e one example, by spliting in batches we can proccess in parallel)\n",
    "batch = tf.ragged.constant([\n",
    "    [1, 2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "padded = batch.to_tensor(default_value=0) # Output: [[1, 2, 3], [4, 5, 0]]  <- 0 is the PAD token ID (these are the new tokens)\n",
    "mask = tf.cast(padded != 0, tf.int32) # Output: [[1, 1, 1], [1, 1, 0]]  <- tells attention to ignore the last position in sequence 2 (this is the attention scores not token values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47c9fa",
   "metadata": {},
   "source": [
    "### 1.4 Special Tokens (see ML notes page 121)\n",
    "- `<BOS>`: Beginning of sequence (marks where a sequence starts)\n",
    "- `<EOS>`: End of sequence (marks where a sequence ends)\n",
    "- `<PAD>`: Padding (fills shorter sequences to match batch length)\n",
    "- `<UNK>`: Unknown/ out of vocab token (represents words not in vocabulary)\n",
    "- etc\n",
    "\n",
    "**usage in training**\n",
    "``` text\n",
    "Input:  <BOS> hello world <EOS> <PAD> <PAD>    # BOS is fed as a conditioning token ((a special input token that provides initial context/prompt for the model; the model conditions its next-token predictions on it but is not trained to predict it)) EOS is included so the model learns to predict sequence end PADs fill to uniform length\n",
    "\n",
    "Target: hello world <EOS> <PAD> <PAD> <PAD>   # Target = input shifted left (model predicts the next token at each step, including EOS); PADs fill to uniform length\n",
    "\n",
    "Mask:   1 1 1 1 0 0 0                          # Mask=1 for positions to compute loss (we compute loss for real tokens and EOS), 0 for PADs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde25d45",
   "metadata": {},
   "source": [
    "### 1.5 Sequence Packing and Contiguous Streams \n",
    "- Why: LLM training is compute-heavy, to use memory efficiently, multiple short examples can be concatenated into a single long sequence and then chunked\n",
    "- Benefits: \n",
    "  - Reduces wasted padding\n",
    "  - keepinh sequences dense for attention\n",
    "  \n",
    "**Example (pseudo)**\n",
    "```text\n",
    "Examples: [\"hello\", \"world\"], [\"goodbye\", \"moon\"]\n",
    "Packed sequence: \"hello world goodbye moon\"\n",
    "```\n",
    "- Then split into fixed length chunks (ex: 8 tokens per chunk) for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2eaec",
   "metadata": {},
   "source": [
    "### 1.6 Sliding Window Chunking \n",
    "- When text is too long to fit in memory, we create overlapping windown to preserve context\n",
    "- why: prevents cutting off dependencies between sequences.\n",
    "- Example Sequence length = 6, chunk size = 4, stride = 2\n",
    "``` text\n",
    "Sequence: [A B C D E F] (len = 6)\n",
    "\n",
    "Chunk 1: Start at position 0 â†’ [A B C D] (len = 4 beacuse chunk size = 4)\n",
    "Chunk 2: Start at position 0 (position) + 2(stride) = 2 â†’ [C D E F] (move the window by 2, keeps last 2 tokens of last chink in new chunk)\n",
    "\n",
    "Chunks:  [A B C D], [C D E F]\n",
    "\n",
    "Result Sequence: [A B C D], [C D E F]\n",
    "                      â†‘overlapâ†‘\n",
    "```\n",
    "- Edge case: If the final window doesn't have enough tokens (e.g., only 3 tokens left for chunk_size=4), you either pad it with `<PAD>` tokens or discard it depending on your training strategy.\n",
    "- overlapping ensures context continuity for training \n",
    "- common in LLM pretraining \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff521f58",
   "metadata": {},
   "source": [
    "### 1.7 Complete Example: Combining All Tokenization Steps\n",
    "\n",
    "This example demonstrates the entire pipeline from raw text to training-ready sequences, incorporating all concepts from 1.1-1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2957877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello worldGoodbye\n",
      " worldGoodbye moon\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer (handles vocab, special tokens, BPE subword)\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Create dataset pipeline: tokenization â†’ chunking â†’ padding\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices([\"Hello world\", \"Goodbye moon\"])\n",
    "    .map(tokenizer)  # Tokenize: text â†’ IDs\n",
    "    .unbatch()  # Flatten to token stream (packing)\n",
    "    .batch(8, drop_remainder=False)  # Chunk into sequences of 8 tokens\n",
    "    .map(lambda x: (x[:-1], x[1:]))  # Create (input, target) pairs\n",
    "    .padded_batch(2, padded_shapes=([None], [None]))  # Pad and batch\n",
    ")\n",
    "\n",
    "# Usage:\n",
    "inputs, targets = next(iter(dataset))\n",
    "print(tokenizer.detokenize(inputs[0]))\n",
    "print(tokenizer.detokenize(targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14482b70",
   "metadata": {},
   "source": [
    "**OUTPUT:**\n",
    "```\n",
    "Hello worldGoodbye\n",
    "worldGoodbye moon\n",
    "```\n",
    "\n",
    "**EXPLANATION:**\n",
    "\n",
    "The dataset variable is a `tf.data.Dataset` pipeline that transforms raw text into training-ready (input, target) pairs. It's a lazy iterator (doesn't process until called).\n",
    "\n",
    "**Pipeline steps:**\n",
    "1. **from_tensor_slices**: Creates dataset from list of strings\n",
    "2. **map(tokenizer)**: Converts each text â†’ token IDs (e.g., \"Hello\" â†’ [15496, 995])\n",
    "3. **unbatch()**: Flattens all sequences into one continuous token stream (sequence packing)\n",
    "4. **batch(8)**: Groups tokens into chunks of 8 (creates fixed-length sequences)\n",
    "5. **map(lambda)**: Splits each chunk into (input, target) where target = input shifted left\n",
    "6. **padded_batch(2)**: Groups 2 sequences into a batch, pads shorter ones to match length\n",
    "\n",
    "**How next-token prediction works:**\n",
    "\n",
    "The model predicts the NEXT token at EACH position, not just the last one:\n",
    "- Position 0: Given \"Hello\" â†’ predict \"world\"\n",
    "- Position 1: Given \"Hello world\" â†’ predict \"Goodbye\"  \n",
    "- Position 2: Given \"Hello worldGoodbye\" â†’ predict \"moon\"\n",
    "\n",
    "So the target sequence shows what should be predicted at each step. The entire target = input shifted left by 1 token (each target is the next token).\n",
    "\n",
    "**Chunking Strategy Comparison:**\n",
    "\n",
    "Token stream: `[A, B, C, D, E, F, G, H, I, J]`\n",
    "\n",
    "**Fixed batch** (current): `.batch(4)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[E, F, G, H]` (tokens 4-7)\n",
    "- Chunk 3: `[I, J]` (tokens 8-9)\n",
    "- â†’ No overlap, each token appears once\n",
    "\n",
    "**Sliding window**: `.window(size=4, shift=2, drop_remainder=True)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[C, D, E, F]` (tokens 2-5, overlaps last 2 from chunk 1)\n",
    "- Chunk 3: `[E, F, G, H]` (tokens 4-7, overlaps last 2 from chunk 2)\n",
    "- â†’ Overlap preserves context across chunks, useful for long documents\n",
    "\n",
    "**IMPORTANT: Both strategies train on next-token prediction at EVERY position!**\n",
    "\n",
    "**Fixed batch:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[E,F,G,H]` â†’ trains: (Eâ†’F), (E,Fâ†’G), (E,F,Gâ†’H)\n",
    "- Each token appears ONCE\n",
    "\n",
    "**Sliding window:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[C,D,E,F]` â†’ trains: (Câ†’D), (C,Dâ†’E), (C,D,Eâ†’F)\n",
    "- Tokens C and D appear TWICE (extra training for better context)\n",
    "\n",
    "**Key Takeaway:** The chunking method only affects which tokens are grouped together, not how training works. Sliding window gives overlapping tokens extra exposure for better long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d86c5a",
   "metadata": {},
   "source": [
    "## 2 Embedding and Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7187be",
   "metadata": {},
   "source": [
    "### 2.1 Word embedding lookup tables\n",
    "\n",
    "**What a Embedding is**\n",
    "\n",
    "- An embedding is just a trainable lookup table:\n",
    "  - Shape: (vocab_size, d_model)\n",
    "  - Input: token ID's of shape (batch, seq_len)\n",
    "  - Output: vectors of shape (batch, seq_len, d_model)\n",
    "\n",
    "Each token ID indexes one row, the learning happens because gradients update those rows based on prediction error\n",
    "\n",
    "**Traning Proccess**: How Embeddings Are Learned\n",
    "\n",
    "1. **Random Initialization**: Embedding table starts with random values (shape: `vocab_size Ã— d_model`)\n",
    "\n",
    "2. **Forward Pass**: \n",
    "    - Token IDs â†’ Look up embeddings â†’ Feed through transformer â†’ Predict next token\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare prediction to actual next token\n",
    "    - Compute cross-entropy loss\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back through the entire model\n",
    "    - Embedding table rows get gradient updates based on which tokens were used\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    embedding[token_id] -= learning_rate Ã— gradient[token_id]\n",
    "    ```\n",
    "\n",
    "**Key Insight**: Tokens that appear in similar contexts will develop similar embeddings because they receive similar gradient updates. For example:\n",
    "- \"cat\" and \"dog\" â†’ often surrounded by words like \"pet\", \"animal\" â†’ embeddings become similar\n",
    "- \"king\" and \"queen\" â†’ share contexts like \"royal\", \"throne\" â†’ learn related representations\n",
    "\n",
    "The model learns embeddings **jointly** with all other parameters (attention weights, feedforward layers) to minimize prediction error across the entire training corpus.\n",
    "\n",
    "**Example Matrix:**\n",
    "``` text\n",
    "vocab_size = 50000  # Total unique tokens in vocabulary\n",
    "d_model = 512       # Each token â†’ 512-dimensional vector\n",
    "embedding_table = tf.Variable(shape=(50000, 512))\n",
    "# If token ID = 42, its embedding is embedding_table[42] (a 512-length vector)\n",
    "```\n",
    "\n",
    "In short the embedding is the models repersentation of a token, attention uses these repersentations to operate\n",
    "\n",
    "**TensorFlow Example**\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenEmbedding(tf.keras.layers.Layer): # inherit tf Layer Class\n",
    "    def __init__(self, vocab_size, d_model): # init model\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding( # create embedding layer\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=d_model\n",
    "        )\n",
    "\n",
    "    # override call function with out custom embeddign layer\n",
    "    def call(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "```\n",
    "\n",
    "Example IO:\n",
    "```text\n",
    "input:  [12,   431,   98] # token ID's\n",
    "\n",
    "# each token gets a embedding returned (output[i] = E[token_ids[i]]) where 'E' is teh embedding matrix\n",
    "output: [\n",
    "  E[12],     # âˆˆ R^d_model here 'R' is (real numbers) and 'd_model' is the number of dimentions in the model its a hyperparameter you chose (like 64, 128, 512, etc) for ex R^4 = [x1, x2, x3, x4] x can be any real number\n",
    "  E[431],    # âˆˆ R^d_model\n",
    "  E[98]      # âˆˆ R^d_model\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032ee2e",
   "metadata": {},
   "source": [
    "### 2.2 Unembedding and tied embeddings\n",
    "\n",
    "#### 2.2.1 Unembedding\n",
    "\n",
    "Unembedding converts each transformer output vector into a score for every token in the vocabulary so the model can predict which token comes next. we need this at the output of the transformer\n",
    "```\n",
    "Vocabulary: [\"hello\", \"world\", \"cat\", \"dog\", ...] (ex 50000 words)\n",
    "                  â†“       â†“      â†“     â†“\n",
    "Logits:          [2.1,   0.5,  -1.2,  3.4, ...]  â† Higher score = more likely next token (50000 scores one for each token)\n",
    "```\n",
    "this is done with linear projection.\n",
    "\n",
    "- Input: (batch, seq_len, d_model)\n",
    "- Weight: (d_model, vocab_size)\n",
    "- Output: (batch, seq_len, vocab_size)\n",
    "\n",
    "**NOTE:** this layer is also called the unembedding or LM head\n",
    "\n",
    "**NOTE:** The converstion of the token ID back to a final word is the tokenizers job do not mistake unebedding for that\n",
    "\n",
    "\n",
    "**Training Process**: How Unembedding Weights Are Learned\n",
    "\n",
    "1. **Random Initialization**: Unembedding matrix starts with random values (shape: `d_model Ã— vocab_size`)\n",
    "\n",
    "2. **Forward Pass**:\n",
    "    - Transformer outputs â†’ (batch, seq_len, d_model)\n",
    "    - Matrix multiply with unembedding weights â†’ (batch, seq_len, vocab_size)\n",
    "    - Apply softmax â†’ probability distribution over vocabulary\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare predicted probabilities to actual next token (one-hot encoded)\n",
    "    - Compute cross-entropy loss: `loss = -log(P(correct_token))`\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back from loss through softmax and unembedding layer\n",
    "    - Each row of the unembedding matrix (corresponding to one output dimension) gets updated based on prediction errors\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    unembedding_weights -= learning_rate Ã— gradient\n",
    "    ```\n",
    "\n",
    "**Key Insight**: The unembedding layer learns which transformer output patterns correspond to which tokens. If the model frequently outputs vectors in a certain direction when \"cat\" should be next, those weights get strengthened to produce higher logits for \"cat\".\n",
    "\n",
    "**Example:**\n",
    "``` text\n",
    "d_model = 512\n",
    "vocab_size = 50000\n",
    "unembedding_matrix = tf.Variable(shape=(512, 50000))\n",
    "\n",
    "# Transformer output: (batch=1, seq_len=1, d_model=512)\n",
    "# After matmul: (batch=1, seq_len=1, vocab_size=50000)\n",
    "# Each position gets 50000 scores (one per possible next token)\n",
    "\n",
    "# Mathamatically\n",
    "\n",
    "# Hidden vector h âˆˆ R^d_model # from tansfromer \n",
    "# Unembedding matrix W âˆˆ R^(d_model Ã— vocab_size) # learned matrix\n",
    "# Logits l = h Â· W   â†’ l âˆˆ R^vocab_size # 50000 logits as a result\n",
    "```\n",
    "\n",
    "**The flow so far is as follows**: Token ID â†’ Embedding â†’ Transformer â†’ Hidden Vector â†’ Unembedding â†’ Logits â†’ Token ID â†’ Word (where we pick the highest logit as next word)\n",
    "\n",
    "**Tensorflow Example**\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenUnembedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        # Linear layer without bias: projects hidden vectors to vocab logits\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # hidden_states: (batch, seq_len, d_model)\n",
    "        # logits: (batch, seq_len, vocab_size)\n",
    "        logits = self.dense(hidden_states)\n",
    "        return logits\n",
    "\n",
    "```\n",
    "\n",
    "#### 2.2.2 Tied Embeddings\n",
    "\n",
    "Modern LLMs tie the input embedding matrix and output projection weights \n",
    "\n",
    "- Why it works:\n",
    "  - the same geometric space is used for reading and writing tokens \n",
    "  - reduces parameters \n",
    "  - improves sample efficiency and stability\n",
    "- Mathamatically\n",
    "  - input embedding: E[token_id]\n",
    "  - output logits: h Â· Eáµ€ (h is the hidden vector)\n",
    "  \n",
    "This enforces symmetry between encoding and decoding\n",
    "\n",
    "**Tensorflow example**\n",
    "``` py\n",
    "class TiedOutputProjection(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        embedding_matrix = self.embedding_layer.embedding.embeddings\n",
    "        logits = tf.einsum(\"btd,vd->btv\", hidden_states, embedding_matrix)\n",
    "        return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a76fb",
   "metadata": {},
   "source": [
    "### 2.3 Why positional representations are required (ML notes pg 112)\n",
    "\n",
    "**Self attention is permutation-invariant.**\n",
    "\n",
    "That means:\n",
    "- â€œcat sat matâ€ and â€œmat sat catâ€ look identical without position.\n",
    "- Order must be injected explicitly.\n",
    "\n",
    "Positions noting sequence index are added or applied to embeddings before attention.\n",
    "\n",
    "**Key idea:**\n",
    "Token meaning + position meaning = input representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b4c2d",
   "metadata": {},
   "source": [
    "### 2.4 Positional encoding types\n",
    "\n",
    "#### 2.4.1 Sinusodal positional encoding (ML Notes pg 112)\n",
    "\n",
    "**Concept**\n",
    "- Deterministic, non-trainable\n",
    "- Uses sine and cosine at different frequencies\n",
    "- Allows extrapolation to longer sequences\n",
    "\n",
    "**Formulas:**\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "**Why it works:**\n",
    "- Relative positions can be inferred from linear combinations\n",
    "- Attention can compute distance relationships\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "import math\n",
    "\n",
    "def sinusoidal_position_encoding(seq_len, d_model):\n",
    "    position = tf.range(seq_len, dtype=tf.float32)[:, None]\n",
    "    div_term = tf.exp(\n",
    "        tf.range(0, d_model, 2, dtype=tf.float32) *\n",
    "        -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    pe = tf.zeros((seq_len, d_model))\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(0, d_model, 2)[:, None],\n",
    "        updates=tf.sin(position * div_term)\n",
    "    )\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(1, d_model, 2)[:, None],\n",
    "        updates=tf.cos(position * div_term)\n",
    "    )\n",
    "    return pe\n",
    "```\n",
    "\n",
    "**added directly to token embeddings**\n",
    "\n",
    "#### 2.4.2 Learned positional embeddings (ML notes pg 112)\n",
    "**Concept**\n",
    "- Position IDs get their own embedding table\n",
    "- Fully learned\n",
    "- Used in GPT-2, BERT\n",
    "\n",
    "**Pros:**\n",
    "- Flexible\n",
    "- Often better on fixed context lengths\n",
    "\n",
    "**Cons:**\n",
    "- Cannot extrapolate beyond max length trained\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "class LearnedPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
    "\n",
    "    def call(self, seq_len):\n",
    "        positions = tf.range(seq_len)\n",
    "        return self.embedding(positions)\n",
    "```\n",
    "\n",
    "#### 2.4.3 Rotary Position Embeddings (RoPE)\n",
    "This is where modern LLMs diverge from early transformers.\n",
    "\n",
    "**Core idea:** RoPE rotates query and key vectors in embedding space based on position.\n",
    "\n",
    "**Key properties:**\n",
    "- Position information is applied inside attention\n",
    "- Enables relative position reasoning\n",
    "- Scales well to long contexts\n",
    "\n",
    "Instead of adding position vectors, we rotate pairs of dimensions:\n",
    "```text\n",
    "embedding vector = [x1, x2] â†’ rotation by angle Î¸( determined by tokens position)\n",
    "x1' = x1*cosÎ¸ - x2*sinÎ¸\n",
    "x2' = x1*sinÎ¸ + x2*cosÎ¸\n",
    "x_rotated = [x1', x2']\n",
    "```\n",
    "\n",
    "**Why this is powerful**\n",
    "- Dot products encode relative distance naturally when dot product of K,Q are rotated\n",
    "- No learned position embeddings\n",
    "- Better extrapolation\n",
    "\n",
    "**TensorFlow implementation**\n",
    "```py\n",
    "def rotary_embedding(x, seq_len):\n",
    "    d_model = x.shape[-1]\n",
    "    half = d_model // 2\n",
    "\n",
    "    freqs = tf.exp(\n",
    "        -tf.range(0, half, dtype=tf.float32) / half * tf.math.log(10000.0)\n",
    "    )\n",
    "    positions = tf.range(seq_len, dtype=tf.float32)\n",
    "    angles = positions[:, None] * freqs[None, :]\n",
    "\n",
    "    sin = tf.sin(angles)\n",
    "    cos = tf.cos(angles)\n",
    "\n",
    "    x1 = x[..., :half]\n",
    "    x2 = x[..., half:]\n",
    "\n",
    "    rotated = tf.concat(\n",
    "        [x1 * cos - x2 * sin,\n",
    "         x1 * sin + x2 * cos],\n",
    "        axis=-1\n",
    "    )\n",
    "    return rotated\n",
    "```\n",
    "**Applied to queries and keys only, never values.**\n",
    "\n",
    "#### 2.4.4 ALiBi\n",
    "**Concept**\n",
    "- No position embeddings at all\n",
    "- Adds a linear bias to attention scores\n",
    "- Penalizes distant tokens\n",
    "\n",
    "**Attention score becomes:**\n",
    "``` text\n",
    "QKáµ€ / sqrt(d) + bias(distance)\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Extremely simple\n",
    "- Strong extrapolation to long sequences\n",
    "- Used in MPT and others\n",
    "\n",
    "**NOTE:** ALiBi changes attention, not embeddings.\n",
    "\n",
    "**You do not add position vectors at input.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8b1e2",
   "metadata": {},
   "source": [
    "### 2.5 How positional encoding interacts with attention\n",
    "**Important clarity:**\n",
    "- Additive encodings modify token representations before attention.\n",
    "- RoPE modifies query and key geometry.\n",
    "- ALiBi modifies attention logits directly.\n",
    "\n",
    "**All three inject order, but at different stages.**\n",
    "\n",
    "**This choice affects:**\n",
    "- Long context scaling\n",
    "- Memory behavior\n",
    "- Generalization beyond training length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786ff05",
   "metadata": {},
   "source": [
    "### 2.6 Embedding scaling by sqrt(d_model)\n",
    "\n",
    "When embeddings are initialized, their variance is small.\n",
    "\n",
    "If we add positional encodings directly, they can dominate early training.\n",
    "\n",
    "**Standard fix:**\n",
    "```text\n",
    "x = embedding(token_ids) * sqrt(d_model)\n",
    "x = x + position_encoding\n",
    "```\n",
    "\n",
    "**This ensures:**\n",
    "- Token identity dominates initially\n",
    "- Position is a refinement, not the signal\n",
    "\n",
    "**TF snippet**\n",
    "```py\n",
    "x = token_embedding(token_ids)\n",
    "x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "x += position_encoding\n",
    "```\n",
    "\n",
    "**This is not cosmetic. It stabilizes training.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbf855",
   "metadata": {},
   "source": [
    "### 2.7 Complete Example: Combining All Embedding Steps\n",
    "\n",
    "This example demonstrates the entire embedding pipeline from token IDs to final logits, a code example for 2.1-2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03605a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The cat sat on the mat. I love my cat very much.\n",
      "\n",
      "1. Token IDs: [ 464 3797 3332  319  262 2603   13  314 1842  616 3797  845  881   13]\n",
      "\n",
      "2. Token Embeddings shape: (14, 768)\n",
      "   Each token â†’ 768-dimensional vector\n",
      "\n",
      "3. Position Embeddings shape: (14, 768)\n",
      "\n",
      "4. Final Embeddings (token + position): (14, 768)\n",
      "\n",
      "\n",
      "5. Unembedding (tied weights):\n",
      "   Logits shape: (14, 50257)\n",
      "   â†’ 14 positions Ã— 50257 vocab scores\n",
      "   (Using pretrained GPT-2 embeddings - trained on web text)\n",
      "\n",
      "Top predicted tokens (first 5 positions):\n",
      "  Position 0: 'theless'\n",
      "  Position 1: ' cat'\n",
      "  Position 2: ' sat'\n",
      "  Position 3: ' on'\n",
      "  Position 4: ' the'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Example text: Notice \"cat\" appears at different positions\n",
    "text = \"The cat sat on the mat. I love my cat very much.\"\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "# Step 1: Tokenize (from section 1)\n",
    "token_ids = tokenizer(text)\n",
    "print(f\"1. Token IDs: {token_ids.numpy()}\\n\")\n",
    "\n",
    "# Step 2: Token Embedding (2.1) - Convert IDs to vectors\n",
    "token_embedding = backbone.token_embedding\n",
    "token_embeds = token_embedding(token_ids)\n",
    "print(f\"2. Token Embeddings shape: {token_embeds.shape}\")\n",
    "print(f\"   Each token â†’ 768-dimensional vector\\n\")\n",
    "\n",
    "# Step 3: Position Embedding (2.4) - Add position information\n",
    "position_embedding = backbone.position_embedding\n",
    "pos_embeds = position_embedding(token_embeds)\n",
    "print(f\"3. Position Embeddings shape: {pos_embeds.shape}\\n\")\n",
    "\n",
    "# Step 4: Combine (2.5) - Token meaning + Position\n",
    "final_embeds = token_embeds + pos_embeds\n",
    "print(f\"4. Final Embeddings (token + position): {final_embeds.shape}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# KEY DEMONSTRATION: Same word, different position\n",
    "# ============================================================\n",
    "cat_token = tokenizer(\"cat\")[0]\n",
    "cat_positions = [i for i, t in enumerate(token_ids.numpy()) if t == cat_token.numpy()]\n",
    "\n",
    "if len(cat_positions) >= 2:\n",
    "    pos1, pos2 = cat_positions[0], cat_positions[1]\n",
    "    \n",
    "    print(f\"Word 'cat' appears at positions: {cat_positions}\")\n",
    "    print(f\"\\nPosition {pos1}:\")\n",
    "    print(f\"  Token embed (first 3): {token_embeds[pos1, :3].numpy()}\")\n",
    "    print(f\"  Pos embed (first 3):   {pos_embeds[pos1, :3].numpy()}\")\n",
    "    print(f\"  Final embed (first 3): {final_embeds[pos1, :3].numpy()}\")\n",
    "    \n",
    "    print(f\"\\nPosition {pos2}:\")\n",
    "    print(f\"  Token embed (first 3): {token_embeds[pos2, :3].numpy()}\")\n",
    "    print(f\"  Pos embed (first 3):   {pos_embeds[pos2, :3].numpy()}\")\n",
    "    print(f\"  Final embed (first 3): {final_embeds[pos2, :3].numpy()}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Token embeddings: IDENTICAL (same word)\")\n",
    "    print(\"âœ“ Position embeddings: DIFFERENT (different locations)\")\n",
    "    print(\"âœ“ Final embeddings: DIFFERENT (context-aware)\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Unembedding (2.2) - Project back to vocabulary\n",
    "# ============================================================\n",
    "print(f\"\\n5. Unembedding (tied weights):\")\n",
    "# NOTE: We're using GPT-2's PRETRAINED embedding weights\n",
    "# These were learned on billions of tokens, so predictions are meaningful\n",
    "# In practice, you'd train these from scratch on your data\n",
    "embedding_matrix = token_embedding.embeddings  # Reuse same pretrained weights\n",
    "logits = tf.matmul(final_embeds, embedding_matrix, transpose_b=True)\n",
    "print(f\"   Logits shape: {logits.shape}\")\n",
    "print(f\"   â†’ {logits.shape[0]} positions Ã— {logits.shape[1]} vocab scores\")\n",
    "print(f\"   (Using pretrained GPT-2 embeddings - trained on web text)\")\n",
    "\n",
    "# Show top predictions for first few positions\n",
    "print(f\"\\nTop predicted tokens (first 5 positions):\")\n",
    "for i in range(min(5, len(token_ids))):\n",
    "    predicted_id = tf.argmax(logits[i]).numpy()\n",
    "    predicted_word = tokenizer.detokenize([predicted_id])\n",
    "    print(f\"  Position {i}: '{predicted_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebce668",
   "metadata": {},
   "source": [
    "**SUMMARY: What we covered from sections 2.1-2.6**\n",
    "\n",
    "**1. TOKEN EMBEDDING (2.1):**\n",
    "- Input: Token IDs [464, 3797, ...]\n",
    "- Output: Dense vectors [(768 dims), (768 dims), ...]\n",
    "- â†’ Each token gets a learned representation\n",
    "\n",
    "**2. POSITION EMBEDDING (2.4):**\n",
    "- Learned embeddings that encode position\n",
    "- Position 0 â‰  Position 10 (different vectors)\n",
    "- â†’ Tells model WHERE each token is\n",
    "\n",
    "**3. COMBINING (2.5):**\n",
    "- final = token_embed + pos_embed\n",
    "- â†’ Same word at different positions has different final representations\n",
    "\n",
    "**4. UNEMBEDDING (2.2):**\n",
    "- Projects embeddings â†’ vocabulary logits\n",
    "- Uses TIED WEIGHTS (same matrix as input embeddings)\n",
    "- â†’ Predicts next token distribution\n",
    "\n",
    "**IMPORTANT:** This example uses GPT-2's PRETRAINED embeddings!\n",
    "- These weights were learned on billions of tokens of web text\n",
    "- That's why predictions are meaningful (not random)\n",
    "- In your own model, you'd train these from scratch on your data\n",
    "- Training process: same backpropagation as described in 2.1\n",
    "\n",
    "**Key Insight:** Without position embeddings, \"cat sat mat\" and \"mat sat cat\" would look identical to the model. Position info is CRITICAL.\n",
    "\n",
    "**Note:** We haven't covered transformers yet - that processes these embeddings! The transformer would sit between step 4 (final embeddings) and step 5 (unembedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a45ff",
   "metadata": {},
   "source": [
    "## 3 Attention (ML notes pg 54-61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9acf6e",
   "metadata": {},
   "source": [
    "#### 3.1 Query, Key, Value fundamentals\n",
    "Every token embedding is projected into three different spaces:\n",
    "- Query (Q): what this token is looking for\n",
    "- Key (K): what this token offers\n",
    "- Value (V): the information this token contributes if selected\n",
    "\n",
    "All three come from the same input embedding x, but with different learned linear projections. The input token starts as the same vector x and is transformed using a learned weight matrix\n",
    "\n",
    "**Mathematically:**\n",
    "- Q = x Wq\n",
    "- K = x Wk\n",
    "- V = x Wv\n",
    "  \n",
    "Why this works:\n",
    "- Attention becomes content-addressable memory.\n",
    "- Tokens donâ€™t attend by position or index, but by similarity in meaning.\n",
    "\n",
    "Conceptual analogy:\n",
    "- Query = question\n",
    "- Key = label on a memory slot\n",
    "- Value = content inside the slot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4c411",
   "metadata": {},
   "source": [
    "### 3.2. Dot product attention\n",
    "\n",
    "The core operation:\n",
    "```math\n",
    "attention(Q, K, V) = softmax(Q Káµ€ / âˆšdk) V\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "1. Compute similarity between every query and every key.\n",
    "2. Normalize scores with softmax.\n",
    "3. Use scores to form weighted sums of values.\n",
    "\n",
    "This produces:\n",
    "- For each token, a contextualized representation.\n",
    "- Tokens can pull information from any previous token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38a4ac",
   "metadata": {},
   "source": [
    "### 3.3 Why divide by âˆšdk\n",
    "\n",
    "Without scaling:\n",
    "- Dot products grow with dimension.\n",
    "- Softmax saturates (becomes near one-hot).\n",
    "- Gradients vanish.\n",
    "\n",
    "Scaling by **sqrt(dk)** keeps variance stable.\n",
    "\n",
    "This is not optional. Training becomes unstable without it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1552",
   "metadata": {},
   "source": [
    "### 3.4 Causal masking and autoregressive behavior\n",
    "\n",
    "LLMs generate left to right. A token must not see future tokens during training.\n",
    "\n",
    "Causal mask:\n",
    "- Upper triangular matrix filled with -inf\n",
    "- Added to attention logits before softmax\n",
    "\n",
    "Effect:\n",
    "- Positions j > i get zero probability\n",
    "- Guarantees autoregressive behavior\n",
    "\n",
    "Conceptually:\n",
    "- Training simulates generation\n",
    "- Prediction at position i only depends on < i\n",
    "\n",
    "**Similarity Scores Q Káµ€ (no mask)**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 0.60 | 0.30 | 0.10 |\n",
    "| love       | 0.20 | 0.50 | 0.30 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |\n",
    "\n",
    "**Casual mask**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 1.00 | 0.00 | 0.00 |\n",
    "| love       | 0.29 | 0.71 | 0.00 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15224027",
   "metadata": {},
   "source": [
    "### 3.5 Softmax details and numerical stability\n",
    "\n",
    "Softmax is:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "Numerical issue:\n",
    "- Large logits overflow\n",
    "\n",
    "Standard trick:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max_j x_j)}{\\sum_j \\exp(x_j - \\max_j x_j)}\n",
    "$$\n",
    "This preserves probabilities but stabilizes computation.\n",
    "\n",
    "TensorFlow handles this internally, but you must remember why it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf452b3",
   "metadata": {},
   "source": [
    "### 3.6 Multi-head attention\n",
    "Single head attention has one similarity space basicaly it can only attend to one type of question. see `ML notes pg 61` for more\n",
    "\n",
    "Multi-head attention:\n",
    "- Splits channels into h heads\n",
    "- Each head attends differently\n",
    "- Results are concatenated and projected\n",
    "\n",
    "Shape intuition:\n",
    "```\n",
    "(batch, seq, d_model)\n",
    "â†’ (batch, heads, seq, d_head)\n",
    "```\n",
    "Where:\n",
    "```\n",
    "d_head = d_model / heads\n",
    "```\n",
    "\n",
    "Why this matters:\n",
    "- Different heads learn syntax, semantics, coreference, long-range dependencies\n",
    "- Heads act as independent subspaces\n",
    "\n",
    "**Example**\n",
    "Tokens: T1, T2, T3\n",
    "\n",
    "Head 1 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.1 | 0.3 | 0.6 |\n",
    "| 0.2 | 0.5 | 0.3 |\n",
    "\n",
    "Head 2 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.4 | 0.4 | 0.2 |\n",
    "| 0.3 | 0.3 | 0.4 |\n",
    "\n",
    "Concatenated (along features):\n",
    "| T1         | T2         | T3         |\n",
    "|------------|------------|------------|\n",
    "| 0.1, 0.4   | 0.3, 0.4   | 0.6, 0.2   |\n",
    "| 0.2, 0.3   | 0.5, 0.3   | 0.3, 0.4   |\n",
    "\n",
    "**Multi-Head Attention Combination**\n",
    "\n",
    "Each head computes its own attention output independently:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)\n",
    "$$\n",
    "\n",
    "Outputs of all heads are concatenated along the feature dimension:\n",
    "$$\n",
    "\\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\in \\mathbb{R}^{d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "A final learned linear projection mixes them:\n",
    "$$\n",
    "\\text{MHA\\_output} = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\cdot W_O\n",
    "$$\n",
    "\n",
    "Note: attention scores are not combined across heads; each head attends separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64eab8",
   "metadata": {},
   "source": [
    "### 3.7 Attention complexity\n",
    "\n",
    "Time and memory:\n",
    "```O(seq_lenÂ²)```\n",
    "\n",
    "This is the main scaling bottleneck in LLMs.\n",
    "\n",
    "Implications:\n",
    "- Long context is expensive\n",
    "- Motivates FlashAttention, sparse attention, sliding windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa64d3",
   "metadata": {},
   "source": [
    "### 3.8 Memory layout and tensor shapes (critical)\n",
    "\n",
    "Most bugs in attention come from shape mistakes.\n",
    "\n",
    "Typical shapes:\n",
    "- Q, K, V: (batch, heads, seq_len, d_head) # d_head = d_model / heads\n",
    "- Attention scores: (batch, heads, seq_len, seq_len)\n",
    "- Output: (batch, seq_len, d_model)\n",
    "\n",
    "Reshaping and transposing correctly is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba8c74",
   "metadata": {},
   "source": [
    "### 3.9 Flash Attention (conceptual only)\n",
    "\n",
    "Flash Attention:\n",
    "- Computes attention without materializing full seq_len Ã— seq_len matrix\n",
    "- Uses tiling and fused kernels\n",
    "- Reduces memory bandwidth bottleneck\n",
    "\n",
    "Important takeaway:\n",
    "- Same math, different execution\n",
    "- You do not change the model, only the kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd3832",
   "metadata": {},
   "source": [
    "### 3.10 Self attention vs cross attention\n",
    "\n",
    "- Self attention: Q, K, V come from same sequence\n",
    "- Cross attention: Q from decoder, K/V from encoder\n",
    "\n",
    "GPT style LLMs use only self attention.\n",
    "\n",
    "Encoderâ€“decoder models use both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5147b0",
   "metadata": {},
   "source": [
    "### 3.11 Key-value caching for generation\n",
    "\n",
    "During autoregressive generation:\n",
    "- Past K and V do not change\n",
    "- Only compute Q for new token\n",
    "- Append new K, V to cache\n",
    "\n",
    "Effect:\n",
    "- Reduces per-token cost from O(nÂ²) to O(n)\n",
    "\n",
    "\n",
    "This is essential for fast inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfc97c",
   "metadata": {},
   "source": [
    "### 3.12 Tensorflow Example (no raw kernal high level usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1f50c",
   "metadata": {},
   "source": [
    "#### 3.12.1 Single-head attention (conceptual TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d4a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def single_head_attention(x, mask=None):\n",
    "    d_model = x.shape[-1]\n",
    "\n",
    "    q = tf.keras.layers.Dense(d_model)(x)\n",
    "    k = tf.keras.layers.Dense(d_model)(x)\n",
    "    v = tf.keras.layers.Dense(d_model)(x)\n",
    "\n",
    "    scores = tf.matmul(q, k, transpose_b=True)\n",
    "    scores /= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    output = tf.matmul(weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa167c",
   "metadata": {},
   "source": [
    "#### 3.12.2 Multi-head attention (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7494795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        return self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=mask,\n",
    "            training=training\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6333c89",
   "metadata": {},
   "source": [
    "#### 3.12.3 Causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066809e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    mask = tf.linalg.band_part(\n",
    "        tf.ones((seq_len, seq_len)), -1, 0\n",
    "    )\n",
    "    return 1.0 - mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e0269",
   "metadata": {},
   "source": [
    "#### 3.12.4 Multi-Head Attention in Action: Complete Demo\n",
    "\n",
    "This example demonstrates multi-head attention processing with:\n",
    "- **Embeddings before/after attention** - See how attention updates representations\n",
    "- **Attention scores** - Visualize which tokens attend to which\n",
    "- **Per-head analysis** - See what different heads learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da922e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The cat chased the mouse\n",
      "\n",
      "Tokens: ['The', ' cat', ' chased', ' the', ' mouse']\n",
      "\n",
      "======================================================================\n",
      "PRETRAINED GPT-2: Embeddings Before vs After Transformer\n",
      "======================================================================\n",
      "\n",
      "Embeddings BEFORE attention (first 3 dims of each token):\n",
      "The          [-0.087, -0.218, 0.068, ...]\n",
      " cat         [0.034, -0.017, 0.069, ...]\n",
      " chased      [0.015, -0.164, 0.209, ...]\n",
      " the         [-0.040, -0.069, 0.148, ...]\n",
      " mouse       [0.120, 0.047, 0.155, ...]\n",
      "\n",
      "Embeddings AFTER pretrained attention (first 3 dims):\n",
      "The          [0.156, -0.795, 0.394, ...]\n",
      " cat         [-0.269, -0.858, 0.098, ...]\n",
      " chased      [-1.100, -1.250, -1.771, ...]\n",
      " the         [-1.145, 0.412, -0.563, ...]\n",
      " mouse       [1.029, -1.552, -0.271, ...]\n",
      "\n",
      "======================================================================\n",
      "HOW MUCH EACH TOKEN CHANGED:\n",
      "======================================================================\n",
      "The          130.797 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      " cat         54.544 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      " chased      61.931 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      " the         56.247 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      " mouse       62.451 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "======================================================================\n",
      "NEXT WORD PREDICTION (using 1-layer output):\n",
      "======================================================================\n",
      "Top 3 predicted next tokens:\n",
      "  1. ' mouse' (score: 27.9)\n",
      "  2. ' and' (score: 21.5)\n",
      "  3. ',' (score: 21.3)\n",
      "\n",
      "âœ“ These predictions use only 1 transformer layer\n",
      "âœ“ GPT-2 has 12 layers - predictions improve with each layer!\n",
      "âœ“ Scores are logits (unnormalized) - higher = more likely\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Interesting sentence with clear relationships\n",
    "text = \"The cat chased the mouse\"\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "# Tokenize\n",
    "token_ids = tokenizer(text)\n",
    "tokens = [tokenizer.detokenize([tid]).numpy().decode() if isinstance(tokenizer.detokenize([tid]), tf.Tensor) \n",
    "          else tokenizer.detokenize([tid]) for tid in token_ids.numpy()]\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "if tf.rank(token_ids) == 1:\n",
    "    token_ids = token_ids[None, :]\n",
    "\n",
    "# Get embeddings\n",
    "x_tokens = backbone.token_embedding(token_ids)\n",
    "x_pos = backbone.position_embedding(x_tokens)\n",
    "embeddings = x_tokens + x_pos\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRETRAINED GPT-2: Embeddings Before vs After Transformer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show embeddings BEFORE transformer processing\n",
    "print(\"\\nEmbeddings BEFORE attention (first 3 dims of each token):\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:<12} [{embeddings[0, i, 0].numpy():.3f}, {embeddings[0, i, 1].numpy():.3f}, {embeddings[0, i, 2].numpy():.3f}, ...]\")\n",
    "\n",
    "# Pass through FIRST transformer layer (uses pretrained attention weights!)\n",
    "transformer_layer = backbone.transformer_layers[0]\n",
    "output_embeddings = transformer_layer(embeddings)\n",
    "\n",
    "print(\"\\nEmbeddings AFTER pretrained attention (first 3 dims):\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:<12} [{output_embeddings[0, i, 0].numpy():.3f}, {output_embeddings[0, i, 1].numpy():.3f}, {output_embeddings[0, i, 2].numpy():.3f}, ...]\")\n",
    "\n",
    "# Show the magnitude of change\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HOW MUCH EACH TOKEN CHANGED:\")\n",
    "print(\"=\"*70)\n",
    "for i, token in enumerate(tokens):\n",
    "    change = tf.norm(output_embeddings[0, i] - embeddings[0, i]).numpy()\n",
    "    bar = \"â–ˆ\" * int(change * 2)\n",
    "    print(f\"{token:<12} {change:.3f} {bar}\")\n",
    "\n",
    "# ============================================================\n",
    "# NEXT WORD PREDICTION (after just 1 layer)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT WORD PREDICTION (using 1-layer output):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Unembed: project back to vocabulary\n",
    "embedding_matrix = backbone.token_embedding.embeddings\n",
    "logits = tf.matmul(output_embeddings, embedding_matrix, transpose_b=True)\n",
    "\n",
    "# Get top 3 predictions for the LAST position (next word)\n",
    "last_logits = logits[0, -1, :]\n",
    "top3_ids = tf.argsort(last_logits, direction='DESCENDING')[:3]\n",
    "\n",
    "print(\"Top 3 predicted next tokens:\")\n",
    "for i, token_id in enumerate(top3_ids.numpy(), 1):\n",
    "    token = tokenizer.detokenize([token_id])\n",
    "    if isinstance(token, bytes):\n",
    "        token = token.decode('utf-8')\n",
    "    score = last_logits[token_id].numpy()\n",
    "    print(f\"  {i}. '{token}' (score: {score:.1f})\")\n",
    "\n",
    "print(\"\\nâœ“ These predictions use only 1 transformer layer\")\n",
    "print(\"âœ“ GPT-2 has 12 layers - predictions improve with each layer!\")\n",
    "print(\"âœ“ Scores are logits (unnormalized) - higher = more likely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f192907",
   "metadata": {},
   "source": [
    "#### 3.12.5 What Transformers Do and what do (context integration)\n",
    "**The transformer's job when updating embeddings:**\n",
    "\n",
    "Each transformer layer refines token embeddings by **integrating context from surrounding tokens** via attention:\n",
    "- Before: Each token's embedding is isolated (just word + position info)\n",
    "- After: Each token's embedding incorporates information from relevant context\n",
    "- Example: \"mouse\" embedding gets updated based on \"cat chased\" â†’ now encodes \"something being chased\"\n",
    "\n",
    "**Why this matters for prediction:**\n",
    "\n",
    "Updated embeddings â†’ better next-token predictions. GPT-2 stacks 12 layers because:\n",
    "- **1 layer**: Minimal context integration â†’ weak predictions\n",
    "- **12 layers**: Deep reasoning across all context â†’ strong predictions\n",
    "\n",
    "**Logit scores:** Higher number = more likely next token (passed through softmax to get probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
