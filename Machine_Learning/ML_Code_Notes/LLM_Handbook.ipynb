{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd61ec",
   "metadata": {},
   "source": [
    "# Complete LLM Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3429ce4",
   "metadata": {},
   "source": [
    "Install Dependencies to run code blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras keras_nlp matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f5fca",
   "metadata": {},
   "source": [
    "## Sections \n",
    "\n",
    "### [1 Tokenization, vocab, sequence formatting](#1-Tokenization,-vocab,-sequence-formatting)\n",
    "- [1.1 Byte level words vs Subword tokenization](#1.1-Byte-level-words-vs-Subword-tokenization)\n",
    "  - [1.1.1 Byte level tokenization](#1.1.1-Byte-level-tokenization)\n",
    "  - [1.1.2 Subword tokenization (BPE, WordPiece, SentencePiece)](#1.1.2-Subword-tokenization-(BPE,-WordPiece,-SentencePiece))\n",
    "- [1.2 Token Ids and Vocabulary Size](#1.2-Token-Ids-and-Vocabulary-Size)\n",
    "- [1.3 Padding and Masking](#1.3-Padding-and-Masking)\n",
    "- [1.4 Special Tokens](#1.4-Special-Tokens)\n",
    "- [1.5 Sequence Packing and Contiguous Streams](#1.5-Sequence-Packing-and-Contiguous-Streams)\n",
    "- [1.6 Sliding Window Chunking](#1.6-Sliding-Window-Chunking)\n",
    "- [1.7 Complete Example: Combining All Tokenization Steps](#1.7-Complete-Example:-Combining-All-Tokenization-Steps)\n",
    "\n",
    "### [2 Embedding and Unembedding](#2-Embedding-and-Unembedding)\n",
    "- [2.1 Word embedding lookup tables](#2.1-Word-embedding-lookup-tables)\n",
    "- [2.2 Unembedding and tied embeddings](#2.2-Unembedding-and-tied-embeddings)\n",
    "  - [2.2.1 Unembedding](#2.2.1-Unembedding)\n",
    "  - [2.2.2 Tied Embeddings](#2.2.2-Tied-Embeddings)\n",
    "- [2.3 Why positional representations are required](#2.3-Why-positional-representations-are-required)\n",
    "- [2.4 Positional encoding types](#2.4-Positional-encoding-types)\n",
    "  - [2.4.1 Sinusoidal positional encoding](#2.4.1-Sinusodal-positional-encoding)\n",
    "  - [2.4.2 Learned positional embeddings](#2.4.2-Learned-positional-embeddings)\n",
    "  - [2.4.3 Rotary Position Embeddings (RoPE)](#2.4.3-Rotary-Position-Embeddings-(RoPE))\n",
    "  - [2.4.4 ALiBi](#2.4.4-ALiBi)\n",
    "- [2.5 How positional encoding interacts with attention](#2.5-How-positional-encoding-interacts-with-attention)\n",
    "- [2.6 Embedding scaling by sqrt(d_model)](#2.6-Embedding-scaling-by-sqrt(d_model))\n",
    "- [2.7 Complete Example: Combining All Embedding Steps](#2.7-Complete-Example:-Combining-All-Embedding-Steps)\n",
    "\n",
    "### [3 Attention](#3-Attention)\n",
    "- [3.1 Query, Key, Value fundamentals](#3.1-Query,-Key,-Value-fundamentals)\n",
    "- [3.2 Dot product attention](#3.2-Dot-product-attention)\n",
    "- [3.3 Why divide by âˆšdk](#3.3-Why-divide-by-âˆšdk)\n",
    "- [3.4 Causal masking and autoregressive behavior](#3.4-Causal-masking-and-autoregressive-behavior)\n",
    "- [3.5 Softmax details and numerical stability](#3.5-Softmax-details-and-numerical-stability)\n",
    "- [3.6 Multi-head attention](#3.6-Multi-head-attention)\n",
    "- [3.7 Attention complexity](#3.7-Attention-complexity)\n",
    "- [3.8 Memory layout and tensor shapes](#3.8-Memory-layout-and-tensor-shapes)\n",
    "- [3.9 Flash Attention (conceptual)](#3.9-Flash-Attention-(conceptual))\n",
    "- [3.10 Self attention vs cross attention](#3.10-Self-attention-vs-cross-attention)\n",
    "- [3.11 Key-value caching for generation](#3.11-Key-value-caching-for-generation)\n",
    "- [3.12 TensorFlow Examples](#3.12-Tensorflow-Example-(no-raw-kernal-high-level-usage))\n",
    "  - [3.12.1 Single-head attention](#3.12.1-Single-head-attention-(conceptual-TF))\n",
    "  - [3.12.2 Multi-head attention](#3.12.2-Multi-head-attention-(high-level))\n",
    "  - [3.12.3 Causal mask](#3.12.3-Causal-mask)\n",
    "  - [3.12.4 Multi-head attention demo](#3.12.4-Multi-Head-Attention-in-Action:-Example)\n",
    "  - [3.12.5 What transformers do: context integration](#3.12.5-What-Transformers-Do-and-what-do-(context-integration))\n",
    "\n",
    "### [4 Feed Forward Networks (MLP block)](#4-Feed-Forward-Networks-(MLP-block))\n",
    "- [4.1 What the FFN is really doing](#4.1-What-the-FFN-is-really-doing)\n",
    "- [4.2 Two-layer MLP structure](#4.2-Two-layer-MLP-structure)\n",
    "- [4.3 Activation functions and why GELU matters](#4.3-Activation-functions-and-why-GELU-matters)\n",
    "  - [4.3.1 ReLU](#4.3.1-ReLU)\n",
    "  - [4.3.2 GELU (Gaussian Error Linear Unit)](#4.3.2-GELU-(Gaussian-Error-Linear-Unit))\n",
    "- [4.4 Modern variants: SwiGLU](#4.4-Modern-variants:-SwiGLU)\n",
    "- [4.5 Dropout and residual scaling](#4.5-Dropout-and-residual-scaling)\n",
    "- [4.6 TensorFlow: Configurable FFN block (high level)](#4.6-TensorFlow:-Configurable-FFN-block-(high-level))\n",
    "\n",
    "### [5 Normalization and Stabilization in Transformers](#5.-Normalization-and-Stabilization-in-Transformers)\n",
    "- [5.1 Why normalization is critical in deep transformers](#5.1-Why-normalization-is-critical-in-deep-transformers)\n",
    "- [5.2 LayerNorm inside transformers](#5.2-LayerNorm-inside-transformers)\n",
    "- [5.3 Post-Norm vs Pre-Norm](#5.3-Post-Norm-vs-Pre-Norm)\n",
    "  - [5.3.1 Post-Norm (original Transformer)](#5.3.1-Post-Norm-(original-Transformer))\n",
    "  - [5.3.2 Pre-Norm (modern standard)](#5.3.2-Pre-Norm-(modern-standard))\n",
    "- [5.4 RMSNorm](#5.4-RMSNorm)\n",
    "- [5.5 Why normalization stabilizes attention and FFNs](#5.5-Why-normalization-stabilizes-attention-and-FFNs)\n",
    "- [5.6 Initialization strategies for stability](#5.6-Initialization-strategies-for-stability)\n",
    "- [5.7 Residual connection depth scaling](#5.7-Residual-connection-depth-scaling)\n",
    "- [5.8 TensorFlow: Pre-Norm transformer components (high level)](#5.8-TensorFlow:-Pre-Norm-transformer-components-(high-level))\n",
    "\n",
    "### [6 The Transformer Decoder Block](#6-The-Transformer-Decoder-Block)\n",
    "- [6.1 Decoder block order and why it is this way](#6.1-Decoder-block-order-and-why-it-is-this-way)\n",
    "- [6.2 Shape propagation through the block](#6.2-Shape-propagation-through-the-block)\n",
    "- [6.3 Dropout behavior inside the decoder](#6.3-Dropout-behavior-inside-the-decoder)\n",
    "- [6.4 Trainable parameter count intuition](#6.4-Trainable-parameter-count-intuition)\n",
    "- [6.5 How to tune hidden sizes](#6.5-How-to-tune-hidden-sizes)\n",
    "- [6.6 Why decoder-only models work for LLMs](#6.6-Why-decoder-only-models-work-for-LLMs)\n",
    "- [6.7 TensorFlow: Full DecoderBlock implementation](#6.7-TensorFlow:-Full-DecoderBlock-implementation)\n",
    "\n",
    "### [7 Putting blocks together into a Transformer Decoder](#7-Putting-blocks-together-into-a-Transformer-Decoder)\n",
    "- [7.1 Stacking N decoder blocks](#7.1-Stacking-N-decoder-blocks)\n",
    "- [7.2 Why weights are not shared across layers](#7.2-Why-weights-are-not-shared-across-layers)\n",
    "- [7.3 How input embeddings flow into the decoder](#7.3-How-input-embeddings-flow-into-the-decoder)\n",
    "- [7.4 How logits are produced](#7.4-How-logits-are-produced)\n",
    "- [7.5 TensorFlow: Tiny GPT style model from scratch](#7.5-TensorFlow:-Tiny-GPT-style-model-from-scratch)\n",
    "\n",
    "### [8 Training LLMs](#8-Training-LLMs)\n",
    "- [8.1 Next-token prediction objective](#8.1-Next-token-prediction-objective)\n",
    "- [8.2 Teacher forcing](#8.2-Teacher-forcing)\n",
    "- [8.3 Loss calculation and padding masks](#8.3-Loss-calculation-and-padding-masks)\n",
    "- [8.4 Gradient accumulation](#8.4-Gradient-accumulation)\n",
    "- [8.5 Gradient clipping](#8.5-Gradient-clipping)\n",
    "- [8.6 Learning rate schedules](#8.6-Learning-rate-schedules)\n",
    "- [8.7 Mixed precision training](#8.7-Mixed-precision-training)\n",
    "  - [8.7.1 Loss Scaling](#8.7.1-Loss-Scaling)\n",
    "- [8.8 TensorFlow: Custom autoregressive training loop](#8.8-TensorFlow:-Custom-autoregressive-training-loop)\n",
    "  - [8.8.1 Enable mixed precision](#8.8.1-Enable-mixed-precision)\n",
    "  - [8.8.2 Loss function with padding mask](#8.8.2-Loss-function-with-padding-mask)\n",
    "  - [8.8.3 Custom training step](#8.8.3-Custom-training-step)\n",
    "  - [8.8.4 Optimizer setup](#8.8.4-Optimizer-setup)\n",
    "  - [8.8.5 Load built-in dataset and create a small model](#8.8.5-Load-built-in-dataset-and-create-a-small-model)\n",
    "  - [8.8.6 Training loop](#8.8.6-Training-loop)\n",
    "\n",
    "### [9 Generation Controls for LLMs](#9-Generation-Controls-for-LLMs)\n",
    "- [9.1 Greedy decoding](#9.1-Greedy-decoding)\n",
    "- [9.2 Sampling](#9.2-Sampling)\n",
    "- [9.3 Temperature](#9.3-Temperature-(ml-notes-p-53))\n",
    "- [9.4 Top-k sampling](#9.4-Top-k-sampling)\n",
    "- [9.5 Top-p (nucleus) sampling](#9.5-Top-p-(nucleus)-sampling)\n",
    "- [9.6 Repetition penalty](#9.6-Repetition-penalty)\n",
    "- [9.7 Length penalty](#9.7-Length-penalty)\n",
    "- [9.8 Stopping conditions](#9.8-Stopping-conditions)\n",
    "- [9.9 Key-value cache for speed](#9.9-Key-value-cache-for-speed)\n",
    "- [9.10 Generation in tf.function](#9.10-Generation-in-tf.function)\n",
    "- [9.11 TensorFlow: Full autoregressive generation function](#9.11-TensorFlow:-Full-autoregressive-generation-function)\n",
    "\n",
    "### [10 Scaling and regularization](#10-Scaling-and-regularization)\n",
    "- [10.1 Parameter scaling laws](#10.1-Parameter-scaling-laws)\n",
    "- [10.2 Dropout placement](#10.2-Dropout-placement)\n",
    "- [10.3 Weight decay](#10.3-Weight-decay)\n",
    "- [10.4 Learning rate schedules](#10.4-Learning-rate-schedules)\n",
    "- [10.5 Warmup linear schedules](#10.5-Warmup-linear-schedules)\n",
    "- [10.6 Cosine decay](#10.6-Cosine-decay)\n",
    "- [10.7 Adam vs AdamW](#10.7-Adam-vs-AdamW)\n",
    "- [10.8 Tokenization impact on scale](#10.8-Tokenization-impact-on-scale)\n",
    "- [10.9 Transformer depth and expressivity](#10.9-Transformer-depth-and-expressivity)\n",
    "- [10.10 Putting it together: real vs toy training](#10.10-Putting-it-together:-real-vs-toy-training)\n",
    "\n",
    "### [11 Evaluation and Metrics for LLMs](#11-Evaluation-and-Metrics-for-LLMs)\n",
    "- [11.1 Perplexity](#11.1-Perplexity)\n",
    "- [11.2 Bits per character (BPC)](#11.2-Bits-per-character-(BPC))\n",
    "- [11.3 Loss curves](#11.3-Loss-curves)\n",
    "- [11.4 Out-of-distribution evaluation](#11.4-Out-of-distribution-evaluation)\n",
    "- [11.5 Token-level accuracy](#11.5-Token-level-accuracy)\n",
    "- [11.6 Sampling-based evaluation](#11.6-Sampling-based-evaluation)\n",
    "- [11.7 Practical evaluation workflow](#11.7-Practical-evaluation-workflow)\n",
    "- [11.8 When metrics lie](#11.8-When-metrics-lie)\n",
    "- [11.9 End-to-end evaluation of a pretrained GPT-2 (TensorFlow)](#11.9-End-to-end-evaluation-of-a-pretrained-GPT-2-(TensorFlow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5011d5",
   "metadata": {},
   "source": [
    "## 1 Tokenization, vocab, sequence formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36007de8",
   "metadata": {},
   "source": [
    "### 1.1 Byte level words vs Subword tokenization \n",
    "\n",
    "**Why**: Transformers cannot process raw text, text must be converted into numbers. The way we break text into tokens affects efficency, generalization and memory usage\n",
    "\n",
    "#### 1.1.1 Byte level tokenization\n",
    "\n",
    "- works at the byte level (0-255)\n",
    "- Real world usage: GPT-2 uses byte pair encoding (BPE) at byte level\n",
    "- Pros:\n",
    "  - Handels any charecter, any language, emojis, symbol\n",
    "  - no OOV (out of vocab) tokens\n",
    "- Cons:\n",
    "  - Toekn sequences can be longer -> means more compute \n",
    "- Example: \"hello ðŸ‘‹\" â€“> [104, 101, 108, 108, 111, 32, 240, 159, 145, 139] (in token ids where range is 0 -> vocab_size)\n",
    "\n",
    "#### 1.1.2 Subword tokenization (BPE, WordPeice, SentencePiece)\n",
    "\n",
    "-  Breaks Text into frequent subwords insted of characters or words. \n",
    "-  Example: \n",
    "   -  \"unhappiness\" -> [\"un\", \"happi\", \"ness\"] -> [217, 9812, 403] # in token ids (range is 0 -> vocab size)\n",
    "- Pros: \n",
    "  - Shorter sequences than byte\n",
    "  - Can handle rare words via subword decomposition (breaking unknown words into known smaller parts)\n",
    "- Cons:\n",
    "  - some complexity in building vocab and handling edge cases\n",
    "  \n",
    "**NOTE:** LLM's often use subword BPE (BPE applied at the subword level) it iteratively merges the most frequent character or subword pairs to build a vocabulary, balancing between character-level and word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce1ea",
   "metadata": {},
   "source": [
    "### 1.2 Token Ids and Vocabulary Size\n",
    "- After Tokenization, each token is mapped to a integer ID using a vocabulary\n",
    "- Vocabulary size (V) is very important\n",
    "  - Larger V -> model must have a bigger embedding matrix (page 50 in written notes) -> more parameters (hence a larger model)\n",
    "  - Smaller V -> more subword splitting (words broken into more pieces) -> longer sequences -> slower training (but smaller model size)\n",
    "- Typical LLM vocab sizes: 30K-100K for english models \n",
    "- Example: In TensorFlow, keras_nlp.tokenizers handles both mapping tokens â†’ IDs and IDs â†’ tokens.\n",
    "\n",
    "```py\n",
    "from keras_nlp.tokenizers import BytePairTokenizer\n",
    "\n",
    "tokenizer = BytePairTokenizer(vocabulary=[\"hello\", \"world\", \"un\", \"happi\", \"ness\", \"<PAD>\", \"<BOS>\", \"<EOS>\"])\n",
    "tokens = tokenizer.tokenize([\"hello world\", \"unhappiness\"])\n",
    "token_ids = tokenizer(tokens)\n",
    "print(token_ids)\n",
    "```\n",
    "\n",
    "**How Keras NLP Tokenizers Handle Token â†” ID Mapping** Under the hood, Keras NLP tokenizers maintain two key data structures (`token_to_id` and `id_to_token`) for bidirectional mapping. When you call `tokenizer.tokenize(text)`, it returns tokens as strings; `tokenizer(text)` returns token IDs; and `tokenizer.detokenize(ids)` converts IDs back to text. The vocabulary is built during training or loaded from a pre-trained model, with special tokens (PAD, UNK, BOS, EOS) typically assigned fixed IDs at the beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5d852",
   "metadata": {},
   "source": [
    "### 1.3 Padding and Masking \n",
    "1. Padding: Short sequences are extended with PAD tokens to match the longest sequence in a batch, enabling efficient parallel processing (e.g., `[5, 10, 15]` â†’ `[5, 10, 15, <PAD>, <PAD>]`)\n",
    "\n",
    "2. Attention Masking: Tells the transformer which positions to ignore during attention.\n",
    "- **No Mask (Bidirectional)**: All tokens attend to all tokens; used in BERT for full context understanding\n",
    "- **Causal Mask (Autoregressive)**: Each token only attends to previous tokens; used in GPT to prevent future information leakage during training\n",
    "- **Padding Mask**: Masks PAD tokens so they don't affect attention scores; combined with other masks in most models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example: batch of token IDs (here each array of token ids in a batch is a sqeuence i.e one example, by spliting in batches we can proccess in parallel)\n",
    "batch = tf.ragged.constant([\n",
    "    [1, 2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "padded = batch.to_tensor(default_value=0) # Output: [[1, 2, 3], [4, 5, 0]]  <- 0 is the PAD token ID (these are the new tokens)\n",
    "mask = tf.cast(padded != 0, tf.int32) # Output: [[1, 1, 1], [1, 1, 0]]  <- tells attention to ignore the last position in sequence 2 (this is the attention scores not token values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47c9fa",
   "metadata": {},
   "source": [
    "### 1.4 Special Tokens (see ML notes page 121)\n",
    "- `<BOS>`: Beginning of sequence (marks where a sequence starts)\n",
    "- `<EOS>`: End of sequence (marks where a sequence ends)\n",
    "- `<PAD>`: Padding (fills shorter sequences to match batch length)\n",
    "- `<UNK>`: Unknown/ out of vocab token (represents words not in vocabulary)\n",
    "- etc\n",
    "\n",
    "**usage in training**\n",
    "``` text\n",
    "Input:  <BOS> hello world <EOS> <PAD> <PAD>    # BOS is fed as a conditioning token ((a special input token that provides initial context/prompt for the model; the model conditions its next-token predictions on it but is not trained to predict it)) EOS is included so the model learns to predict sequence end PADs fill to uniform length\n",
    "\n",
    "Target: hello world <EOS> <PAD> <PAD> <PAD>   # Target = input shifted left (model predicts the next token at each step, including EOS); PADs fill to uniform length\n",
    "\n",
    "Mask:   1 1 1 1 0 0 0                          # Mask=1 for positions to compute loss (we compute loss for real tokens and EOS), 0 for PADs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde25d45",
   "metadata": {},
   "source": [
    "### 1.5 Sequence Packing and Contiguous Streams \n",
    "- Why: LLM training is compute-heavy, to use memory efficiently, multiple short examples can be concatenated into a single long sequence and then chunked\n",
    "- Benefits: \n",
    "  - Reduces wasted padding\n",
    "  - keepinh sequences dense for attention\n",
    "  \n",
    "**Example (pseudo)**\n",
    "```text\n",
    "Examples: [\"hello\", \"world\"], [\"goodbye\", \"moon\"]\n",
    "Packed sequence: \"hello world goodbye moon\"\n",
    "```\n",
    "- Then split into fixed length chunks (ex: 8 tokens per chunk) for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2eaec",
   "metadata": {},
   "source": [
    "### 1.6 Sliding Window Chunking \n",
    "- When text is too long to fit in memory, we create overlapping windown to preserve context\n",
    "- why: prevents cutting off dependencies between sequences.\n",
    "- Example Sequence length = 6, chunk size = 4, stride = 2\n",
    "``` text\n",
    "Sequence: [A B C D E F] (len = 6)\n",
    "\n",
    "Chunk 1: Start at position 0 â†’ [A B C D] (len = 4 beacuse chunk size = 4)\n",
    "Chunk 2: Start at position 0 (position) + 2(stride) = 2 â†’ [C D E F] (move the window by 2, keeps last 2 tokens of last chink in new chunk)\n",
    "\n",
    "Chunks:  [A B C D], [C D E F]\n",
    "\n",
    "Result Sequence: [A B C D], [C D E F]\n",
    "                      â†‘overlapâ†‘\n",
    "```\n",
    "- Edge case: If the final window doesn't have enough tokens (e.g., only 3 tokens left for chunk_size=4), you either pad it with `<PAD>` tokens or discard it depending on your training strategy.\n",
    "- overlapping ensures context continuity for training \n",
    "- common in LLM pretraining \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff521f58",
   "metadata": {},
   "source": [
    "### 1.7 Complete Example: Combining All Tokenization Steps\n",
    "\n",
    "This example demonstrates the entire pipeline from raw text to training-ready sequences, incorporating all concepts from 1.1-1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2957877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer (handles vocab, special tokens, BPE subword)\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Create dataset pipeline: tokenization â†’ chunking â†’ padding\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices([\"Hello world\", \"Goodbye moon\"])\n",
    "    .map(tokenizer)  # Tokenize: text â†’ IDs\n",
    "    .unbatch()  # Flatten to token stream (packing)\n",
    "    .batch(8, drop_remainder=False)  # Chunk into sequences of 8 tokens\n",
    "    .map(lambda x: (x[:-1], x[1:]))  # Create (input, target) pairs\n",
    "    .padded_batch(2, padded_shapes=([None], [None]))  # Pad and batch\n",
    ")\n",
    "\n",
    "# Usage:\n",
    "inputs, targets = next(iter(dataset))\n",
    "print(tokenizer.detokenize(inputs[0]))\n",
    "print(tokenizer.detokenize(targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14482b70",
   "metadata": {},
   "source": [
    "**OUTPUT:**\n",
    "```\n",
    "Hello worldGoodbye\n",
    "worldGoodbye moon\n",
    "```\n",
    "\n",
    "**EXPLANATION:**\n",
    "\n",
    "The dataset variable is a `tf.data.Dataset` pipeline that transforms raw text into training-ready (input, target) pairs. It's a lazy iterator (doesn't process until called).\n",
    "\n",
    "**Pipeline steps:**\n",
    "1. **from_tensor_slices**: Creates dataset from list of strings\n",
    "2. **map(tokenizer)**: Converts each text â†’ token IDs (e.g., \"Hello\" â†’ [15496, 995])\n",
    "3. **unbatch()**: Flattens all sequences into one continuous token stream (sequence packing)\n",
    "4. **batch(8)**: Groups tokens into chunks of 8 (creates fixed-length sequences)\n",
    "5. **map(lambda)**: Splits each chunk into (input, target) where target = input shifted left\n",
    "6. **padded_batch(2)**: Groups 2 sequences into a batch, pads shorter ones to match length\n",
    "\n",
    "**How next-token prediction works:**\n",
    "\n",
    "The model predicts the NEXT token at EACH position, not just the last one:\n",
    "- Position 0: Given \"Hello\" â†’ predict \"world\"\n",
    "- Position 1: Given \"Hello world\" â†’ predict \"Goodbye\"  \n",
    "- Position 2: Given \"Hello worldGoodbye\" â†’ predict \"moon\"\n",
    "\n",
    "So the target sequence shows what should be predicted at each step. The entire target = input shifted left by 1 token (each target is the next token).\n",
    "\n",
    "**Chunking Strategy Comparison:**\n",
    "\n",
    "Token stream: `[A, B, C, D, E, F, G, H, I, J]`\n",
    "\n",
    "**Fixed batch** (current): `.batch(4)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[E, F, G, H]` (tokens 4-7)\n",
    "- Chunk 3: `[I, J]` (tokens 8-9)\n",
    "- â†’ No overlap, each token appears once\n",
    "\n",
    "**Sliding window**: `.window(size=4, shift=2, drop_remainder=True)`\n",
    "- Chunk 1: `[A, B, C, D]` (tokens 0-3)\n",
    "- Chunk 2: `[C, D, E, F]` (tokens 2-5, overlaps last 2 from chunk 1)\n",
    "- Chunk 3: `[E, F, G, H]` (tokens 4-7, overlaps last 2 from chunk 2)\n",
    "- â†’ Overlap preserves context across chunks, useful for long documents\n",
    "\n",
    "**IMPORTANT: Both strategies train on next-token prediction at EVERY position!**\n",
    "\n",
    "**Fixed batch:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[E,F,G,H]` â†’ trains: (Eâ†’F), (E,Fâ†’G), (E,F,Gâ†’H)\n",
    "- Each token appears ONCE\n",
    "\n",
    "**Sliding window:**\n",
    "- Chunk 1: `[A,B,C,D]` â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "- Chunk 2: `[C,D,E,F]` â†’ trains: (Câ†’D), (C,Dâ†’E), (C,D,Eâ†’F)\n",
    "- Tokens C and D appear TWICE (extra training for better context)\n",
    "\n",
    "**Key Takeaway:** The chunking method only affects which tokens are grouped together, not how training works. Sliding window gives overlapping tokens extra exposure for better long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d86c5a",
   "metadata": {},
   "source": [
    "## 2 Embedding and Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7187be",
   "metadata": {},
   "source": [
    "### 2.1 Word embedding lookup tables\n",
    "\n",
    "**What a Embedding is**\n",
    "\n",
    "- An embedding is just a trainable lookup table:\n",
    "  - Shape: (vocab_size, d_model)\n",
    "  - Input: token ID's of shape (batch, seq_len)\n",
    "  - Output: vectors of shape (batch, seq_len, d_model)\n",
    "\n",
    "Each token ID indexes one row, the learning happens because gradients update those rows based on prediction error\n",
    "\n",
    "**Traning Proccess**: How Embeddings Are Learned\n",
    "\n",
    "1. **Random Initialization**: Embedding table starts with random values (shape: `vocab_size Ã— d_model`)\n",
    "\n",
    "2. **Forward Pass**: \n",
    "    - Token IDs â†’ Look up embeddings â†’ Feed through transformer â†’ Predict next token\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare prediction to actual next token\n",
    "    - Compute cross-entropy loss\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back through the entire model\n",
    "    - Embedding table rows get gradient updates based on which tokens were used\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    embedding[token_id] -= learning_rate Ã— gradient[token_id]\n",
    "    ```\n",
    "\n",
    "**Key Insight**: Tokens that appear in similar contexts will develop similar embeddings because they receive similar gradient updates. For example:\n",
    "- \"cat\" and \"dog\" â†’ often surrounded by words like \"pet\", \"animal\" â†’ embeddings become similar\n",
    "- \"king\" and \"queen\" â†’ share contexts like \"royal\", \"throne\" â†’ learn related representations\n",
    "\n",
    "The model learns embeddings **jointly** with all other parameters (attention weights, feedforward layers) to minimize prediction error across the entire training corpus.\n",
    "\n",
    "**Example Matrix:**\n",
    "``` text\n",
    "vocab_size = 50000  # Total unique tokens in vocabulary\n",
    "d_model = 512       # Each token â†’ 512-dimensional vector\n",
    "embedding_table = tf.Variable(shape=(50000, 512))\n",
    "# If token ID = 42, its embedding is embedding_table[42] (a 512-length vector)\n",
    "```\n",
    "\n",
    "In short the embedding is the models repersentation of a token, attention uses these repersentations to operate\n",
    "\n",
    "**TensorFlow Example**\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenEmbedding(tf.keras.layers.Layer): # inherit tf Layer Class\n",
    "    def __init__(self, vocab_size, d_model): # init model\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding( # create embedding layer\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=d_model\n",
    "        )\n",
    "\n",
    "    # override call function with out custom embeddign layer\n",
    "    def call(self, token_ids):\n",
    "        return self.embedding(token_ids)\n",
    "```\n",
    "\n",
    "Example IO:\n",
    "```text\n",
    "input:  [12,   431,   98] # token ID's\n",
    "\n",
    "# each token gets a embedding returned (output[i] = E[token_ids[i]]) where 'E' is teh embedding matrix\n",
    "output: [\n",
    "  E[12],     # âˆˆ R^d_model here 'R' is (real numbers) and 'd_model' is the number of dimentions in the model its a hyperparameter you chose (like 64, 128, 512, etc) for ex R^4 = [x1, x2, x3, x4] x can be any real number\n",
    "  E[431],    # âˆˆ R^d_model\n",
    "  E[98]      # âˆˆ R^d_model\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032ee2e",
   "metadata": {},
   "source": [
    "### 2.2 Unembedding and tied embeddings\n",
    "\n",
    "#### 2.2.1 Unembedding\n",
    "\n",
    "Unembedding converts each transformer output vector into a score for every token in the vocabulary so the model can predict which token comes next. we need this at the output of the transformer\n",
    "```\n",
    "Vocabulary: [\"hello\", \"world\", \"cat\", \"dog\", ...] (ex 50000 words)\n",
    "                  â†“       â†“      â†“     â†“\n",
    "Logits:          [2.1,   0.5,  -1.2,  3.4, ...]  â† Higher score = more likely next token (50000 scores one for each token)\n",
    "```\n",
    "this is done with linear projection.\n",
    "\n",
    "- Input: (batch, seq_len, d_model)\n",
    "- Weight: (d_model, vocab_size)\n",
    "- Output: (batch, seq_len, vocab_size)\n",
    "\n",
    "**NOTE:** this layer is also called the unembedding or LM head\n",
    "\n",
    "**NOTE:** The converstion of the token ID back to a final word is the tokenizers job do not mistake unebedding for that\n",
    "\n",
    "\n",
    "**Training Process**: How Unembedding Weights Are Learned\n",
    "\n",
    "1. **Random Initialization**: Unembedding matrix starts with random values (shape: `d_model Ã— vocab_size`)\n",
    "\n",
    "2. **Forward Pass**:\n",
    "    - Transformer outputs â†’ (batch, seq_len, d_model)\n",
    "    - Matrix multiply with unembedding weights â†’ (batch, seq_len, vocab_size)\n",
    "    - Apply softmax â†’ probability distribution over vocabulary\n",
    "\n",
    "3. **Loss Calculation**:\n",
    "    - Compare predicted probabilities to actual next token (one-hot encoded)\n",
    "    - Compute cross-entropy loss: `loss = -log(P(correct_token))`\n",
    "\n",
    "4. **Backpropagation**:\n",
    "    - Gradients flow back from loss through softmax and unembedding layer\n",
    "    - Each row of the unembedding matrix (corresponding to one output dimension) gets updated based on prediction errors\n",
    "\n",
    "5. **Update Rule** (simplified):\n",
    "    ```\n",
    "    unembedding_weights -= learning_rate Ã— gradient\n",
    "    ```\n",
    "\n",
    "**Key Insight**: The unembedding layer learns which transformer output patterns correspond to which tokens. If the model frequently outputs vectors in a certain direction when \"cat\" should be next, those weights get strengthened to produce higher logits for \"cat\".\n",
    "\n",
    "**Example:**\n",
    "``` text\n",
    "d_model = 512\n",
    "vocab_size = 50000\n",
    "unembedding_matrix = tf.Variable(shape=(512, 50000))\n",
    "\n",
    "# Transformer output: (batch=1, seq_len=1, d_model=512)\n",
    "# After matmul: (batch=1, seq_len=1, vocab_size=50000)\n",
    "# Each position gets 50000 scores (one per possible next token)\n",
    "\n",
    "# Mathamatically\n",
    "\n",
    "# Hidden vector h âˆˆ R^d_model # from tansfromer \n",
    "# Unembedding matrix W âˆˆ R^(d_model Ã— vocab_size) # learned matrix\n",
    "# Logits l = h Â· W   â†’ l âˆˆ R^vocab_size # 50000 logits as a result\n",
    "```\n",
    "\n",
    "**The flow so far is as follows**: Token ID â†’ Embedding â†’ Transformer â†’ Hidden Vector â†’ Unembedding â†’ Logits â†’ Token ID â†’ Word (where we pick the highest logit as next word)\n",
    "\n",
    "**Tensorflow Example**\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "\n",
    "class TokenUnembedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        # Linear layer without bias: projects hidden vectors to vocab logits\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        # hidden_states: (batch, seq_len, d_model)\n",
    "        # logits: (batch, seq_len, vocab_size)\n",
    "        logits = self.dense(hidden_states)\n",
    "        return logits\n",
    "\n",
    "```\n",
    "\n",
    "#### 2.2.2 Tied Embeddings\n",
    "\n",
    "Modern LLMs tie the input embedding matrix and output projection weights \n",
    "\n",
    "- Why it works:\n",
    "  - the same geometric space is used for reading and writing tokens \n",
    "  - reduces parameters \n",
    "  - improves sample efficiency and stability\n",
    "- Mathamatically\n",
    "  - input embedding: E[token_id]\n",
    "  - output logits: h Â· Eáµ€ (h is the hidden vector)\n",
    "  \n",
    "This enforces symmetry between encoding and decoding\n",
    "\n",
    "**Tensorflow example**\n",
    "``` py\n",
    "class TiedOutputProjection(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_layer):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "\n",
    "    def call(self, hidden_states):\n",
    "        embedding_matrix = self.embedding_layer.embedding.embeddings\n",
    "        logits = tf.einsum(\"btd,vd->btv\", hidden_states, embedding_matrix)\n",
    "        return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a76fb",
   "metadata": {},
   "source": [
    "### 2.3 Why positional representations are required (ML notes pg 112)\n",
    "\n",
    "**Self attention is permutation-invariant.**\n",
    "\n",
    "That means:\n",
    "- â€œcat sat matâ€ and â€œmat sat catâ€ look identical without position.\n",
    "- Order must be injected explicitly.\n",
    "\n",
    "Positions noting sequence index are added or applied to embeddings before attention.\n",
    "\n",
    "**Key idea:**\n",
    "Token meaning + position meaning = input representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b4c2d",
   "metadata": {},
   "source": [
    "### 2.4 Positional encoding types\n",
    "\n",
    "#### 2.4.1 Sinusodal positional encoding (ML Notes pg 112)\n",
    "\n",
    "**Concept**\n",
    "- Deterministic, non-trainable\n",
    "- Uses sine and cosine at different frequencies\n",
    "- Allows extrapolation to longer sequences\n",
    "\n",
    "**Formulas:**\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "**Why it works:**\n",
    "- Relative positions can be inferred from linear combinations\n",
    "- Attention can compute distance relationships\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "import math\n",
    "\n",
    "def sinusoidal_position_encoding(seq_len, d_model):\n",
    "    position = tf.range(seq_len, dtype=tf.float32)[:, None]\n",
    "    div_term = tf.exp(\n",
    "        tf.range(0, d_model, 2, dtype=tf.float32) *\n",
    "        -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    pe = tf.zeros((seq_len, d_model))\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(0, d_model, 2)[:, None],\n",
    "        updates=tf.sin(position * div_term)\n",
    "    )\n",
    "    pe = tf.tensor_scatter_nd_update(\n",
    "        pe,\n",
    "        indices=tf.range(1, d_model, 2)[:, None],\n",
    "        updates=tf.cos(position * div_term)\n",
    "    )\n",
    "    return pe\n",
    "```\n",
    "\n",
    "**added directly to token embeddings**\n",
    "\n",
    "#### 2.4.2 Learned positional embeddings (ML notes pg 112)\n",
    "**Concept**\n",
    "- Position IDs get their own embedding table\n",
    "- Fully learned\n",
    "- Used in GPT-2, BERT\n",
    "\n",
    "**Pros:**\n",
    "- Flexible\n",
    "- Often better on fixed context lengths\n",
    "\n",
    "**Cons:**\n",
    "- Cannot extrapolate beyond max length trained\n",
    "\n",
    "**Tensorflow Example:**\n",
    "```py\n",
    "class LearnedPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(max_len, d_model)\n",
    "\n",
    "    def call(self, seq_len):\n",
    "        positions = tf.range(seq_len)\n",
    "        return self.embedding(positions)\n",
    "```\n",
    "\n",
    "#### 2.4.3 Rotary Position Embeddings (RoPE)\n",
    "This is where modern LLMs diverge from early transformers.\n",
    "\n",
    "**Core idea:** RoPE rotates query and key vectors in embedding space based on position.\n",
    "\n",
    "**Key properties:**\n",
    "- Position information is applied inside attention\n",
    "- Enables relative position reasoning\n",
    "- Scales well to long contexts\n",
    "\n",
    "Instead of adding position vectors, we rotate pairs of dimensions:\n",
    "```text\n",
    "embedding vector = [x1, x2] â†’ rotation by angle Î¸( determined by tokens position)\n",
    "x1' = x1*cosÎ¸ - x2*sinÎ¸\n",
    "x2' = x1*sinÎ¸ + x2*cosÎ¸\n",
    "x_rotated = [x1', x2']\n",
    "```\n",
    "\n",
    "**Why this is powerful**\n",
    "- Dot products encode relative distance naturally when dot product of K,Q are rotated\n",
    "- No learned position embeddings\n",
    "- Better extrapolation\n",
    "\n",
    "**TensorFlow implementation**\n",
    "```py\n",
    "def rotary_embedding(x, seq_len):\n",
    "    d_model = x.shape[-1]\n",
    "    half = d_model // 2\n",
    "\n",
    "    freqs = tf.exp(\n",
    "        -tf.range(0, half, dtype=tf.float32) / half * tf.math.log(10000.0)\n",
    "    )\n",
    "    positions = tf.range(seq_len, dtype=tf.float32)\n",
    "    angles = positions[:, None] * freqs[None, :]\n",
    "\n",
    "    sin = tf.sin(angles)\n",
    "    cos = tf.cos(angles)\n",
    "\n",
    "    x1 = x[..., :half]\n",
    "    x2 = x[..., half:]\n",
    "\n",
    "    rotated = tf.concat(\n",
    "        [x1 * cos - x2 * sin,\n",
    "         x1 * sin + x2 * cos],\n",
    "        axis=-1\n",
    "    )\n",
    "    return rotated\n",
    "```\n",
    "**Applied to queries and keys only, never values.**\n",
    "\n",
    "#### 2.4.4 ALiBi\n",
    "**Concept**\n",
    "- No position embeddings at all\n",
    "- Adds a linear bias to attention scores\n",
    "- Penalizes distant tokens\n",
    "\n",
    "**Attention score becomes:**\n",
    "``` text\n",
    "QKáµ€ / sqrt(d) + bias(distance)\n",
    "```\n",
    "\n",
    "**Why it matters:**\n",
    "- Extremely simple\n",
    "- Strong extrapolation to long sequences\n",
    "- Used in MPT and others\n",
    "\n",
    "**NOTE:** ALiBi changes attention, not embeddings.\n",
    "\n",
    "**You do not add position vectors at input.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8b1e2",
   "metadata": {},
   "source": [
    "### 2.5 How positional encoding interacts with attention\n",
    "**Important clarity:**\n",
    "- Additive encodings modify token representations before attention.\n",
    "- RoPE modifies query and key geometry.\n",
    "- ALiBi modifies attention logits directly.\n",
    "\n",
    "**All three inject order, but at different stages.**\n",
    "\n",
    "**This choice affects:**\n",
    "- Long context scaling\n",
    "- Memory behavior\n",
    "- Generalization beyond training length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786ff05",
   "metadata": {},
   "source": [
    "### 2.6 Embedding scaling by sqrt(d_model)\n",
    "\n",
    "When embeddings are initialized, their variance is small.\n",
    "\n",
    "If we add positional encodings directly, they can dominate early training.\n",
    "\n",
    "**Standard fix:**\n",
    "```text\n",
    "x = embedding(token_ids) * sqrt(d_model)\n",
    "x = x + position_encoding\n",
    "```\n",
    "\n",
    "**This ensures:**\n",
    "- Token identity dominates initially\n",
    "- Position is a refinement, not the signal\n",
    "\n",
    "**TF snippet**\n",
    "```py\n",
    "x = token_embedding(token_ids)\n",
    "x *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "x += position_encoding\n",
    "```\n",
    "\n",
    "**This is not cosmetic. It stabilizes training.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbf855",
   "metadata": {},
   "source": [
    "### 2.7 Complete Example: Combining All Embedding Steps\n",
    "\n",
    "This example demonstrates the entire embedding pipeline from token IDs to final logits, a code example for 2.1-2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03605a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer and model\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Example text: Notice \"cat\" appears at different positions\n",
    "text = \"The cat sat on the mat. I love my cat very much.\"\n",
    "\n",
    "# Step 1: Tokenize (from section 1)\n",
    "token_ids = tokenizer(text) # Token IDs: [ 464 3797 3332  319  262 2603   13  314 1842  616 3797  845  881   13]\n",
    "\n",
    "# Step 2: Token Embedding (2.1) - Convert IDs to vectors\n",
    "token_embedding = backbone.token_embedding # Token Embeddings shape: (14, 768), Each token â†’ 768-dimensional vector\n",
    "token_embeds = token_embedding(token_ids)\n",
    "\n",
    "# Step 3: Position Embedding (2.4) - Add position information\n",
    "position_embedding = backbone.position_embedding # Position Embeddings shape: (14, 768)\n",
    "pos_embeds = position_embedding(token_embeds)\n",
    "\n",
    "# Step 4: Combine (2.5) - Token meaning + Position\n",
    "final_embeds = token_embeds + pos_embeds #  Final Embeddings (token + position): (14, 768)\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Unembedding (2.2) - Project back to vocabulary\n",
    "# ============================================================\n",
    "print(f\"\\n5. Unembedding (tied weights):\")\n",
    "# NOTE: We're using GPT-2's PRETRAINED embedding weights\n",
    "# These were learned on billions of tokens, so predictions are meaningful\n",
    "# In practice, you'd train these from scratch on your data\n",
    "embedding_matrix = token_embedding.embeddings  # Reuse same pretrained weights\n",
    "logits = tf.matmul(final_embeds, embedding_matrix, transpose_b=True) # Logits shape: (14, 50257)\n",
    "predicted_token_ids = tf.argmax(logits, axis=-1)  # Predicted Token IDs shape: (14,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebce668",
   "metadata": {},
   "source": [
    "**SUMMARY: What we covered from sections 2.1-2.6**\n",
    "\n",
    "**1. TOKEN EMBEDDING (2.1):**\n",
    "- Input: Token IDs [464, 3797, ...]\n",
    "- Output: Dense vectors [(768 dims), (768 dims), ...]\n",
    "- â†’ Each token gets a learned representation\n",
    "\n",
    "**2. POSITION EMBEDDING (2.4):**\n",
    "- Learned embeddings that encode position\n",
    "- Position 0 â‰  Position 10 (different vectors)\n",
    "- â†’ Tells model WHERE each token is\n",
    "\n",
    "**3. COMBINING (2.5):**\n",
    "- final = token_embed + pos_embed\n",
    "- â†’ Same word at different positions has different final representations\n",
    "\n",
    "**4. UNEMBEDDING (2.2):**\n",
    "- Projects embeddings â†’ vocabulary logits\n",
    "- Uses TIED WEIGHTS (same matrix as input embeddings)\n",
    "- â†’ Predicts next token distribution\n",
    "\n",
    "**IMPORTANT:** This example uses GPT-2's PRETRAINED embeddings!\n",
    "- These weights were learned on billions of tokens of web text\n",
    "- That's why predictions are meaningful (not random)\n",
    "- In your own model, you'd train these from scratch on your data\n",
    "- Training process: same backpropagation as described in 2.1\n",
    "\n",
    "**Key Insight:** Without position embeddings, \"cat sat mat\" and \"mat sat cat\" would look identical to the model. Position info is CRITICAL.\n",
    "\n",
    "**Note:** We haven't covered transformers yet - that processes these embeddings! The transformer would sit between step 4 (final embeddings) and step 5 (unembedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a45ff",
   "metadata": {},
   "source": [
    "## 3 Attention\n",
    "(ML notes pg 54-61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9acf6e",
   "metadata": {},
   "source": [
    "#### 3.1 Query, Key, Value fundamentals\n",
    "Every token embedding is projected into three different spaces:\n",
    "- Query (Q): what this token is looking for\n",
    "- Key (K): what this token offers\n",
    "- Value (V): the information this token contributes if selected\n",
    "\n",
    "All three come from the same input embedding x, but with different learned linear projections. The input token starts as the same vector x and is transformed using a learned weight matrix\n",
    "\n",
    "**Mathematically:**\n",
    "- Q = x Wq\n",
    "- K = x Wk\n",
    "- V = x Wv\n",
    "  \n",
    "Why this works:\n",
    "- Attention becomes content-addressable memory.\n",
    "- Tokens donâ€™t attend by position or index, but by similarity in meaning.\n",
    "\n",
    "Conceptual analogy:\n",
    "- Query = question\n",
    "- Key = label on a memory slot\n",
    "- Value = content inside the slot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4c411",
   "metadata": {},
   "source": [
    "### 3.2. Dot product attention\n",
    "\n",
    "The core operation:\n",
    "```math\n",
    "attention(Q, K, V) = softmax(Q Káµ€ / âˆšdk) V\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "1. Compute similarity between every query and every key.\n",
    "2. Normalize scores with softmax.\n",
    "3. Use scores to form weighted sums of values.\n",
    "\n",
    "This produces:\n",
    "- For each token, a contextualized representation.\n",
    "- Tokens can pull information from any previous token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38a4ac",
   "metadata": {},
   "source": [
    "### 3.3 Why divide by âˆšdk\n",
    "\n",
    "Without scaling:\n",
    "- Dot products grow with dimension.\n",
    "- Softmax saturates (becomes near one-hot).\n",
    "- Gradients vanish.\n",
    "\n",
    "Scaling by **sqrt(dk)** keeps variance stable.\n",
    "\n",
    "This is not optional. Training becomes unstable without it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1552",
   "metadata": {},
   "source": [
    "### 3.4 Causal masking and autoregressive behavior\n",
    "\n",
    "LLMs generate left to right. A token must not see future tokens during training.\n",
    "\n",
    "Causal mask:\n",
    "- Upper triangular matrix filled with -inf\n",
    "- Added to attention logits before softmax\n",
    "\n",
    "Effect:\n",
    "- Positions j > i get zero probability\n",
    "- Guarantees autoregressive behavior\n",
    "\n",
    "Conceptually:\n",
    "- Training simulates generation\n",
    "- Prediction at position i only depends on < i\n",
    "\n",
    "**Similarity Scores Q Káµ€ (no mask)**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 0.60 | 0.30 | 0.10 |\n",
    "| love       | 0.20 | 0.50 | 0.30 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |\n",
    "\n",
    "**Casual mask**\n",
    "| Query \\ Key | I    | love | ML   |\n",
    "|------------|------|------|------|\n",
    "| I          | 1.00 | 0.00 | 0.00 |\n",
    "| love       | 0.29 | 0.71 | 0.00 |\n",
    "| ML         | 0.10 | 0.67 | 0.23 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15224027",
   "metadata": {},
   "source": [
    "### 3.5 Softmax details and numerical stability\n",
    "\n",
    "Softmax is:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "$$\n",
    "\n",
    "Numerical issue:\n",
    "- Large logits overflow\n",
    "\n",
    "Standard trick:\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max_j x_j)}{\\sum_j \\exp(x_j - \\max_j x_j)}\n",
    "$$\n",
    "This preserves probabilities but stabilizes computation.\n",
    "\n",
    "TensorFlow handles this internally, but you must remember why it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf452b3",
   "metadata": {},
   "source": [
    "### 3.6 Multi-head attention\n",
    "Single head attention has one similarity space basicaly it can only attend to one type of question. see `ML notes pg 61` for more\n",
    "\n",
    "Multi-head attention:\n",
    "- Splits channels into h heads\n",
    "- Each head attends differently\n",
    "- Results are concatenated and projected\n",
    "\n",
    "Shape intuition:\n",
    "```\n",
    "(batch, seq, d_model)\n",
    "â†’ (batch, heads, seq, d_head)\n",
    "```\n",
    "Where:\n",
    "```\n",
    "d_head = d_model / heads\n",
    "```\n",
    "\n",
    "Why this matters:\n",
    "- Different heads learn syntax, semantics, coreference, long-range dependencies\n",
    "- Heads act as independent subspaces\n",
    "\n",
    "**Example**\n",
    "Tokens: T1, T2, T3\n",
    "\n",
    "Head 1 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.1 | 0.3 | 0.6 |\n",
    "| 0.2 | 0.5 | 0.3 |\n",
    "\n",
    "Head 2 output:\n",
    "| T1 | T2 | T3 |\n",
    "|----|----|----|\n",
    "| 0.4 | 0.4 | 0.2 |\n",
    "| 0.3 | 0.3 | 0.4 |\n",
    "\n",
    "Concatenated (along features):\n",
    "| T1         | T2         | T3         |\n",
    "|------------|------------|------------|\n",
    "| 0.1, 0.4   | 0.3, 0.4   | 0.6, 0.2   |\n",
    "| 0.2, 0.3   | 0.5, 0.3   | 0.3, 0.4   |\n",
    "\n",
    "**Multi-Head Attention Combination**\n",
    "\n",
    "Each head computes its own attention output independently:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)\n",
    "$$\n",
    "\n",
    "Outputs of all heads are concatenated along the feature dimension:\n",
    "$$\n",
    "\\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\in \\mathbb{R}^{d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "A final learned linear projection mixes them:\n",
    "$$\n",
    "\\text{MHA\\_output} = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\cdot W_O\n",
    "$$\n",
    "\n",
    "Note: attention scores are not combined across heads; each head attends separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64eab8",
   "metadata": {},
   "source": [
    "### 3.7 Attention complexity\n",
    "\n",
    "Time and memory:\n",
    "```O(seq_lenÂ²)```\n",
    "\n",
    "This is the main scaling bottleneck in LLMs.\n",
    "\n",
    "Implications:\n",
    "- Long context is expensive\n",
    "- Motivates FlashAttention, sparse attention, sliding windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa64d3",
   "metadata": {},
   "source": [
    "### 3.8 Memory layout and tensor shapes (critical)\n",
    "\n",
    "Most bugs in attention come from shape mistakes.\n",
    "\n",
    "Typical shapes:\n",
    "- Q, K, V: (batch, heads, seq_len, d_head) # d_head = d_model / heads\n",
    "- Attention scores: (batch, heads, seq_len, seq_len)\n",
    "- Output: (batch, seq_len, d_model)\n",
    "\n",
    "Reshaping and transposing correctly is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba8c74",
   "metadata": {},
   "source": [
    "### 3.9 Flash Attention (conceptual only)\n",
    "\n",
    "Flash Attention:\n",
    "- Computes attention without materializing full seq_len Ã— seq_len matrix\n",
    "- Uses tiling and fused kernels\n",
    "- Reduces memory bandwidth bottleneck\n",
    "\n",
    "Important takeaway:\n",
    "- Same math, different execution\n",
    "- You do not change the model, only the kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd3832",
   "metadata": {},
   "source": [
    "### 3.10 Self attention vs cross attention\n",
    "\n",
    "- Self attention: Q, K, V come from same sequence\n",
    "- Cross attention: Q from decoder, K/V from encoder\n",
    "\n",
    "GPT style LLMs use only self attention.\n",
    "\n",
    "Encoderâ€“decoder models use both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5147b0",
   "metadata": {},
   "source": [
    "### 3.11 Key-value caching for generation\n",
    "\n",
    "During autoregressive generation:\n",
    "- Past K and V do not change\n",
    "- Only compute Q for new token\n",
    "- Append new K, V to cache\n",
    "\n",
    "Effect:\n",
    "- Reduces per-token cost from O(nÂ²) to O(n)\n",
    "\n",
    "\n",
    "This is essential for fast inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebfc97c",
   "metadata": {},
   "source": [
    "### 3.12 Tensorflow Example (no raw kernal high level usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1f50c",
   "metadata": {},
   "source": [
    "#### 3.12.1 Single-head attention (conceptual TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def single_head_attention(x, mask=None):\n",
    "    d_model = x.shape[-1]\n",
    "\n",
    "    q = tf.keras.layers.Dense(d_model)(x)\n",
    "    k = tf.keras.layers.Dense(d_model)(x)\n",
    "    v = tf.keras.layers.Dense(d_model)(x)\n",
    "\n",
    "    scores = tf.matmul(q, k, transpose_b=True)\n",
    "    scores /= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    output = tf.matmul(weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa167c",
   "metadata": {},
   "source": [
    "#### 3.12.2 Multi-head attention (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7494795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        return self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=mask,\n",
    "            training=training\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6333c89",
   "metadata": {},
   "source": [
    "#### 3.12.3 Causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066809e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    mask = tf.linalg.band_part(\n",
    "        tf.ones((seq_len, seq_len)), -1, 0\n",
    "    )\n",
    "    return 1.0 - mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e0269",
   "metadata": {},
   "source": [
    "#### 3.12.4 Multi-Head Attention in Action: Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da922e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "backbone = keras_nlp.models.GPT2Backbone.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Interesting sentence with clear relationships\n",
    "text = \"The cat chased the mouse\"\n",
    "\n",
    "# Tokenize\n",
    "token_ids = tokenizer(text)\n",
    "tokens = [tokenizer.detokenize([tid]).numpy().decode() if isinstance(tokenizer.detokenize([tid]), tf.Tensor) \n",
    "          else tokenizer.detokenize([tid]) for tid in token_ids.numpy()]\n",
    "\n",
    "if tf.rank(token_ids) == 1:\n",
    "    token_ids = token_ids[None, :]\n",
    "\n",
    "# Get embeddings\n",
    "x_tokens = backbone.token_embedding(token_ids)\n",
    "x_pos = backbone.position_embedding(x_tokens)\n",
    "embeddings = x_tokens + x_pos\n",
    "\n",
    "# Pass through FIRST transformer layer (uses pretrained attention weights!)\n",
    "transformer_layer = backbone.transformer_layers[0]\n",
    "output_embeddings = transformer_layer(embeddings)\n",
    "\n",
    "# NEXT WORD PREDICTION (after just 1 layer)\n",
    "\n",
    "# Unembed: project back to vocabulary\n",
    "embedding_matrix = backbone.token_embedding.embeddings\n",
    "logits = tf.matmul(output_embeddings, embedding_matrix, transpose_b=True)\n",
    "\n",
    "# Get top 3 predictions for the LAST position (next word)\n",
    "last_logits = logits[0, -1, :]\n",
    "top3_ids = tf.argsort(last_logits, direction='DESCENDING')[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f192907",
   "metadata": {},
   "source": [
    "#### 3.12.5 What Transformers Do and what do (context integration)\n",
    "**The transformer's job when updating embeddings:**\n",
    "\n",
    "Each transformer layer refines token embeddings by **integrating context from surrounding tokens** via attention:\n",
    "- Before: Each token's embedding is isolated (just word + position info)\n",
    "- After: Each token's embedding incorporates information from relevant context\n",
    "- Example: \"mouse\" embedding gets updated based on \"cat chased\" â†’ now encodes \"something being chased\"\n",
    "\n",
    "**Why this matters for prediction:**\n",
    "\n",
    "Updated embeddings â†’ better next-token predictions. GPT-2 stacks 12 layers because:\n",
    "- **1 layer**: Minimal context integration â†’ weak predictions\n",
    "- **12 layers**: Deep reasoning across all context â†’ strong predictions\n",
    "\n",
    "**Logit scores:** Higher number = more likely next token (passed through softmax to get probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7a163",
   "metadata": {},
   "source": [
    "## 4 Feed Forward Networks (MLP block)\n",
    "ML notes pages 62-68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82210f00",
   "metadata": {},
   "source": [
    "### 4.1 What the FFN is really doing\n",
    "\n",
    "In a transformer layer, attention answers:\n",
    "\n",
    "â€œWhich tokens should talk to each other?â€\n",
    "\n",
    "The FFN answers:\n",
    "\n",
    "â€œWhat computation should I apply to each token independently? its basically where your questions are answerd and facts are storedâ€\n",
    "\n",
    "Key property:\n",
    "- The FFN is position-wise.\n",
    "- Same MLP is applied to every token.\n",
    "- No interaction across sequence here, only across channels.\n",
    "\n",
    "Mathematically, for each token vector x:\n",
    "```math\n",
    "FFN(x) = W2 Â· Ïƒ(W1 Â· x + b1) + b2\n",
    "```\n",
    "the W2 and B2 are for projecting the linear step W1 Â· x + b1 back down to a lower dim, the lenaer step is made up of a wight matrix and bias just like a NN and just liek a NN it also has a activation function Ïƒ to add non linearity\n",
    "\n",
    "This is where:\n",
    "- Nonlinearity enters the model.\n",
    "- Feature composition happens.\n",
    "- Model capacity explodes.\n",
    "\n",
    "Most of the parameters in modern LLMs live here, not in attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5bd0d",
   "metadata": {},
   "source": [
    "### 4.2 Two-layer MLP structure\n",
    "\n",
    "Standard configuration:\n",
    "- Input dimension: d_model\n",
    "- Hidden dimension: d_ff (usually 4Ã— d_model)\n",
    "- Output dimension: d_model\n",
    "\n",
    "Example for GPT-like models:\n",
    "- d_model = 768\n",
    "- d_ff = 3072\n",
    "\n",
    "Why expand then contract:\n",
    "- Expansion creates a high-dimensional feature space.\n",
    "- Nonlinearity selects and combines features.\n",
    "- Projection back keeps residual dimensions stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a9c4f",
   "metadata": {},
   "source": [
    "### 4.3 Activation functions and why GELU matters\n",
    "#### 4.3.1 ReLU\n",
    "`ReLU(x) = max(0, x)`\n",
    "\n",
    "Pros:\n",
    "- Simple\n",
    "- Fast\n",
    "\n",
    "Cons:\n",
    "- Hard cutoff at zero\n",
    "- Less expressive for language\n",
    "\n",
    "#### 4.3.2 GELU (Gaussian Error Linear Unit)\n",
    "```\n",
    "GELU(x) = x Â· Î¦(x)\n",
    "```\n",
    "Where Î¦ is the standard normal CDF.\n",
    "\n",
    "Intuition:\n",
    "- Smooth gating\n",
    "- Small negative values partially pass\n",
    "- Better gradient flow\n",
    "\n",
    "Why LLMs prefer GELU:\n",
    "- Language is continuous, not sparse\n",
    "- Smooth activations model nuance better\n",
    "- Empirically improves perplexity\n",
    "\n",
    "GPT-2, BERT, and many successors use GELU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476b3fc",
   "metadata": {},
   "source": [
    "### 4.4 Modern variants: SwiGLU\n",
    "This is where newer LLMs (PaLM, LLaMA, Mistral) improve capacity.\n",
    "\n",
    "**Core idea**\n",
    "Instead of one activation, split the hidden layer and gate it:\n",
    "```\n",
    "FFN(x) = (W1 x âŠ™ Ïƒ(W2 x)) W3\n",
    "```\n",
    "\n",
    "For SwiGLU:\n",
    "- Ïƒ is Swish: x Â· sigmoid(x)\n",
    "- âŠ™ means element-wise multiplication\n",
    "- The gate controls information flow\n",
    "\n",
    "Why this works:\n",
    "- Explicit multiplicative interaction\n",
    "- Better feature selection\n",
    "- More expressive than plain GELU\n",
    "\n",
    "Important note:\n",
    "- Hidden dimension is often reduced (e.g. 2/3 of 4Ã—) to keep parameter count similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ee1df",
   "metadata": {},
   "source": [
    "### 4.5 Dropout and residual scaling\n",
    "\n",
    "Dropout:\n",
    "- Applied after activation or projection\n",
    "- Regularizes training\n",
    "- Less critical in massive pretraining, more in fine-tuning\n",
    "\n",
    "Residual scaling:\n",
    "- Output of FFN is added to residual stream\n",
    "- Large activations can destabilize training\n",
    "- Some models scale FFN output by a small constant early in training\n",
    "\n",
    "Core pattern:\n",
    "`x = x + dropout(FFN(LN(x)))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40016a8a",
   "metadata": {},
   "source": [
    "### 4.6 TensorFlow: Configurable FFN block (high level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\" \n",
    "    Feed-Forward Network with configurable activation.\n",
    "    Supports ReLU, GELU, and SwiGLU activations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        activation=\"gelu\",\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        if activation == \"swiglu\":\n",
    "            self.w1 = tf.keras.layers.Dense(d_ff)\n",
    "            self.w2 = tf.keras.layers.Dense(d_ff)\n",
    "            self.w3 = tf.keras.layers.Dense(d_model)\n",
    "        else:\n",
    "            self.dense1 = tf.keras.layers.Dense(d_ff)\n",
    "            self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if self.activation == \"relu\":\n",
    "            h = tf.nn.relu(self.dense1(x))\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.dense2(h)\n",
    "\n",
    "        if self.activation == \"gelu\":\n",
    "            h = tf.nn.gelu(self.dense1(x))\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.dense2(h)\n",
    "\n",
    "        if self.activation == \"swiglu\":\n",
    "            h = self.w1(x)\n",
    "            gate = tf.nn.swish(self.w2(x))\n",
    "            h = h * gate\n",
    "            h = self.dropout(h, training=training)\n",
    "            return self.w3(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da5292",
   "metadata": {},
   "source": [
    "using it inside the transformer layer (EX)\n",
    "```py\n",
    "ffn = FeedForward(\n",
    "    d_model=768,\n",
    "    d_ff=3072,\n",
    "    activation=\"gelu\"\n",
    ")\n",
    "\n",
    "y = ffn(x, training=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf12016",
   "metadata": {},
   "source": [
    "## 5. Normalization and Stabilization in Transformers\n",
    "\n",
    "ML notes page 117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3c6f4",
   "metadata": {},
   "source": [
    "### 5.1 Why normalization is critical in deep transformers\n",
    "\n",
    "Transformers stack dozens to hundreds of layers. Each layer:\n",
    "- Applies attention\n",
    "- Applies an FFN\n",
    "- Adds residual connections (You add the layerâ€™s input back to its output, this way Each layer learns a small correction, not a full transformation.)\n",
    "\n",
    "```\n",
    "DEFINITION:\n",
    "Residual stream = the running hidden state that flows through the model via residual connections.\n",
    "Short definition: It is the vector being repeatedly updated as: x â† x + sublayer_output at every layer.\n",
    "```\n",
    "\n",
    "Without normalization:\n",
    "- Activations grow with depth\n",
    "- Variance drifts layer by layer\n",
    "- Gradients either explode or vanish\n",
    "- Training becomes extremely sensitive to learning rate and initialization\n",
    "\n",
    "Normalization exists to control the statistics of the residual stream so depth becomes feasible.\n",
    "\n",
    "**The residual stream must remain well-conditioned across layers.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613b0aa",
   "metadata": {},
   "source": [
    "### 5.2 LayerNorm inside transformers\n",
    "\n",
    "**What LayerNorm actually does**\n",
    "\n",
    "LayerNorm normalizes across features, not across batch.\n",
    "\n",
    "For a token vector `x âˆˆ R^d_model`:\n",
    "\n",
    "```\n",
    "LN(x) = (x âˆ’ mean(x)) / sqrt(var(x) + Îµ) * Î³ + Î²\n",
    "```\n",
    "\n",
    "Important properties:\n",
    "- Applied per token\n",
    "- Independent of batch size\n",
    "- Ideal for sequence models\n",
    "\n",
    "This is why transformers do not use BatchNorm.\n",
    "\n",
    "**Where LayerNorm is applied**\n",
    "\n",
    "In transformers, LayerNorm is applied inside each block, not globally.\n",
    "\n",
    "**NOTE:** Two main placements exist. This choice matters a lot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cca33",
   "metadata": {},
   "source": [
    "### 5.3 Post-Norm vs Pre-Norm\n",
    "\n",
    "#### 5.3.1 Post-Norm (original Transformer)\n",
    "Structure:\n",
    "```\n",
    "x â†’ Attention â†’ Add â†’ LayerNorm\n",
    "x â†’ FFN â†’ Add â†’ LayerNorm\n",
    "```\n",
    "Add = residual connection, It means: take the input x and add it element-wise to the sublayer output, Sublayer output = the result of the operation inside the block before the residual add.\n",
    "\n",
    "Problems:\n",
    "- Gradients must pass through many nonlinear layers before normalization\n",
    "- Deep models become unstable\n",
    "- Training requires careful warmup and small learning rates\n",
    "\n",
    "This works for shallow models but breaks at scale.\n",
    "\n",
    "#### 5.3.2 Pre-Norm (modern standard)\n",
    "Structure:\n",
    "```\n",
    "x â†’ LayerNorm â†’ Attention â†’ Add\n",
    "x â†’ LayerNorm â†’ FFN â†’ Add\n",
    "```\n",
    "\n",
    "Why this works better:\n",
    "- Residual path is clean and unnormalized\n",
    "- Gradients flow directly through identity connections\n",
    "- Much more stable for deep stacks\n",
    "\n",
    "Key insight:\n",
    "- Pre-norm turns the transformer into a well-behaved residual network.\n",
    "- Almost all modern LLMs use Pre-Norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35db75",
   "metadata": {},
   "source": [
    "### 5.4 RMSNorm\n",
    "\n",
    "**What RMSNorm is**\n",
    "\n",
    "RMSNorm removes mean-centering and only normalizes by root mean square.\n",
    "\n",
    "`RMSNorm(x) = x / rms(x) * Î³`\n",
    "\n",
    "Where:\n",
    "`rms(x) = sqrt(mean(xÂ²))`\n",
    "\n",
    "Differences from LayerNorm:\n",
    "- No subtraction of mean\n",
    "- Fewer operations\n",
    "- Slightly faster\n",
    "- Numerically simpler\n",
    "\n",
    "**Why modern LLMs prefer RMSNorm**\n",
    "\n",
    "Empirical findings:\n",
    "- Mean-centering is not strictly necessary\n",
    "- RMS scaling alone stabilizes training\n",
    "- Works well with large batch sizes and long contexts\n",
    "\n",
    "Used in:\n",
    "- LLaMA\n",
    "- Mistral\n",
    "- PaLM\n",
    "\n",
    "\n",
    "RMSNorm is not about expressiveness. It is about efficiency and stability at scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ca00f",
   "metadata": {},
   "source": [
    "### 5.5 Why normalization stabilizes attention and FFNs\n",
    "\n",
    "Two failure modes normalization prevents:\n",
    "\n",
    "1. Attention collapse\n",
    "- QKáµ€ grows too large\n",
    "- Softmax saturates\n",
    "- One token dominates\n",
    "2. FFN blow-up\n",
    "- Expansion layer produces large activations\n",
    "- Residual accumulation amplifies them\n",
    "\n",
    "Normalization ensures:\n",
    "- Attention logits stay in a reasonable range\n",
    "- FFN outputs do not dominate the residual stream\n",
    "- Each layer operates in a similar statistical regime\n",
    "\n",
    "This makes hyperparameters transferable across depths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d6770",
   "metadata": {},
   "source": [
    "### 5.6 Initialization strategies for stability\n",
    "\n",
    "Initialization is tightly coupled with normalization.\n",
    "\n",
    "Key principles:\n",
    "- Linear layers should preserve variance\n",
    "- Residual paths should start near identity\n",
    "- Early training should be conservative\n",
    "\n",
    "Common strategies:\n",
    "- Xavier or Kaiming for dense layers\n",
    "- Small initialization for output projections\n",
    "- Zero or near-zero initialization for some residual branches\n",
    "\n",
    "In GPT-2:\n",
    "- Output projection of attention and FFN initialized with smaller std\n",
    "- This slows early residual accumulation\n",
    "\n",
    "The goal:\n",
    "- Let the model learn depth gradually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c476f79",
   "metadata": {},
   "source": [
    "### 5.7 Residual connection depth scaling\n",
    "\n",
    "As depth increases, residual additions accumulate.\n",
    "\n",
    "If each layer adds a signal of similar magnitude:\n",
    "`||x_L|| â‰ˆ L Â· ||Î”||`\n",
    "\n",
    "This is bad.\n",
    "\n",
    "Solutions:\n",
    "- Scale residual outputs by a constant\n",
    "- Implicit scaling via normalization\n",
    "- Careful initialization\n",
    "\n",
    "GPT-2 style scaling:\n",
    "- Attention and FFN outputs are scaled implicitly via initialization\n",
    "- Residual stream remains stable even at 48+ layers\n",
    "\n",
    "Some modern models explicitly scale residuals by **1/âˆšN.**\n",
    "\n",
    "$$\n",
    "x_{l+1} = x_l + \\frac{1}{\\sqrt{N}} \\, \\text{Sublayer}(x_l)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509a874",
   "metadata": {},
   "source": [
    "### 5.8 TensorFlow: Pre-Norm transformer components (high level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f229d5",
   "metadata": {},
   "source": [
    "LayerNorm block\n",
    "``` py\n",
    "norm = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "y = norm(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = self.add_weight(\n",
    "            shape=(d_model,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True))\n",
    "        return x / (rms + self.epsilon) * self.scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1f02",
   "metadata": {},
   "source": [
    "Pre-Norm pattern inside a block\n",
    "```py\n",
    "x = x + attention(norm1(x))\n",
    "x = x + ffn(norm2(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbc9b1",
   "metadata": {},
   "source": [
    "## 6 The Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7765f43",
   "metadata": {},
   "source": [
    "### 6.1 Decoder block order and why it is this way\n",
    "\n",
    "A decoder block is a transformer layer that takes a sequence of token representations, applies causal self-attention and a feedforward network with residual connections and normalization, and outputs updated representations for next-token prediction.\n",
    "\n",
    "Modern GPT style `decoder block` that uses Pre-Norm:\n",
    "```\n",
    "x â†’ LayerNorm â†’ Self-Attention â†’ Dropout â†’ Residual Add\n",
    "  â†’ LayerNorm â†’ FFN â†’ Dropout â†’ Residual Add\n",
    "```\n",
    "\n",
    "Why this order matters:\n",
    "- Normalization happens before heavy computation\n",
    "- Residual path stays clean\n",
    "- Gradients flow directly through depth\n",
    "- Training remains stable even at 100+ layers\n",
    "\n",
    "Attention always comes before FFN because:\n",
    "- First mix information across tokens\n",
    "- Then transform that information per token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01a178",
   "metadata": {},
   "source": [
    "### 6.2 Shape propagation through the block\n",
    "\n",
    "Let:\n",
    "- batch = B\n",
    "- sequence length = T\n",
    "- model dimension = D\n",
    "- number of heads = H\n",
    "- head dimension = D / H\n",
    "\n",
    "Input:\n",
    "`x: (B, T, D)`\n",
    "\n",
    "After attention:\n",
    "```\n",
    "Q, K, V: (B, H, T, D/H)\n",
    "attention output: (B, T, D)\n",
    "```\n",
    "\n",
    "After FFN:\n",
    "```\n",
    "expanded: (B, T, d_ff)\n",
    "projected back: (B, T, D)\n",
    "```\n",
    "\n",
    "Critical rule: The residual stream shape never changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2831731",
   "metadata": {},
   "source": [
    "### 6.3 Dropout behavior inside the decoder\n",
    "\n",
    "Dropout appears in three places conceptually:\n",
    "1. Attention weights\n",
    "2. Attention output projection\n",
    "3. FFN output\n",
    "\n",
    "Dropout is:\n",
    "- Active only during training\n",
    "- A regularizer, not a core mechanism\n",
    "\n",
    "In large pretraining runs:\n",
    "- Dropout is often small or zero\n",
    "- In fine-tuning: Dropout becomes important again\n",
    "\n",
    "**NOTE:** Always apply dropout before residual addition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a6b7f",
   "metadata": {},
   "source": [
    "### 6.4 Trainable parameter count intuition\n",
    "Per decoder block, roughly:\n",
    "\n",
    "Attention:\n",
    "- QKV projections: 3 Ã— D Ã— D\n",
    "- Output projection: D Ã— D\n",
    "\n",
    "FFN:\n",
    "- Expansion: D Ã— d_ff\n",
    "- Projection: d_ff Ã— D\n",
    "\n",
    "If **d_ff = 4D**:\n",
    "\n",
    "Total per layer is dominated by FFN:\n",
    "`~ 12DÂ² parameters`\n",
    "\n",
    "This is why:\n",
    "- Most parameters live in FFNs\n",
    "- Reducing FFN size dramatically reduces model size\n",
    "- Attention is compute-heavy but parameter-light\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5842a",
   "metadata": {},
   "source": [
    "### 6.5 How to tune hidden sizes\n",
    "\n",
    "Rules of thumb:\n",
    "- d_model: controls representation capacity\n",
    "- num_heads: controls diversity of attention patterns\n",
    "- d_ff: controls nonlinear capacity\n",
    "\n",
    "Common ratios:\n",
    "- d_ff = 4 Ã— d_model for GELU\n",
    "- d_ff â‰ˆ 2.7 Ã— d_model for SwiGLU\n",
    "\n",
    "Increasing depth tends to be more effective than increasing width for language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cb91e",
   "metadata": {},
   "source": [
    "### 6.6 Why decoder-only models work for LLMs\n",
    "\n",
    "Decoder-only transformers:\n",
    "- Predict next token given past tokens\n",
    "- Train with causal self-attention\n",
    "- Match generation exactly at training time\n",
    "\n",
    "Key advantages:\n",
    "- Simpler architecture\n",
    "- No encoder-decoder mismatch\n",
    "- Scales cleanly with data and compute\n",
    "\n",
    "This alignment between training and inference is a major reason GPT style models dominate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b674023",
   "metadata": {},
   "source": [
    "### 6.7 TensorFlow: Full DecoderBlock implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            activation=activation,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, causal_mask=None, training=False):\n",
    "        # Self attention block\n",
    "        h = self.ln1(x)\n",
    "        h = self.attention(\n",
    "            query=h,\n",
    "            value=h,\n",
    "            key=h,\n",
    "            attention_mask=causal_mask,\n",
    "            training=training\n",
    "        )\n",
    "        h = self.dropout(h, training=training)\n",
    "        x = x + h\n",
    "\n",
    "        # Feed forward block\n",
    "        h = self.ln2(x)\n",
    "        h = self.ffn(h, training=training)\n",
    "        h = self.dropout(h, training=training)\n",
    "        x = x + h\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2232d8",
   "metadata": {},
   "source": [
    "Testing with random input\n",
    "\n",
    "**NOTE:** Run the FeedForward Code block in `4.6` before running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d5f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "\n",
    "x = tf.random.normal((batch, seq_len, d_model))\n",
    "\n",
    "causal_mask = tf.linalg.band_part(\n",
    "    tf.ones((seq_len, seq_len)), -1, 0\n",
    ")\n",
    "\n",
    "block = DecoderBlock(\n",
    "    d_model=d_model,\n",
    "    num_heads=4,\n",
    "    d_ff=256\n",
    ")\n",
    "\n",
    "y = block(x, causal_mask=causal_mask, training=True)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5424851",
   "metadata": {},
   "source": [
    "output:\n",
    "```\n",
    "(2, 8, 64)\n",
    "```\n",
    "\n",
    "**If this shape is preserved, the block is correct.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93e1ed",
   "metadata": {},
   "source": [
    "## 7 Putting blocks together into a Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd109883",
   "metadata": {},
   "source": [
    "### 7.1 Stacking N decoder blocks\n",
    "\n",
    "A transformer decoder is simply:\n",
    "- Token embedding\n",
    "- Positional representation\n",
    "- A stack of identical decoder blocks\n",
    "- Final normalization\n",
    "- Output projection to vocabulary\n",
    "\n",
    "There is no recursion, no branching, no hierarchy. Just depth.\n",
    "\n",
    "LLM Structure with `N Decoder Blocks`:\n",
    "```\n",
    "tokens\n",
    "â†’ token embedding\n",
    "â†’ positional embedding\n",
    "â†’ [DecoderBlock Ã— N] # xl+1 = DecoderBlock(xlâ€‹) for l=0â€¦Nâˆ’1 simply, the result from one decoder block is passed to the next one as input\n",
    "â†’ final norm\n",
    "â†’ linear projection (unembedding)\n",
    "â†’ logits\n",
    "```\n",
    "\n",
    "Why stacking works:\n",
    "- Each block refines the same residual stream\n",
    "- Early layers learn local and syntactic features\n",
    "- Middle layers learn structure and relations\n",
    "- Late layers learn semantics and prediction cues\n",
    "\n",
    "Depth increases abstraction, not sequence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e06e4",
   "metadata": {},
   "source": [
    "### 7.2 Why weights are not shared across layers\n",
    "\n",
    "You might ask why not reuse the same decoder block weights N times.\n",
    "\n",
    "Reasons:\n",
    "- Each layer needs to learn a different transformation\n",
    "- Sharing collapses representational hierarchy\n",
    "- Empirically hurts performance\n",
    "- Transformers rely on progressive feature refinement. Weight sharing removes that ladder.\n",
    "\n",
    "The only common sharing is:\n",
    "- Token embedding â†” output projection (tied embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a9e36",
   "metadata": {},
   "source": [
    "### 7.3 How input embeddings flow into the decoder\n",
    "\n",
    "Input tokens are integers. The decoder never sees integers again.\n",
    "\n",
    "Flow:\n",
    "\n",
    "1. Token IDs â†’ embedding vectors\n",
    "2. Embeddings scaled by sqrt(d_model)\n",
    "3. Positional information added or applied\n",
    "4. Result becomes the initial residual stream\n",
    "\n",
    "From here on:\n",
    "- Shape stays (B, T, D)\n",
    "- All blocks operate on the same stream\n",
    "- No block knows about tokens, only vectors\n",
    "\n",
    "This is critical to internalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdcb642",
   "metadata": {},
   "source": [
    "### 7.4 How logits are produced\n",
    "\n",
    "After the final decoder block:\n",
    "- We normalize once more\n",
    "- Project each token vector to vocab size\n",
    "\n",
    "Logits:\n",
    "\n",
    "`(B, T, vocab_size)`\n",
    "\n",
    "These logits represent:\n",
    "- For every position\n",
    "- A probability distribution over the next token\n",
    "\n",
    "Softmax is applied later during loss computation or generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf9cae",
   "metadata": {},
   "source": [
    "### 7.5 TensorFlow: Tiny GPT style model from scratch\n",
    "This is a clean, minimal, but correct GPT style forward pass.\n",
    "\n",
    "Assumptions:\n",
    "- Pre-Norm decoder blocks\n",
    "- Causal self attention\n",
    "- Tied embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TinyGPT(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        num_layers, # of decoder blocks\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, d_model\n",
    "        )\n",
    "\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            max_seq_len, d_model\n",
    "        )\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.blocks = [\n",
    "            DecoderBlock(\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                d_ff=d_ff,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.final_norm = tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-5\n",
    "        )\n",
    "\n",
    "    def call(self, token_ids, training=False):\n",
    "        batch_size, seq_len = tf.shape(token_ids)[0], tf.shape(token_ids)[1]\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = tf.linalg.band_part(\n",
    "            tf.ones((seq_len, seq_len)), -1, 0\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        x = self.token_embedding(token_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(x.shape[-1], tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        x = x + pos_emb\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Decoder blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                causal_mask=causal_mask,\n",
    "                training=training\n",
    "            )\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Tied output projection\n",
    "        logits = tf.einsum(\n",
    "            \"btd,vd->btv\",\n",
    "            x,\n",
    "            self.token_embedding.embeddings\n",
    "        )\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02f4b7",
   "metadata": {},
   "source": [
    "Testing the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_seq_len = 32\n",
    "\n",
    "model = TinyGPT(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=4\n",
    ")\n",
    "\n",
    "dummy_tokens = tf.random.uniform(\n",
    "    (2, 16),\n",
    "    minval=0,\n",
    "    maxval=vocab_size,\n",
    "    dtype=tf.int32\n",
    ")\n",
    "\n",
    "logits = model(dummy_tokens, training=True)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4de76b",
   "metadata": {},
   "source": [
    "Output:\n",
    "`(2, 16, 1000)`\n",
    "\n",
    "this confirms the logits (possible next words) are being generated hence the model is working corectly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ece022c",
   "metadata": {},
   "source": [
    "## 8 Training Objective and Loss for LLMs\n",
    "\n",
    "At a high level, LLM training is brutally simple:\n",
    "- Given tokens up to position t, predict token t+1.\n",
    "\n",
    "Everything else is engineering to make this stable and efficient.\n",
    "\n",
    "see ml notes pg 69-70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611a43b",
   "metadata": {},
   "source": [
    "### 8.1 Next-token prediction objective\n",
    "\n",
    "The model defines:\n",
    "\n",
    "`P(xâ‚œ | xâ‚, xâ‚‚, â€¦, xâ‚œâ‚‹â‚)`\n",
    "\n",
    "Training maximizes the log-likelihood of the dataset under this factorization.\n",
    "\n",
    "Log-likelihood measures how well a model's predicted probability distribution matches the actual observed data, with higher values indicating better fit.\n",
    "\n",
    "Equivalent view:\n",
    "- The model outputs logits for every position.\n",
    "- Each position predicts the next token.\n",
    "- Loss is summed across all tokens in the batch.\n",
    "\n",
    "This aligns training exactly with generation behavior. This alignment is a major reason decoder-only models dominate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653958aa",
   "metadata": {},
   "source": [
    "### 8.2 Cross-entropy loss in detail\n",
    "\n",
    "For a single token prediction:\n",
    "\n",
    "`loss = -log softmax(logits)[true_token]`\n",
    "\n",
    "- `âˆ’logp(true token)` is the loss function itself\n",
    "- `logits` are the raw, unnormalized scores that can be any real number.\n",
    "- `softmax(logits)` gives a probibility distrabution over the logits where each probability is [0,1] and all together they add to 1\n",
    "- [true_token] is to index the probability of the correct token from the probabilities given by softmax\n",
    "\n",
    "Expanded:\n",
    "- Softmax converts logits into probabilities.\n",
    "- Cross-entropy penalizes low probability assigned to the true token.\n",
    "- Loss is high when the model is confident and wrong.\n",
    "\n",
    "Important properties:\n",
    "- Loss is additive across time steps.\n",
    "- Loss is additive across batch elements.\n",
    "- Lower loss corresponds to better next-token prediction.\n",
    "\n",
    "In practice, we never compute softmax explicitly. We use a numerically stable fused op.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86839",
   "metadata": {},
   "source": [
    "### 8.3 Label shifting\n",
    "\n",
    "The model outputs logits for each input token position.\n",
    "\n",
    "But the target is the next token.\n",
    "\n",
    "So we shift:\n",
    "\n",
    "Input tokens:\n",
    "- [BOS, A, B, C]\n",
    "\n",
    "Targets:\n",
    "- [A, B, C, EOS]\n",
    "\n",
    "Implementation rule:\n",
    "- Inputs are tokens[:-1]\n",
    "- Targets are tokens[1:]\n",
    "\n",
    "The model never sees the token it is trying to predict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f88d62",
   "metadata": {},
   "source": [
    "### 8.4 Padding removal in the loss\n",
    "\n",
    "Sequences are padded to form batches.\n",
    "\n",
    "Padding tokens must not contribute to the loss.\n",
    "\n",
    "Mechanism:\n",
    "- Create a mask where target != PAD\n",
    "- Multiply loss by mask\n",
    "- Normalize by number of real tokens\n",
    "\n",
    "If you skip this:\n",
    "- Model learns to predict PAD\n",
    "- Loss statistics become meaningless\n",
    "- Training degrades silently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e41e05",
   "metadata": {},
   "source": [
    "### 8.5 Teacher forcing\n",
    "\n",
    "Teacher forcing means:\n",
    "\n",
    "During training, the model always receives the true previous token.\n",
    "\n",
    "It never feeds its own predictions back during training.\n",
    "\n",
    "Why this is correct:\n",
    "- Objective is conditional likelihood\n",
    "- This gives unbiased gradients\n",
    "- Generation-time mismatch is handled by scale, not architecture\n",
    "\n",
    "All GPT-style models use teacher forcing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e5179",
   "metadata": {},
   "source": [
    "### 8.6 Gradient clipping and why it is essential (ML notes p 123)\n",
    "\n",
    "Transformers can produce large gradients due to:\n",
    "- Attention score spikes\n",
    "- FFN expansion layers\n",
    "- Long sequences\n",
    "\n",
    "Gradient clipping prevents:\n",
    "- Sudden divergence\n",
    "- NaNs during early training\n",
    "- Instability when scaling batch size\n",
    "\n",
    "Standard practice:\n",
    "- Clip by global norm\n",
    "- Typical values: 0.5 to 1.0\n",
    "\n",
    "This is not optional for LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b8554",
   "metadata": {},
   "source": [
    "### 8.7 Mixed precision training\n",
    "\n",
    "Mixed precision uses:\n",
    "- float16 or bfloat16 for most operations\n",
    "- float32 for accumulations and loss\n",
    "\n",
    "Benefits:\n",
    "- Faster training\n",
    "- Lower memory usage\n",
    "- Larger batch sizes\n",
    "\n",
    "Key requirement:\n",
    "- Loss scaling to avoid underflow\n",
    "\n",
    "TensorFlow handles this automatically when configured correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d6abf",
   "metadata": {},
   "source": [
    "\n",
    "#### 8.7.1 Loss Scaling\n",
    "\n",
    "Loss scaling means multiplying the loss by a constant before backpropagation to keep gradients numerically stable.\n",
    "\n",
    "Why it exists:\n",
    "- In mixed precision training, gradients can underflow to zero in float16.\n",
    "- Scaling the loss makes gradients larger so they stay representable.\n",
    "\n",
    "What actually happens:\n",
    "Compute loss\n",
    "Multiply by scale factor **S**\n",
    "Backpropagate\n",
    "Divide gradients by **S**\n",
    "\n",
    "It rescales the math so small gradients do not disappear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cdf09",
   "metadata": {},
   "source": [
    "### 8.8 TensorFlow: Custom autoregressive training loop\n",
    "\n",
    "This is a clean, minimal training loop for a GPT-style model.\n",
    "\n",
    "This loop:\n",
    "- Implements next-token prediction\n",
    "- Handles padding correctly\n",
    "- Uses teacher forcing\n",
    "- Clips gradients\n",
    "- Trains with mixed precision\n",
    "\n",
    "This is the same structure used in real LLM training codebases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a9d1d",
   "metadata": {},
   "source": [
    "#### 8.8.1 Enable mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Using float32 for simplicity (mixed precision can cause issues with @tf.function)\n",
    "mixed_precision.set_global_policy(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040a771",
   "metadata": {},
   "source": [
    "#### 8.8.2 Loss function with padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f636f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, targets, pad_token_id):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "\n",
    "    loss = loss_fn(targets, logits)\n",
    "\n",
    "    mask = tf.cast(targets != pad_token_id, tf.float32)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d7ec2",
   "metadata": {},
   "source": [
    "#### 8.8.3 Custom training step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4376b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, tokens, pad_token_id):\n",
    "    inputs = tokens[:, :-1]\n",
    "    targets = tokens[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(inputs, training=True)\n",
    "        loss = compute_loss(logits, targets, pad_token_id)\n",
    "\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Gradient clipping\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "\n",
    "    optimizer.apply_gradients(\n",
    "        zip(grads, model.trainable_variables)\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc9c11",
   "metadata": {},
   "source": [
    "#### 8.8.4 Optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simple AdamW optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=3e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.95,\n",
    "    weight_decay=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296113a2",
   "metadata": {},
   "source": [
    "#### 8.8.5 Load built-in dataset and create a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use IMDB reviews dataset (built-in with TensorFlow)\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load top 5000 words only\n",
    "vocab_size = 5000\n",
    "max_length = 50\n",
    "\n",
    "(train_texts, _), _ = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Take small subset for demo (100 samples)\n",
    "train_texts = train_texts[:100]\n",
    "\n",
    "# Clip token IDs to be within vocab range (some tokens may be exactly vocab_size)\n",
    "train_texts = [[min(token, vocab_size-1) for token in seq] for seq in train_texts]\n",
    "\n",
    "# Pad sequences\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_texts, \n",
    "    maxlen=max_length,\n",
    "    padding='post',\n",
    "    value=0  # pad token\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(padded_sequences).batch(4)\n",
    "\n",
    "# Simple model for this vocab\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb38d7",
   "metadata": {},
   "source": [
    "#### 8.8.6 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee55d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "max_steps = 25\n",
    "\n",
    "for step, batch_tokens in enumerate(dataset):\n",
    "    if step >= max_steps:\n",
    "        break\n",
    "        \n",
    "    loss = train_step(\n",
    "        model,\n",
    "        optimizer,\n",
    "        batch_tokens,\n",
    "        pad_token_id=0\n",
    "    )\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        tf.print(\"Step:\", step, \"Loss:\", loss)\n",
    "\n",
    "print(f\"\\nTraining complete! Ran {min(step+1, max_steps)} steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d112a3",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "Step: 0 Loss: 8.51775169\n",
    "Step: 5 Loss: 8.51544094\n",
    "Step: 10 Loss: 8.51357555\n",
    "Step: 15 Loss: 8.51210499\n",
    "Step: 20 Loss: 8.50906372\n",
    "\n",
    "Training complete! Ran 25 steps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a65790",
   "metadata": {},
   "source": [
    "## 9 Generation Controls for LLMs \n",
    "\n",
    "NOTE:\n",
    "- Given a sequence of tokens, iteratively predict the next token and append it, until a stopping condition is met.\n",
    "- Everything else (temperature, top-k, top-p, repetition penalty) is a mechanism for controlling diversity and coherence.\n",
    "\n",
    "(ML notes p 94)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f113bb",
   "metadata": {},
   "source": [
    "### 9.1 Greedy decoding\n",
    "\n",
    "Concept:\n",
    "- At each step, pick the token with the highest probability.\n",
    "- `next_token = argmax(P(x_t | x_<t))`\n",
    "\n",
    "Pros:\n",
    "- Deterministic\n",
    "- Fast\n",
    "Cons:\n",
    "- Can be repetitive\n",
    "- Lacks diversity\n",
    "\n",
    "Analogy: Always choose the most obvious next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff5926",
   "metadata": {},
   "source": [
    "### 9.2 Sampling\n",
    "Instead of argmax, sample from the probability distribution:\n",
    "\n",
    "`next_token ~ P(x_t | x_<t)`\n",
    "\n",
    "Pros:\n",
    "- Introduces variety\n",
    "- Can generate creative sequences\n",
    "\n",
    "Cons:\n",
    "- Can produce low-quality or incoherent tokens if probability mass is diffuse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fa74c",
   "metadata": {},
   "source": [
    "### 9.3 Temperature (ml notes p 53)\n",
    "\n",
    "Temperature T scales logits before softmax:\n",
    "\n",
    "`P(x) âˆ exp(logits / T)`\n",
    "\n",
    "- T < 1 â†’ sharper distribution â†’ more deterministic\n",
    "- T = 1 â†’ unmodified\n",
    "- T > 1 â†’ flatter distribution â†’ more randomness\n",
    "\n",
    "Temperature controls risk vs safety in predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec1027",
   "metadata": {},
   "source": [
    "### 9.4 Top-k sampling\n",
    "\n",
    "Top-k limits sampling to the k most probable tokens:\n",
    "1. Sort logits\n",
    "2. Keep top k tokens\n",
    "3. Renormalize probabilities\n",
    "4. Sample\n",
    "\n",
    "Effect:\n",
    "- Prevents rare nonsense tokens from appearing\n",
    "- Controls diversity explicitly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f9e8e",
   "metadata": {},
   "source": [
    "### 9.5 Top-p (nucleus) sampling\n",
    "\n",
    "Top-p keeps the smallest set of tokens whose cumulative probability â‰¥ p:\n",
    "- Example: p = 0.9\n",
    "- Only sample from tokens covering 90% probability mass\n",
    "- Dynamic per step, unlike fixed k\n",
    "\n",
    "Advantage:\n",
    "- Adaptive to uncertainty\n",
    "- Avoids both overly deterministic or overly random generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247dd0e",
   "metadata": {},
   "source": [
    "### 9.6 Repetition penalty\n",
    "\n",
    "- Penalizes tokens that have appeared recently\n",
    "- Multiplies logits of repeated tokens by a factor < 1\n",
    "- Helps prevent loops like â€œthe the theâ€¦â€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef10e7",
   "metadata": {},
   "source": [
    "### 9.7 Length penalty\n",
    "\n",
    "- Penalizes sequences that are too short or too long\n",
    "- Often applied during beam search (less common in standard autoregressive sampling)\n",
    "- Encourages more natural-length output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f4bb7",
   "metadata": {},
   "source": [
    "### 9.8 Stopping conditions\n",
    "\n",
    "- Maximum sequence length\n",
    "- EOS token generation\n",
    "- Custom stop strings\n",
    "- Prevent infinite loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a957c",
   "metadata": {},
   "source": [
    "### 9.9 Key-value cache for speed\n",
    "\n",
    "During generation:\n",
    "- Self-attention recomputes K/V for all previous tokens every step â†’ quadratic cost\n",
    "- Cache previous K/V tensors\n",
    "- Only compute Q for new token\n",
    "- Append new K/V to cache\n",
    "\n",
    "Effect:\n",
    "- Reduces per-step cost from O(TÂ²) â†’ O(T)\n",
    "- Essential for real-time inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614a01a",
   "metadata": {},
   "source": [
    "### 9.10 Generation in tf.function\n",
    "\n",
    "- Wrapping generation in tf.function converts Python loops into graph ops\n",
    "- Greatly improves speed\n",
    "- Important for long sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963502a",
   "metadata": {},
   "source": [
    "### 9.11 TensorFlow: Full autoregressive generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_new_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    repetition_penalty=None,\n",
    "    eos_token_id=None\n",
    "):\n",
    "    input_ids = tf.identity(input_ids)\n",
    "    batch_size = tf.shape(input_ids)[0]\n",
    "    cache = []\n",
    "\n",
    "    for step in tf.range(max_new_tokens):\n",
    "        logits = model(input_ids, training=False)  # shape (B, T, V)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Repetition penalty\n",
    "        if repetition_penalty is not None:\n",
    "            for b in range(batch_size):\n",
    "                for token in input_ids[b].numpy():\n",
    "                    next_token_logits = tf.tensor_scatter_nd_update(\n",
    "                        next_token_logits,\n",
    "                        indices=[[b, token]],\n",
    "                        updates=[next_token_logits[b, token] / repetition_penalty]\n",
    "                    )\n",
    "\n",
    "        # Temperature\n",
    "        next_token_logits /= temperature\n",
    "\n",
    "        # Top-k\n",
    "        if top_k is not None:\n",
    "            values, _ = tf.math.top_k(next_token_logits, k=top_k)\n",
    "            min_values = tf.reduce_min(values, axis=-1, keepdims=True)\n",
    "            next_token_logits = tf.where(\n",
    "                next_token_logits < min_values, -1e10, next_token_logits\n",
    "            )\n",
    "\n",
    "        # Top-p\n",
    "        if top_p is not None:\n",
    "            sorted_logits = tf.sort(next_token_logits, direction='DESCENDING', axis=-1)\n",
    "            sorted_probs = tf.nn.softmax(sorted_logits, axis=-1)\n",
    "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "            mask = cum_probs > top_p\n",
    "            # Set masked logits to -inf\n",
    "            sorted_indices = tf.argsort(next_token_logits, direction='DESCENDING')\n",
    "            scatter_mask = tf.scatter_nd(\n",
    "                tf.expand_dims(sorted_indices, -1),\n",
    "                tf.cast(mask, tf.float32) * -1e10,\n",
    "                tf.shape(next_token_logits)\n",
    "            )\n",
    "            next_token_logits += scatter_mask\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = tf.random.categorical(next_token_logits, 1, dtype=tf.int32)\n",
    "        input_ids = tf.concat([input_ids, next_token], axis=-1)\n",
    "\n",
    "        # Stop if EOS token generated\n",
    "        if eos_token_id is not None:\n",
    "            if tf.reduce_all(tf.reduce_any(next_token == eos_token_id, axis=-1)):\n",
    "                break\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce1ac5",
   "metadata": {},
   "source": [
    "## 10 Scaling and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea84a15",
   "metadata": {},
   "source": [
    "### 10.1 Parameter scaling laws\n",
    "\n",
    "Empirical scaling laws show that loss follows a power law with respect to:\n",
    "- Model parameters\n",
    "- Dataset size\n",
    "- Compute\n",
    "\n",
    "Key insight:\n",
    "- For a fixed compute budget, there is an optimal balance between model size and data size.\n",
    "\n",
    "Practical consequences:\n",
    "- Bigger models underfit if data is too small\n",
    "- Smaller models saturate early if data is abundant\n",
    "- Token count matters more than raw dataset size\n",
    "\n",
    "Rule of thumb:\n",
    "- Train on at least 20 to 30 tokens per parameter\n",
    "- Under that, you are compute rich but data poor\n",
    "\n",
    "This is why modern training focuses on trillions of tokens, not just billions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf6513",
   "metadata": {},
   "source": [
    "### 10.2 Dropout placement\n",
    "\n",
    "Dropout is not used everywhere in transformers.\n",
    "\n",
    "Effective locations:\n",
    "- Attention output projection\n",
    "- FFN output projection\n",
    "- Residual stream after sublayers\n",
    "\n",
    "Usually avoided:\n",
    "- Inside attention softmax\n",
    "- Inside normalization layers\n",
    "\n",
    "Typical values:\n",
    "- 0.1 for small models\n",
    "- 0.0 to 0.05 for very large models\n",
    "\n",
    "Key insight:\n",
    "- Dropout fights co-adaptation, but large datasets already provide strong regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b57899",
   "metadata": {},
   "source": [
    "### 10.3 Weight decay\n",
    "\n",
    "Weight decay penalizes large weights to prevent overfitting.\n",
    "\n",
    "Important distinction:\n",
    "- L2 regularization couples with optimizer\n",
    "- Weight decay is applied directly to weights\n",
    "\n",
    "Modern LLMs use AdamW, not Adam.\n",
    "\n",
    "What to decay:\n",
    "- Dense layer weights\n",
    "- Projection matrices\n",
    "\n",
    "What NOT to decay:\n",
    "- Biases\n",
    "- LayerNorm or RMSNorm parameters\n",
    "- Embedding scale parameters\n",
    "\n",
    "Typical values:\n",
    "- 0.01 to 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8aeaa",
   "metadata": {},
   "source": [
    "### 10.4 Learning rate schedules\n",
    "\n",
    "Learning rate choice dominates training success.\n",
    "\n",
    "Key idea:\n",
    "- Large models are extremely sensitive to early training instability.\n",
    "\n",
    "This is why schedules matter more than raw LR values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617a503",
   "metadata": {},
   "source": [
    "### 10.5 Warmup linear schedules\n",
    "\n",
    "Warmup increases learning rate gradually at the start.\n",
    "\n",
    "Why it matters:\n",
    "- Early gradients are noisy\n",
    "- Large initial updates can destroy representations\n",
    "\n",
    "Common strategy:\n",
    "- Linear warmup for first 1 to 5 percent of total steps\n",
    "- Then decay\n",
    "\n",
    "Warmup is mandatory for transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b688384",
   "metadata": {},
   "source": [
    "### 10.6 Cosine decay\n",
    "\n",
    "After warmup, learning rate decays smoothly:\n",
    "\n",
    "`lr(t) = lr_max * 0.5 * (1 + cos(Ï€t / T))`\n",
    "\n",
    "Benefits:\n",
    "- Smooth convergence\n",
    "- Avoids sudden drops\n",
    "- Works well with long training runs\n",
    "\n",
    "Often combined with:\n",
    "- Warmup + cosine decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc829b",
   "metadata": {},
   "source": [
    "### 10.7 Adam vs AdamW\n",
    "\n",
    "Adam:\n",
    "- Couples L2 regularization with adaptive moments\n",
    "- Poor weight decay behavior\n",
    "\n",
    "AdamW:\n",
    "- Decouples weight decay from gradient updates\n",
    "- Stable at scale\n",
    "- Industry standard for LLMs\n",
    "\n",
    "Typical hyperparameters:\n",
    "- beta1 = 0.9\n",
    "- beta2 = 0.95 or 0.999\n",
    "0 epsilon = 1e-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b99113",
   "metadata": {},
   "source": [
    "### 10.8 Tokenization impact on scale\n",
    "\n",
    "Tokenization affects:\n",
    "- Sequence length\n",
    "- Effective context usage\n",
    "- Compute cost\n",
    "\n",
    "Observations:\n",
    "- Byte-level tokenizers increase sequence length\n",
    "- Subword tokenizers reduce length but increase vocab size\n",
    "- Larger vocab increases embedding and softmax cost\n",
    "\n",
    "Tradeoff:\n",
    "- Smaller vocab â†’ longer sequences\n",
    "- Larger vocab â†’ more parameters\n",
    "\n",
    "This directly impacts:\n",
    "- Memory\n",
    "- Throughput\n",
    "- Training stability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b656a4",
   "metadata": {},
   "source": [
    "### 10.9 Transformer depth and expressivity\n",
    "\n",
    "Depth increases expressivity more than width for a fixed parameter budget.\n",
    "\n",
    "Why:\n",
    "- Each layer refines the residual stream\n",
    "- Attention compounds information across layers\n",
    "- Deeper models build hierarchical abstractions\n",
    "\n",
    "But depth increases:\n",
    "- Gradient instability\n",
    "- Optimization difficulty\n",
    "\n",
    "This is why:\n",
    "- Pre-norm\n",
    "- Residual scaling\n",
    "- Careful initialization are mandatory for deep LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded2838",
   "metadata": {},
   "source": [
    "### 10.10 Putting it together: real vs toy training\n",
    "\n",
    "Toy models:\n",
    "- Few layers\n",
    "- Small datasets\n",
    "- Minimal regularization\n",
    "- Short training runs\n",
    "\n",
    "Real LLMs:\n",
    "- Careful LR schedules\n",
    "- AdamW with selective decay\n",
    "- Minimal dropout\n",
    "- Massive datasets\n",
    "- Gradient clipping\n",
    "- Mixed precision\n",
    "- Continuous evaluation\n",
    "\n",
    "Core insight:\n",
    "- Architecture gets you expressivity. Optimization keeps it alive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8d33f",
   "metadata": {},
   "source": [
    "## 11 Evaluation and Metrics for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e75220",
   "metadata": {},
   "source": [
    "### 11.1 Perplexity\n",
    "\n",
    "Perplexity measures how surprised the model is by the data.\n",
    "\n",
    "Definition:\n",
    "\n",
    "`Perplexity = exp(cross_entropy_loss)`\n",
    "\n",
    "where: exp = e^x\n",
    "\n",
    "Interpretation:\n",
    "- Lower perplexity means better predictive modeling\n",
    "- It is the geometric mean of branching factor per token\n",
    "\n",
    "Intuition:\n",
    "- If your model has perplexity 20, it behaves like it is choosing among 20 plausible next tokens at each step.\n",
    "\n",
    "Important nuances:\n",
    "- Only meaningful when evaluated with the same tokenizer\n",
    "- Sensitive to sequence length\n",
    "- Cannot compare across tokenization schemes\n",
    "\n",
    "Perplexity is the standard metric for language modeling quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b24f1c",
   "metadata": {},
   "source": [
    "### 11.2 Bits per character (BPC)\n",
    "\n",
    "BPC is perplexity measured at the character level.\n",
    "\n",
    "`BPC = cross_entropy_loss / log(2)`\n",
    "\n",
    "Why it exists:\n",
    "- Character tokenization varies widely\n",
    "- BPC normalizes across tokenizers\n",
    "\n",
    "Use cases:\n",
    "- Comparing byte-level models\n",
    "- Evaluating compression-like efficiency\n",
    "- Lower BPC means better compression of language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859e8dd",
   "metadata": {},
   "source": [
    "### 11.3 Loss curves\n",
    "\n",
    "Loss curves show training dynamics, not final quality.\n",
    "\n",
    "What to watch:\n",
    "- Smooth downward trend\n",
    "- No sharp spikes during warmup\n",
    "- Validation loss tracks training loss\n",
    "\n",
    "Red flags:\n",
    "- Training loss decreases but validation loss rises â†’ overfitting\n",
    "- Loss plateaus early â†’ under-capacity or learning rate too low\n",
    "- Sudden spikes â†’ optimizer instability\n",
    "\n",
    "Loss curves are the fastest debugging tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372932a",
   "metadata": {},
   "source": [
    "### 11.4 Out-of-distribution evaluation\n",
    "\n",
    "LLMs fail silently on distribution shifts.\n",
    "\n",
    "OOD examples:\n",
    "- Different domain text\n",
    "- Different language style\n",
    "- Longer context than trained on\n",
    "\n",
    "Symptoms:\n",
    "- Confident but wrong predictions\n",
    "- Repetitive output\n",
    "- Degenerate loops\n",
    "\n",
    "Best practice:\n",
    "- Evaluate on multiple domains\n",
    "- Include longer contexts\n",
    "- Monitor perplexity degradation\n",
    "\n",
    "OOD evaluation is more important than in-distribution scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ea5b8",
   "metadata": {},
   "source": [
    "### 11.5 Token-level accuracy\n",
    "\n",
    "Definition:\n",
    "- Fraction of tokens where argmax prediction equals ground truth\n",
    "\n",
    "Why it is misleading:\n",
    "- Language is multimodal\n",
    "- Multiple valid next tokens exist\n",
    "\n",
    "Use it for:\n",
    "- Debugging alignment\n",
    "- Verifying pipeline correctness\n",
    "\n",
    "Do not use it as a quality metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beec198",
   "metadata": {},
   "source": [
    "### 11.6 Sampling-based evaluation\n",
    "\n",
    "Quantitative metrics miss qualitative failure modes.\n",
    "\n",
    "Sampling evaluation includes:\n",
    "- Prompted generation\n",
    "- Long-context continuation\n",
    "- Style consistency tests\n",
    "\n",
    "What to check:\n",
    "- Coherence over long spans\n",
    "- Repetition behavior\n",
    "- Sensitivity to temperature\n",
    "- Robustness to small prompt changes\n",
    "\n",
    "This is where most issues surface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aafa4",
   "metadata": {},
   "source": [
    "### 11.7 Practical evaluation workflow\n",
    "\n",
    "A strong evaluation loop includes:\n",
    "1. Perplexity on held-out data\n",
    "2. Loss curve inspection\n",
    "3. OOD perplexity tests\n",
    "4. Sampled generations at multiple temperatures\n",
    "5. Regression tests on known prompts\n",
    "\n",
    "Key insight:\n",
    "- No single metric correlates perfectly with usefulness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d85512",
   "metadata": {},
   "source": [
    "### 11.8 When metrics lie\n",
    "\n",
    "Situations where metrics mislead:\n",
    "- Over-optimized perplexity harms creativity\n",
    "- Tokenization differences skew comparisons\n",
    "- Short context evaluation hides long-range failures\n",
    "\n",
    "This is why modern LLM evaluation mixes:\n",
    "- Automated metrics\n",
    "- Human judgment\n",
    "- Task-based benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a41ae",
   "metadata": {},
   "source": [
    "### 11.9 End-to-end evaluation of a pretrained GPT-2 (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------\n",
    "# Load pretrained tokenizer and GPT-2 model\n",
    "# --------------------------------------------\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\n",
    "    \"gpt2_base_en\"\n",
    ")\n",
    "\n",
    "model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
    "    \"gpt2_base_en\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Prepare evaluation text (in-distribution)\n",
    "# --------------------------------------------\n",
    "texts = [\n",
    "    \"Deep learning models rely on gradient based optimization.\",\n",
    "    \"Transformers scale well with data and compute.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Neural networks can fail silently on out of distribution data.\"\n",
    "]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Simpler approach: Evaluate each text independently\n",
    "# --------------------------------------------\n",
    "print(\"=== In-Distribution Evaluation ===\")\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction=\"none\"\n",
    ")\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    # Tokenize single text\n",
    "    tokens = tokenizer(text)\n",
    "    tokens_tensor = tf.convert_to_tensor([tokens], dtype=tf.int32)\n",
    "    \n",
    "    # Label shifting for next-token prediction\n",
    "    input_ids = tokens_tensor[:, :-1]\n",
    "    labels = tokens_tensor[:, 1:]\n",
    "    \n",
    "    # Create padding mask (all 1s since no padding in single sequence)\n",
    "    padding_mask = tf.ones_like(input_ids)\n",
    "    \n",
    "    # Forward pass with token_ids and padding_mask\n",
    "    logits = model({\"token_ids\": input_ids, \"padding_mask\": padding_mask}, training=False)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_per_token = loss_fn(labels, logits)\n",
    "    mean_loss = tf.reduce_mean(loss_per_token)\n",
    "    perplexity = tf.exp(mean_loss)\n",
    "    \n",
    "    # Token accuracy (debugging metric) - cast to same dtype\n",
    "    pred_tokens = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "    token_accuracy = tf.reduce_mean(\n",
    "        tf.cast(pred_tokens == labels, tf.float32)\n",
    "    )\n",
    "    \n",
    "    losses.append(mean_loss.numpy())\n",
    "    accuracies.append(token_accuracy.numpy())\n",
    "    \n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
    "    print(f\"  Loss: {mean_loss.numpy():.4f}\")\n",
    "    print(f\"  Perplexity: {perplexity.numpy():.4f}\")\n",
    "    print(f\"  Token Accuracy: {token_accuracy.numpy():.4f}\")\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"\\n=== Overall In-Distribution Statistics ===\")\n",
    "print(f\"Mean Loss: {np.mean(losses):.4f}\")\n",
    "print(f\"Mean Perplexity: {np.exp(np.mean(losses)):.4f}\")\n",
    "print(f\"Mean Token Accuracy: {np.mean(accuracies):.4f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Sampling-based evaluation (generation)\n",
    "# --------------------------------------------\n",
    "print(\"\\n=== Generation Evaluation ===\")\n",
    "print(\"(Skipped - generation may fail on some TensorFlow/Metal configurations)\")\n",
    "print(\"The model.generate() method may require additional backend configuration.\")\n",
    "\n",
    "# Alternative: Show next-token prediction for a prompt\n",
    "prompt = \"Large language models are\"\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "tokens = tokenizer(prompt)\n",
    "tokens_tensor = tf.convert_to_tensor([tokens], dtype=tf.int32)\n",
    "padding_mask = tf.ones_like(tokens_tensor)\n",
    "\n",
    "logits = model({\"token_ids\": tokens_tensor, \"padding_mask\": padding_mask}, training=False)\n",
    "next_token_logits = logits[0, -1, :]  # Get last token's predictions\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_k = 5\n",
    "top_indices = tf.argsort(next_token_logits, direction='DESCENDING')[:top_k]\n",
    "print(f\"\\nTop {top_k} next token predictions:\")\n",
    "for i, idx in enumerate(top_indices.numpy()):\n",
    "    token = tokenizer.detokenize([idx])\n",
    "    prob = tf.nn.softmax(next_token_logits)[idx].numpy()\n",
    "    print(f\"  {i+1}. Token ID {idx}: '{token}' (prob: {prob:.4f})\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# OOD evaluation (distribution shift)\n",
    "# --------------------------------------------\n",
    "print(\"\\n=== Out-of-Distribution Evaluation ===\")\n",
    "ood_texts = [\n",
    "    \"The eigenvalues of a Hermitian operator are real.\",\n",
    "    \"Quantum field theory describes particle interactions.\",\n",
    "    \"In category theory, a functor maps objects to objects.\"\n",
    "]\n",
    "\n",
    "ood_losses = []\n",
    "ood_perplexities = []\n",
    "\n",
    "for i, text in enumerate(ood_texts):\n",
    "    tokens = tokenizer(text)\n",
    "    tokens_tensor = tf.convert_to_tensor([tokens], dtype=tf.int32)\n",
    "    \n",
    "    input_ids = tokens_tensor[:, :-1]\n",
    "    labels = tokens_tensor[:, 1:]\n",
    "    padding_mask = tf.ones_like(input_ids)\n",
    "    \n",
    "    logits = model({\"token_ids\": input_ids, \"padding_mask\": padding_mask}, training=False)\n",
    "    loss_per_token = loss_fn(labels, logits)\n",
    "    mean_loss = tf.reduce_mean(loss_per_token)\n",
    "    perplexity = tf.exp(mean_loss)\n",
    "    \n",
    "    ood_losses.append(mean_loss.numpy())\n",
    "    ood_perplexities.append(perplexity.numpy())\n",
    "    \n",
    "    print(f\"\\nOOD Text {i+1}: {text[:50]}...\")\n",
    "    print(f\"  Loss: {mean_loss.numpy():.4f}\")\n",
    "    print(f\"  Perplexity: {perplexity.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\n=== Overall OOD Statistics ===\")\n",
    "print(f\"Mean OOD Loss: {np.mean(ood_losses):.4f}\")\n",
    "print(f\"Mean OOD Perplexity: {np.mean(ood_perplexities):.4f}\")\n",
    "print(f\"\\n=== Comparison ===\")\n",
    "print(f\"In-dist Perplexity: {np.exp(np.mean(losses)):.4f}\")\n",
    "print(f\"OOD Perplexity: {np.mean(ood_perplexities):.4f}\")\n",
    "print(f\"Perplexity Ratio (OOD/In-dist): {np.mean(ood_perplexities) / np.exp(np.mean(losses)):.2f}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Key Observations:\n",
    "# - OOD perplexity is higher, indicating distribution shift\n",
    "# - Token accuracy is low (not a meaningful quality metric)\n",
    "# - Perplexity quantifies model uncertainty\n",
    "# - Lower perplexity = better fit to the data distribution\n",
    "# --------------------------------------------\n",
    "\n",
    "# --------------------------------------------\n",
    "# Notes:\n",
    "# - Perplexity is tokenizer-dependent\n",
    "# - Token accuracy is not a quality metric\n",
    "# - Higher OOD perplexity indicates distribution shift\n",
    "# - Need to ensure consistent dtypes (int32) for comparisons\n",
    "# --------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cd9c8",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "=== In-Distribution Evaluation ===\n",
    "\n",
    "Text 1: Deep learning models rely on gradient based optimi...\n",
    "  Loss: 5.5754\n",
    "  Perplexity: 263.8670\n",
    "  Token Accuracy: 0.1250\n",
    "\n",
    "Text 2: Transformers scale well with data and compute....\n",
    "  Loss: 5.4709\n",
    "  Perplexity: 237.6756\n",
    "  Token Accuracy: 0.1250\n",
    "\n",
    "Text 3: The capital of France is Paris....\n",
    "  Loss: 3.7983\n",
    "  Perplexity: 44.6244\n",
    "  Token Accuracy: 0.1667\n",
    "\n",
    "Text 4: Neural networks can fail silently on out of distri...\n",
    "  Loss: 5.9228\n",
    "  Perplexity: 373.4550\n",
    "  Token Accuracy: 0.0909\n",
    "\n",
    "=== Overall In-Distribution Statistics ===\n",
    "Mean Loss: 5.1919\n",
    "Mean Perplexity: 179.8023\n",
    "Mean Token Accuracy: 0.1269\n",
    "\n",
    "=== Generation Evaluation ===\n",
    "(Skipped - generation may fail on some TensorFlow/Metal configurations)\n",
    "The model.generate() method may require additional backend configuration.\n",
    "\n",
    "Prompt: 'Large language models are'\n",
    "\n",
    "Top 5 next token predictions:\n",
    "  1. Token ID 407: ' not' (prob: 0.0475)\n",
    "  2. Token ID 973: ' used' (prob: 0.0433)\n",
    "  3. Token ID 635: ' also' (prob: 0.0399)\n",
    "  4. Token ID 1695: ' available' (prob: 0.0293)\n",
    "  5. Token ID 1690: ' often' (prob: 0.0268)\n",
    "\n",
    "=== Out-of-Distribution Evaluation ===\n",
    "\n",
    "OOD Text 1: The eigenvalues of a Hermitian operator are real....\n",
    "  Loss: 4.8738\n",
    "  Perplexity: 130.8193\n",
    "\n",
    "OOD Text 2: Quantum field theory describes particle interactio...\n",
    "  Loss: 4.6373\n",
    "  Perplexity: 103.2607\n",
    "\n",
    "OOD Text 3: In category theory, a functor maps objects to obje...\n",
    "  Loss: 4.7473\n",
    "  Perplexity: 115.2712\n",
    "\n",
    "=== Overall OOD Statistics ===\n",
    "Mean OOD Loss: 4.7528\n",
    "Mean OOD Perplexity: 116.4504\n",
    "\n",
    "=== Comparison ===\n",
    "In-dist Perplexity: 179.8023\n",
    "OOD Perplexity: 116.4504\n",
    "Perplexity Ratio (OOD/In-dist): 0.65\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb4c9",
   "metadata": {},
   "source": [
    "## 12. Final Integration: Building a Tiny LLM End-to-End\n",
    "\n",
    "**Goal**: Synthesize all concepts from this handbook into one complete pipeline - from raw text to a trained transformer that generates text.\n",
    "\n",
    "**This is conceptual** - not a full project, but shows how all pieces fit together. You can later attempt something like NanoGPT with full understanding.\n",
    "\n",
    "**Pipeline Overview**:\n",
    "1. Load dataset and tokenize\n",
    "2. Create sequences for next-token prediction\n",
    "3. Serialize to TFRecord for efficient training\n",
    "4. Build tf.data pipeline with optimizations\n",
    "5. Build a tiny transformer model\n",
    "6. Train with custom loop (explicit gradient steps)\n",
    "7. Generate text with the trained model\n",
    "8. Evaluate perplexity\n",
    "\n",
    "**Why This Matters**: After this section, you'll understand the full stack of LLM training - data prep, model architecture, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142c41c",
   "metadata": {},
   "source": [
    "### 12.1 Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration for our tiny LLM\n",
    "CONFIG = {\n",
    "    'vocab_size': 5000,      # Small vocabulary for demo\n",
    "    'seq_length': 64,        # Context window\n",
    "    'd_model': 128,          # Embedding dimension\n",
    "    'num_heads': 4,          # Attention heads\n",
    "    'num_layers': 2,         # Transformer blocks\n",
    "    'dff': 512,              # Feed-forward dimension\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c596a",
   "metadata": {},
   "source": [
    "### 12.2 Step 1: Load Dataset and Tokenize\n",
    "\n",
    "We'll use a simple text corpus (or you can use a small dataset like tiny shakespeare).\n",
    "\n",
    "**Concepts Applied**:\n",
    "- Character or subword tokenization\n",
    "- Building vocabulary\n",
    "- Converting text to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loading a text dataset (in practice, load from file)\n",
    "sample_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence. It focuses on building systems \n",
    "that learn from data. Deep learning uses neural networks with many layers. Transformers \n",
    "revolutionized natural language processing. Attention mechanisms allow models to focus on \n",
    "relevant parts of the input. Large language models are trained on massive text corpora.\n",
    "Tokenization breaks text into manageable units. Embeddings represent tokens as vectors.\n",
    "Positional encodings add sequence information. Self-attention computes relationships between tokens.\n",
    "\"\"\" * 100  # Repeat to get more data\n",
    "\n",
    "# Simple word-level tokenization (for demo; use BPE/WordPiece in practice)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=CONFIG['vocab_size'], oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts([sample_text])\n",
    "\n",
    "# Convert text to sequences\n",
    "token_ids = tokenizer.texts_to_sequences([sample_text])[0]\n",
    "\n",
    "print(f\"Original text length: {len(sample_text)} characters\")\n",
    "print(f\"Token sequence length: {len(token_ids)} tokens\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)} unique tokens\")\n",
    "print(f\"Sample tokens: {token_ids[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c919263",
   "metadata": {},
   "source": [
    "### 12.3 Step 2: Create Sequences for Next-Token Prediction\n",
    "\n",
    "**Language Modeling Task**: Given tokens `[t1, t2, ..., tn]`, predict `t(n+1)`\n",
    "\n",
    "We create overlapping windows:\n",
    "- Input: `[t1, t2, ..., t64]` \n",
    "- Target: `[t2, t3, ..., t65]` (shifted by 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(token_ids, seq_length):\n",
    "    \"\"\"\n",
    "    Create input-target pairs for next-token prediction.\n",
    "    \n",
    "    Input:  [0, 1, 2, 3, 4, 5]  with seq_length=3\n",
    "    Creates:\n",
    "        Input: [[0, 1, 2], [1, 2, 3], [2, 3, 4]]\n",
    "        Target: [[1, 2, 3], [2, 3, 4], [3, 4, 5]]\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(len(token_ids) - seq_length):\n",
    "        input_seq = token_ids[i:i + seq_length]\n",
    "        target_seq = token_ids[i + 1:i + seq_length + 1]\n",
    "        sequences.append((input_seq, target_seq))\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Create training sequences\n",
    "sequences = create_sequences(token_ids, CONFIG['seq_length'])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "input_seqs = np.array([seq[0] for seq in sequences], dtype=np.int32)\n",
    "target_seqs = np.array([seq[1] for seq in sequences], dtype=np.int32)\n",
    "\n",
    "print(f\"Created {len(sequences)} training examples\")\n",
    "print(f\"Input shape: {input_seqs.shape}\")\n",
    "print(f\"Target shape: {target_seqs.shape}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Input:  {input_seqs[0][:10]}\")\n",
    "print(f\"  Target: {target_seqs[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d84a74",
   "metadata": {},
   "source": [
    "### 12.4 Step 3: Create TFRecord for Efficient Training\n",
    "\n",
    "**Why TFRecord?**\n",
    "- Efficient storage and loading\n",
    "- Better performance with large datasets\n",
    "- Integrates seamlessly with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(input_seq, target_seq):\n",
    "    \"\"\"Convert numpy arrays to TFRecord format.\"\"\"\n",
    "    feature = {\n",
    "        'input': tf.train.Feature(int64_list=tf.train.Int64List(value=input_seq)),\n",
    "        'target': tf.train.Feature(int64_list=tf.train.Int64List(value=target_seq))\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "# Write to TFRecord\n",
    "tfrecord_path = '/tmp/tiny_llm_data.tfrecord'\n",
    "with tf.io.TFRecordWriter(tfrecord_path) as writer:\n",
    "    for input_seq, target_seq in zip(input_seqs, target_seqs):\n",
    "        serialized = serialize_example(input_seq, target_seq)\n",
    "        writer.write(serialized)\n",
    "\n",
    "print(f\"Wrote {len(sequences)} examples to {tfrecord_path}\")\n",
    "print(f\"File size: {os.path.getsize(tfrecord_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b5908",
   "metadata": {},
   "source": [
    "### 12.5 Step 4: Build tf.data Pipeline\n",
    "\n",
    "**Optimizations Applied**:\n",
    "- Parse TFRecord\n",
    "- Shuffle for better training\n",
    "- Batch efficiently\n",
    "- Prefetch to overlap data loading with training\n",
    "- Cache for repeated epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(serialized_example):\n",
    "    \"\"\"Parse a single TFRecord example.\"\"\"\n",
    "    feature_description = {\n",
    "        'input': tf.io.FixedLenFeature([CONFIG['seq_length']], tf.int64),\n",
    "        'target': tf.io.FixedLenFeature([CONFIG['seq_length']], tf.int64)\n",
    "    }\n",
    "    parsed = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    return tf.cast(parsed['input'], tf.int32), tf.cast(parsed['target'], tf.int32)\n",
    "\n",
    "# Build optimized pipeline\n",
    "def create_dataset(tfrecord_path, batch_size, shuffle_buffer=10000):\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
    "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(tfrecord_path, CONFIG['batch_size'])\n",
    "\n",
    "# Verify dataset\n",
    "for inputs, targets in train_dataset.take(1):\n",
    "    print(f\"Batch input shape: {inputs.shape}\")\n",
    "    print(f\"Batch target shape: {targets.shape}\")\n",
    "    print(f\"Input sample: {inputs[0, :10]}\")\n",
    "    print(f\"Target sample: {targets[0, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95231dc",
   "metadata": {},
   "source": [
    "### 12.6 Step 5: Build Tiny Transformer Model\n",
    "\n",
    "**Architecture**:\n",
    "- Token + Positional Embeddings\n",
    "- 2 Transformer Decoder Blocks (Multi-Head Attention + FFN)\n",
    "- Causal masking (can't attend to future tokens)\n",
    "- Output layer: project to vocabulary logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Single transformer decoder block with causal attention.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model // num_heads\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # Causal mask: position i can only attend to positions <= i\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        \n",
    "        # Self-attention with causal mask\n",
    "        attn_output = self.attention(x, x, attention_mask=causal_mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TinyLLM(tf.keras.Model):\n",
    "    \"\"\"Tiny transformer language model.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.token_embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, training=training)\n",
    "        \n",
    "        logits = self.output_layer(x)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# Build model\n",
    "model = TinyLLM(\n",
    "    vocab_size=CONFIG['vocab_size'],\n",
    "    d_model=CONFIG['d_model'],\n",
    "    num_heads=CONFIG['num_heads'],\n",
    "    num_layers=CONFIG['num_layers'],\n",
    "    dff=CONFIG['dff'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.build(input_shape=(None, CONFIG['seq_length']))\n",
    "print(model.summary())\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30097a03",
   "metadata": {},
   "source": [
    "### 12.7 Step 6: Train with Custom Loop\n",
    "\n",
    "**Why Custom Training Loop?**\n",
    "- Full control over gradient computation\n",
    "- Easier debugging\n",
    "- Custom metrics and logging\n",
    "- Understanding what happens under the hood\n",
    "\n",
    "**Process**:\n",
    "1. Forward pass â†’ compute logits\n",
    "2. Calculate loss (sparse categorical cross-entropy)\n",
    "3. Backward pass â†’ compute gradients\n",
    "4. Update weights with optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb521d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for training\n",
    "optimizer = tf.keras.optimizers.Adam(CONFIG['learning_rate'])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    \"\"\"Single training step with gradient tape.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        logits = model(inputs, training=True)\n",
    "        \n",
    "        # Calculate loss (averaged over sequence and batch)\n",
    "        loss = loss_fn(targets, logits)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, logits)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reset metrics (use reset_state for newer Keras versions)\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    \n",
    "    # Train on batches\n",
    "    for step, (inputs, targets) in enumerate(train_dataset):\n",
    "        loss = train_step(inputs, targets)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']} completed in {epoch_time:.2f}s\")\n",
    "    print(f\"  Loss: {train_loss.result():.4f}\")\n",
    "    print(f\"  Accuracy: {train_accuracy.result():.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd1344",
   "metadata": {},
   "source": [
    "### 12.8 Step 7: Generate Text\n",
    "\n",
    "**Autoregressive Generation**:\n",
    "1. Start with a prompt (seed tokens)\n",
    "2. Model predicts next token\n",
    "3. Append predicted token to sequence\n",
    "4. Repeat\n",
    "\n",
    "**Sampling Strategies**:\n",
    "- Greedy: Always pick highest probability token\n",
    "- Top-k: Sample from k most likely tokens\n",
    "- Temperature: Control randomness (high temp = more random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=10):\n",
    "    \"\"\"\n",
    "    Generate text autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_length: Number of tokens to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_k: Sample from top k tokens\n",
    "    \"\"\"\n",
    "    # Tokenize prompt\n",
    "    token_ids = tokenizer.texts_to_sequences([prompt])[0]\n",
    "    \n",
    "    # Ensure we have at least some tokens\n",
    "    if len(token_ids) == 0:\n",
    "        token_ids = [1]  # Start with some token\n",
    "    \n",
    "    generated = token_ids.copy()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Take last seq_length tokens as context\n",
    "        context = generated[-CONFIG['seq_length']:]\n",
    "        context = tf.constant([context])\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(context[0]) < CONFIG['seq_length']:\n",
    "            padding = tf.zeros((1, CONFIG['seq_length'] - len(context[0])), dtype=tf.int32)\n",
    "            context = tf.concat([padding, context], axis=1)\n",
    "        \n",
    "        # Predict next token\n",
    "        logits = model(context, training=False)\n",
    "        logits = logits[0, -1, :] / temperature  # Last position logits with temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        top_k_logits, top_k_indices = tf.nn.top_k(logits, k=top_k)\n",
    "        top_k_probs = tf.nn.softmax(top_k_logits)\n",
    "        \n",
    "        # Sample from top-k\n",
    "        next_token_idx = tf.random.categorical([top_k_probs], num_samples=1)[0, 0]\n",
    "        next_token = top_k_indices[next_token_idx].numpy()\n",
    "        \n",
    "        generated.append(int(next_token))\n",
    "    \n",
    "    # Decode back to text\n",
    "    # Note: tokenizer.sequences_to_texts expects list of sequences\n",
    "    generated_text = tokenizer.sequences_to_texts([generated])[0]\n",
    "    return generated_text\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"machine learning\",\n",
    "    \"deep learning uses\",\n",
    "    \"attention mechanisms\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT GENERATION EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Generate with different temperatures\n",
    "    for temp in [0.5, 1.0, 1.5]:\n",
    "        generated = generate_text(model, tokenizer, prompt, max_length=30, temperature=temp, top_k=10)\n",
    "        print(f\"[Temperature={temp}]: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c432b7",
   "metadata": {},
   "source": [
    "Output Example:\n",
    "\n",
    "```\n",
    "Prompt: 'machine learning'\n",
    "------------------------------------------------------------\n",
    "[Temperature=0.5]: machine learning is on computes self building text models tokens positional encodings networks information many layers into tokens attention layers transformers allow focus tokens attention learn encodings the large into data models\n",
    "[Temperature=1.0]: machine learning is self learning subset information focuses are add positional tokenization positional tokenization breaks intelligence large into of positional breaks systems that learn from tokens layers to natural into language processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42b6ab",
   "metadata": {},
   "source": [
    "### 12.9 Step 8: Evaluate Perplexity\n",
    "\n",
    "**Perplexity**: Measures how well the model predicts the test data\n",
    "- Lower perplexity = better model\n",
    "- Formula: `perplexity = exp(average_loss)`\n",
    "- Intuition: \"How surprised is the model by the actual next token?\"\n",
    "\n",
    "**Why It Matters**:\n",
    "- Standard metric for language models\n",
    "- Comparable across different models\n",
    "- Reflects model uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359103d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, dataset):\n",
    "    \"\"\"\n",
    "    Calculate perplexity on a dataset.\n",
    "    \n",
    "    Perplexity = exp(average cross-entropy loss)\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for inputs, targets in dataset:\n",
    "        logits = model(inputs, training=False)\n",
    "        loss = loss_fn(targets, logits)\n",
    "        total_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Calculate perplexity on training data (in practice, use separate validation set)\n",
    "perplexity, avg_loss = calculate_perplexity(model, train_dataset)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"  - The model is approximately {perplexity:.2f}-ways 'confused' on average\")\n",
    "print(f\"  - Lower perplexity indicates better predictions\")\n",
    "print(f\"  - For reference:\")\n",
    "print(f\"    * Random guessing with vocab_size={CONFIG['vocab_size']} â†’ perplexity â‰ˆ {CONFIG['vocab_size']}\")\n",
    "print(f\"    * Perfect prediction â†’ perplexity = 1.0\")\n",
    "print(f\"    * GPT-3 on validation set â†’ perplexity â‰ˆ 20-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de863c",
   "metadata": {},
   "source": [
    "### 12.10 What We've Accomplished\n",
    "\n",
    "âœ… **Complete LLM Pipeline**:\n",
    "1. **Data Prep**: Tokenized text â†’ sequences â†’ TFRecord\n",
    "2. **Data Pipeline**: Efficient tf.data with batching, shuffling, prefetching\n",
    "3. **Model**: Transformer with embeddings, positional encoding, multi-head attention\n",
    "4. **Training**: Custom loop with explicit gradient computation\n",
    "5. **Inference**: Autoregressive text generation with sampling strategies\n",
    "6. **Evaluation**: Perplexity metric for model quality\n",
    "\n",
    "**Key Concepts Mastered**:\n",
    "- End-to-end flow from raw data to trained model\n",
    "- How tokenization affects model vocabulary\n",
    "- Sequence creation for causal language modeling\n",
    "- Efficient data loading with TFRecord + tf.data\n",
    "- Transformer architecture components working together\n",
    "- Custom training loop mechanics (forward, backward, optimize)\n",
    "- Generation strategies (temperature, top-k sampling)\n",
    "- Model evaluation with perplexity\n",
    "\n",
    "### **Next Steps to NanoGPT Level**\n",
    "\n",
    "Now that you understand the full pipeline conceptually, you can:\n",
    "\n",
    "1. **Scale Up**: \n",
    "   - Use larger datasets (e.g., OpenWebText, Wikipedia)\n",
    "   - Increase model size (more layers, larger d_model)\n",
    "   - Train longer with more compute\n",
    "\n",
    "2. **Better Tokenization**:\n",
    "   - Implement BPE (Byte-Pair Encoding)\n",
    "   - Use SentencePiece or tiktoken\n",
    "   - Optimize vocabulary size\n",
    "\n",
    "3. **Advanced Training**:\n",
    "   - Learning rate scheduling (warmup + decay)\n",
    "   - Gradient clipping for stability\n",
    "   - Mixed precision training (FP16)\n",
    "   - Gradient accumulation for larger batches\n",
    "\n",
    "4. **Model Improvements**:\n",
    "   - Add more layers and parameters\n",
    "   - Experiment with different attention patterns\n",
    "   - Implement techniques like Flash Attention\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Multiple benchmarks (LAMBADA, HellaSwag, etc.)\n",
    "   - Human evaluation\n",
    "   - Downstream task fine-tuning\n",
    "\n",
    "**You're now equipped to understand and build projects like NanoGPT!** ðŸš€\n",
    "\n",
    "The key insight: It's all about moving tensors through transformations:\n",
    "```\n",
    "Text â†’ Tokens â†’ Embeddings â†’ Attention â†’ FFN â†’ Logits â†’ Predictions â†’ Text\n",
    "```\n",
    "\n",
    "Every component serves a purpose in this chain, and you now understand how they fit together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
