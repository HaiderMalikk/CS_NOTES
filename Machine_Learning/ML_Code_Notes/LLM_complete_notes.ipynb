{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbd61ec",
   "metadata": {},
   "source": [
    "# Complete LLM Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f5fca",
   "metadata": {},
   "source": [
    "## Sections \n",
    "\n",
    "### 1.1 Tokenization, vocab, sequence formatting\n",
    "\n",
    "1. Byte level tokenization vs subword (BPE, WordPiece, SentencePiece).\n",
    "2. How IDs map to tokens and how vocab size impacts model size.\n",
    "Padding, masking, and attention masks for autoregressive tasks.\n",
    "4. Special tokens: BOS, EOS, PAD, UNK.\n",
    "5. Sequence packing strategies (contiguous examples in one long stream).\n",
    "6. Sliding window chunking: critical for LLM training.\n",
    "TensorFlow specific:\n",
    "â€¢ Use keras_nlp tokenizers\n",
    "â€¢ Understand how to batch variable length sequences\n",
    "â€¢ Learn how tf.data handles ragged tensors if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras keras_nlp matplotlib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5011d5",
   "metadata": {},
   "source": [
    "## 1 Tokenization, vocab, sequence formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36007de8",
   "metadata": {},
   "source": [
    "### 1.1 Byte level words vs Subword tokenization \n",
    "\n",
    "**Why**: Transformers cannot process raw text, text must be converted into numbers. The way we break text into tokens affects efficency, generalization and memory usage\n",
    "\n",
    "#### 1.1.1 Byte level tokenization\n",
    "\n",
    "- works at the byte level (0-255)\n",
    "- Real world usage: GPT-2 uses byte pair encoding (BPE) at byte level\n",
    "- Pros:\n",
    "  - Handels any charecter, any language, emojis, symbol\n",
    "  - no OOV (out of vocab) tokens\n",
    "- Cons:\n",
    "  - Toekn sequences can be longer -> means more compute \n",
    "- Example: \"hello ðŸ‘‹\" â€“> [104, 101, 108, 108, 111, 32, 240, 159, 145, 139]\n",
    "\n",
    "#### 1.1.2 Subword tokenization (BPE, WordPeice, SentencePiece)\n",
    "\n",
    "-  Breaks Text into frequent subwords insted of characters or words. \n",
    "-  Example: \n",
    "   -  \"unhappiness\" -> [\"un\", \"happi\", \"ness\"]\n",
    "- Pros: \n",
    "  - Shorter sequences than byte\n",
    "  - Can handle rare words via subword decomposition (breaking unknown words into known smaller parts)\n",
    "- Cons:\n",
    "  - some complexity in building vocab and handling edge cases\n",
    "  \n",
    "**NOTE:** LLM's often use subword BPE (BPE applied at the subword level) it iteratively merges the most frequent character or subword pairs to build a vocabulary, balancing between character-level and word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce1ea",
   "metadata": {},
   "source": [
    "### 1.2 Token Ids and Vocabulary Size\n",
    "- After Tokenization, each token is mapped to a integer ID using a vocabulary\n",
    "- Vocabulary size (V) is very important\n",
    "  - Larger V -> model must have a bigger embedding matrix (page 50 in written notes) -> more parameters (hence a larger model)\n",
    "  - Smaller V -> more subword splitting (words broken into more pieces) -> longer sequences -> slower training (but smaller model size)\n",
    "- Typical LLM vocab sizes: 30K-100K for english models \n",
    "- Example: In TensorFlow, keras_nlp.tokenizers handles both mapping tokens â†’ IDs and IDs â†’ tokens.\n",
    "\n",
    "``` py\n",
    "from keras_nlp.tokenizers import BytePairTokenizer\n",
    "\n",
    "tokenizer = BytePairTokenizer(vocabulary=[\"hello\", \"world\", \"un\", \"happi\", \"ness\", \"<PAD>\", \"<BOS>\", \"<EOS>\"])\n",
    "tokens = tokenizer.tokenize([\"hello world\", \"unhappiness\"])\n",
    "token_ids = tokenizer(tokens)\n",
    "print(token_ids)\n",
    "\n",
    "```\n",
    "\n",
    "**How Keras NLP Tokenizers Handle Token â†” ID Mapping** Under the hood, Keras NLP tokenizers maintain two key data structures (`token_to_id` and `id_to_token`) for bidirectional mapping. When you call `tokenizer.tokenize(text)`, it returns tokens as strings; `tokenizer(text)` returns token IDs; and `tokenizer.detokenize(ids)` converts IDs back to text. The vocabulary is built during training or loaded from a pre-trained model, with special tokens (PAD, UNK, BOS, EOS) typically assigned fixed IDs at the beginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5d852",
   "metadata": {},
   "source": [
    "### 1.3 Padding and Masking \n",
    "1. Padding: Short sequences are extended with PAD tokens to match the longest sequence in a batch, enabling efficient parallel processing (e.g., `[5, 10, 15]` â†’ `[5, 10, 15, <PAD>, <PAD>]`)\n",
    "\n",
    "2. Attention Masking: Tells the transformer which positions to ignore during attention.\n",
    "- **No Mask (Bidirectional)**: All tokens attend to all tokens; used in BERT for full context understanding\n",
    "- **Causal Mask (Autoregressive)**: Each token only attends to previous tokens; used in GPT to prevent future information leakage during training\n",
    "- **Padding Mask**: Masks PAD tokens so they don't affect attention scores; combined with other masks in most models\n",
    "\n",
    "``` py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example: batch of token IDs\n",
    "batch = tf.ragged.constant([\n",
    "    [1, 2, 3],\n",
    "    [4, 5]\n",
    "])\n",
    "padded = batch.to_tensor(default_value=0) # Output: [[1, 2, 3], [4, 5, 0]]  <- 0 is the PAD token ID (these are the new tokens)\n",
    "mask = tf.cast(padded != 0, tf.int32) # Output: [[1, 1, 1], [1, 1, 0]]  <- tells attention to ignore the last position in sequence 2 (this is the attention scores not token values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47c9fa",
   "metadata": {},
   "source": [
    "### 1.4 Special Tokens (see ML notes page 121)\n",
    "- `<BOS>`: Beginning of sequence (marks where a sequence starts)\n",
    "- `<EOS>`: End of sequence (marks where a sequence ends)\n",
    "- `<PAD>`: Padding (fills shorter sequences to match batch length)\n",
    "- `<UNK>`: Unknown/ out of vocab token (represents words not in vocabulary)\n",
    "- etc\n",
    "\n",
    "**usage in training**\n",
    "``` text\n",
    "Input:  <BOS> hello world <EOS> <PAD> <PAD>    # BOS is fed as a conditioning token ((a special input token that provides initial context/prompt for the model; the model conditions its next-token predictions on it but is not trained to predict it)) EOS is included so the model learns to predict sequence end PADs fill to uniform length\n",
    "Target: hello world <EOS> <PAD> <PAD> <PAD>   # Target = input shifted left (model predicts the next token at each step, including EOS); PADs fill to uniform length\n",
    "Mask:   1 1 1 1 0 0 0                          # Mask=1 for positions to compute loss (we compute loss for real tokens and EOS), 0 for PADs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde25d45",
   "metadata": {},
   "source": [
    "### 1.5 Sequence Packing and Contiguous Streams \n",
    "- Why: LLM training is compute-heavy, to use memory efficiently, multiple short examples can be concatenated into a single long sequence and then chunked\n",
    "- Benefits: \n",
    "  - Reduces wasted padding\n",
    "  - keepinh sequences dense for attention\n",
    "  \n",
    "**Example (pseudo)**\n",
    "```text\n",
    "Examples: [\"hello\", \"world\"], [\"goodbye\", \"moon\"]\n",
    "Packed sequence: \"hello world goodbye moon\"\n",
    "```\n",
    "- Then split into fixed length chunks (ex: 8 tokens per chunk) for processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2eaec",
   "metadata": {},
   "source": [
    "### 1.6 Sliding Window Chunking \n",
    "- When text is too long to fit in memory, we create overlapping windown to preserve context\n",
    "- why: prevents cutting off dependencies between sequences.\n",
    "- Example Sequence length = 6, chunk size = 4, stride = 2\n",
    "``` text\n",
    "Sequence: [A B C D E F] (len = 6)\n",
    "\n",
    "Chunk 1: Start at position 0 â†’ [A B C D] (len = 4 beacuse chunk size = 4)\n",
    "Chunk 2: Start at position 0 (position) + 2(stride) = 2 â†’ [C D E F] (move the window by 2, keeps last 2 tokens of last chink in new chunk)\n",
    "\n",
    "Chunks:  [A B C D], [C D E F]\n",
    "\n",
    "Result Sequence: [A B C D], [C D E F]\n",
    "             â†‘overlapâ†‘\n",
    "```\n",
    "- Edge case: If the final window doesn't have enough tokens (e.g., only 3 tokens left for chunk_size=4), you either pad it with `<PAD>` tokens or discard it depending on your training strategy.\n",
    "- overlapping ensures context continuity for training \n",
    "- common in LLM pretraining \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b38f429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff521f58",
   "metadata": {},
   "source": [
    "### 1.7 Complete Example: Combining All Tokenization Steps\n",
    "\n",
    "This example demonstrates the entire pipeline from raw text to training-ready sequences, incorporating all concepts from 1.1-1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2957877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "# Load pretrained tokenizer (handles vocab, special tokens, BPE subword)\n",
    "tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
    "\n",
    "# Create dataset pipeline: tokenization â†’ chunking â†’ padding\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices([\"Hello world\", \"Goodbye moon\"])\n",
    "    .map(tokenizer)  # Tokenize: text â†’ IDs\n",
    "    .unbatch()  # Flatten to token stream (packing)\n",
    "    .batch(8, drop_remainder=False)  # Chunk into sequences of 8 tokens\n",
    "    .map(lambda x: (x[:-1], x[1:]))  # Create (input, target) pairs\n",
    "    .padded_batch(2, padded_shapes=([None], [None]))  # Pad and batch\n",
    ")\n",
    "\n",
    "# Usage:\n",
    "inputs, targets = next(iter(dataset))\n",
    "print(tokenizer.detokenize(inputs[0]))\n",
    "print(tokenizer.detokenize(targets[0]))\n",
    "\n",
    "\"\"\" \n",
    "OUTPUT:\n",
    "Hello worldGoodbye\n",
    "worldGoodbye moon\n",
    "\n",
    "EXPLANATION:\n",
    "------------\n",
    "The dataset variable is a tf.data.Dataset pipeline that transforms raw text into \n",
    "training-ready (input, target) pairs. It's a lazy iterator (doesn't process until called).\n",
    "\n",
    "Pipeline steps:\n",
    "1. from_tensor_slices: Creates dataset from list of strings\n",
    "2. map(tokenizer): Converts each text â†’ token IDs (e.g., \"Hello\" â†’ [15496, 995])\n",
    "3. unbatch(): Flattens all sequences into one continuous token stream (sequence packing)\n",
    "4. batch(8): Groups tokens into chunks of 8 (creates fixed-length sequences)\n",
    "5. map(lambda): Splits each chunk into (input, target) where target = input shifted left\n",
    "6. padded_batch(2): Groups 2 sequences into a batch, pads shorter ones to match length\n",
    "\n",
    "Usage output - How next-token prediction works:\n",
    "The model predicts the NEXT token at EACH position, not just the last one:\n",
    "- Position 0: Given \"Hello\" â†’ predict \"world\"\n",
    "- Position 1: Given \"Hello world\" â†’ predict \"Goodbye\"  \n",
    "- Position 2: Given \"Hello worldGoodbye\" â†’ predict \"moon\"\n",
    "\n",
    "So the target sequence shows what should be predicted at each step.\n",
    "The entire target = input shifted left by 1 token (each target is the next token)\n",
    "\n",
    "Chunking Strategy Comparison:\n",
    "-----------------------------\n",
    "Token stream: [A, B, C, D, E, F, G, H, I, J]\n",
    "\n",
    "Fixed batch (current): .batch(4)\n",
    "  Chunk 1: [A, B, C, D]          (tokens 0-3)\n",
    "  Chunk 2: [E, F, G, H]          (tokens 4-7)\n",
    "  Chunk 3: [I, J]                (tokens 8-9)\n",
    "  â†’ No overlap, each token appears once\n",
    "\n",
    "Sliding window: .window(size=4, shift=2, drop_remainder=True)\n",
    "  Chunk 1: [A, B, C, D]          (tokens 0-3)\n",
    "  Chunk 2: [C, D, E, F]          (tokens 2-5, overlaps last 2 from chunk 1)\n",
    "  Chunk 3: [E, F, G, H]          (tokens 4-7, overlaps last 2 from chunk 2)\n",
    "  â†’ Overlap preserves context across chunks, useful for long documents\n",
    "\n",
    "IMPORTANT: Both strategies train on next-token prediction at EVERY position!\n",
    "---------------------------------------------------------------------------\n",
    "Fixed batch:\n",
    "  â€¢ Chunk 1: [A,B,C,D] â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "  â€¢ Chunk 2: [E,F,G,H] â†’ trains: (Eâ†’F), (E,Fâ†’G), (E,F,Gâ†’H)\n",
    "  â€¢ Each token appears ONCE\n",
    "\n",
    "Sliding window:\n",
    "  â€¢ Chunk 1: [A,B,C,D] â†’ trains: (Aâ†’B), (A,Bâ†’C), (A,B,Câ†’D)\n",
    "  â€¢ Chunk 2: [C,D,E,F] â†’ trains: (Câ†’D), (C,Dâ†’E), (C,D,Eâ†’F)\n",
    "  â€¢ Tokens C and D appear TWICE (extra training for better context)\n",
    "\n",
    "The chunking method only affects which tokens are grouped together, not how training works.\n",
    "Sliding window gives overlapping tokens extra exposure for better long-range dependencies\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
