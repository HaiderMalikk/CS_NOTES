{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1439c59a",
   "metadata": {},
   "source": [
    "# Basics Of Model Training in TensorFlow\n",
    "\n",
    "### model.fit() = higher-level: Keras manages the loop, metrics, logging, and optimizations for you. Use this when you want productivity and standard behavior.\n",
    "- Keras handles everything for you:\n",
    "  - runs forward + backward automatically\n",
    "  - tracks metrics, batches, logging\n",
    "  - supports callbacks, early stopping, checkpoints\n",
    "- Use when: you want productivity and standard training behavior.\n",
    "\n",
    "### GradientTape loop = lower-level: you write the forward/backward/update steps yourself. Use this to learn or to customize behavior per-step.\n",
    "- You manually control each step:\n",
    "  - compute loss inside the tape\n",
    "  - get gradients yourself\n",
    "  - apply optimizer updates manually\n",
    "- Use when: you need custom per-step logic, research flexibility, or non-standard training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2321a8c",
   "metadata": {},
   "source": [
    "### imports + tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and tiny dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "# tiny dataset: 6 sentences (toy)\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"This film was terrible\",\n",
    "    \"Amazing plot and acting\",\n",
    "    \"Horrible movie experience\",\n",
    "    \"Loved it, would watch again\",\n",
    "    \"Worst movie ever\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# wrap as tf.data.Dataset (strings, ints)\n",
    "ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "# Why: small data so you can step through everything quickly. tf.data.Dataset simply holds your pairs of (text, label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423cff93",
   "metadata": {},
   "source": [
    "### preprocessing with TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0fc515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: (2, 10) labels: [0 1]\n",
      "input_ids example (first batch row): [ 5  2 17  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:21:22.996823: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: TextVectorization to convert strings -> token ids\n",
    "vocab_size = 1000\n",
    "max_len = 10\n",
    "\n",
    "vectorize = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "# adapt the vectorizer on the text data\n",
    "vectorize.adapt(ds.map(lambda t, y: t))\n",
    "\n",
    "# helper to apply vectorization and batch\n",
    "def prepare(ds, batch_size=2, shuffle=True):\n",
    "    ds2 = ds\n",
    "    if shuffle:\n",
    "        ds2 = ds2.shuffle(100)\n",
    "    # Always map vectorization before batching\n",
    "    ds2 = ds2.map(lambda t, y: (vectorize(t), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds2 = ds2.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds2\n",
    "\n",
    "train_ds = prepare(ds, batch_size=2)\n",
    "# show one batch to inspect shapes / values\n",
    "for xb, yb in train_ds.take(1):\n",
    "    print(\"input_ids shape:\", xb.shape, \"labels:\", yb.numpy())\n",
    "    print(\"input_ids example (first batch row):\", xb.numpy()[0])\n",
    "\n",
    "# Why: TextVectorization is a convenient tokenizer + indexer. You see the concrete token IDs and shapes before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817bf90",
   "metadata": {},
   "source": [
    "### small model builder (standalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6926d751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embed (\u001b[38;5;33mEmbedding\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │        \u001b[38;5;34m16,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ out (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,145</span> (63.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,145\u001b[0m (63.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,145</span> (63.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,145\u001b[0m (63.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: tiny model builder (embedding -> pooling -> dense)\n",
    "def build_tiny_model(vocab_size=vocab_size, embed_dim=16, max_len=max_len, num_classes=1):\n",
    "    inputs = keras.Input(shape=(max_len,), dtype='int32', name='input_ids')\n",
    "    x = layers.Embedding(vocab_size, embed_dim, name='embed')(inputs)\n",
    "    x = layers.GlobalAveragePooling1D(name='pool')(x)\n",
    "    x = layers.Dense(8, activation='relu', name='dense')(x)\n",
    "    if num_classes == 1:\n",
    "        outputs = layers.Dense(1, activation='sigmoid', name='out')(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', name='out')(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "model = build_tiny_model()\n",
    "model.summary()\n",
    "\n",
    "# Why: simple model so training finishes instantly — good for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df72b10",
   "metadata": {},
   "source": [
    "### Train using model.fit() (high-level)\n",
    "\n",
    "**Keras gives you a full training engine:**\n",
    "\n",
    "- automatically batches the data\n",
    "- runs the forward pass\n",
    "- calculates the loss\n",
    "- applies the optimizer\n",
    "- tracks metrics\n",
    "- supports callbacks (EarlyStopping, checkpoints, etc.)\n",
    "- supports distributed training with tf.distribute\n",
    "\n",
    "*You only specify:*\n",
    "```py\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(ds, epochs=10)\n",
    "```\n",
    "**Pros**\n",
    "- Very short code\n",
    "- Very reliable & battle-tested\n",
    "- Automatically handles edge cases\n",
    "- Works perfectly with built-in layers/models\n",
    "- Easy multi-GPU/multi-worker training\n",
    "- Built-in callbacks (logging, checkpoints, LR schedules, etc.)\n",
    "  \n",
    "**Cons**\n",
    "- Hard to customize per-step behavior\n",
    "- Hard to do weird training loops (RL, GANs, meta-learning)\n",
    "- Hard to inject custom losses that depend on intermediate tensors or multiple passes\n",
    "\n",
    "**When to use**\n",
    "- Use model.fit() when your training is standard supervised learning and you don’t need custom step-by-step behavior.\n",
    "- It’s the right choice 90% of the time because it’s simple, clean, and handles batching, metrics, and callbacks automatically.\n",
    "- Best for productivity, clean code, and standard training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed4e0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:21:36.538275: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 1s - 366ms/step - binary_accuracy: 0.5000 - loss: 0.6942\n",
      "Epoch 2/5\n",
      "Epoch 2/5\n",
      "3/3 - 0s - 21ms/step - binary_accuracy: 0.3333 - loss: 0.6939\n",
      "Epoch 3/5\n",
      "3/3 - 0s - 21ms/step - binary_accuracy: 0.3333 - loss: 0.6939\n",
      "Epoch 3/5\n",
      "3/3 - 0s - 20ms/step - binary_accuracy: 0.6667 - loss: 0.6926\n",
      "Epoch 4/5\n",
      "3/3 - 0s - 20ms/step - binary_accuracy: 0.6667 - loss: 0.6926\n",
      "Epoch 4/5\n",
      "3/3 - 0s - 19ms/step - binary_accuracy: 0.8333 - loss: 0.6920\n",
      "Epoch 5/5\n",
      "3/3 - 0s - 19ms/step - binary_accuracy: 0.8333 - loss: 0.6920\n",
      "Epoch 5/5\n",
      "3/3 - 0s - 19ms/step - binary_accuracy: 0.8333 - loss: 0.6912\n",
      "fit() took 1.350 s\n",
      "3/3 - 0s - 19ms/step - binary_accuracy: 0.8333 - loss: 0.6912\n",
      "fit() took 1.350 s\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: high-level training with model.fit()\n",
    "model = build_tiny_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "# Train for a few epochs\n",
    "t0 = time.time()\n",
    "history = model.fit(train_ds, epochs=5, verbose=2)\n",
    "print(\"fit() took %.3f s\" % (time.time() - t0))\n",
    "\n",
    "# What happened: fit() ran an internal loop: for each batch it called the model, computed loss, computed gradients, and applied optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23500a1f",
   "metadata": {},
   "source": [
    "### Manual training loop with GradientTape (low-level)\n",
    "\n",
    "**You write everything explicitly:**\n",
    "```py\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for x, y in ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x, training=True)\n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "**Pros**\n",
    "- You can modify every step\n",
    "- Add custom logic:\n",
    "    - reinforcement learning updates\n",
    "    - multiple losses\n",
    "    - contrastive learning\n",
    "    - gradient accumulation\n",
    "    - clipping, freezing, mixing\n",
    "    - GAN training (two models, two optimizers)\n",
    "- Perfect for research and experiments\n",
    "  \n",
    "**Cons**\n",
    "- More code to write\n",
    "- Easier to make mistakes\n",
    "- You must manage metrics manually\n",
    "- You must write your own loop for epochs, batches, validation\n",
    "- Harder to integrate with distribution strategies\n",
    "\n",
    "**When to use**\n",
    "- Use a custom training loop (GradientTape) when you need full control over the training step — e.g., custom losses, unusual update rules, multi-model setups (GANs), reinforcement learning, or any behavior that model.fit() can’t express.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc71fee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.6956, acc=0.5000\n",
      "Epoch 2: loss=0.6958, acc=0.3333\n",
      "Epoch 3: loss=0.6949, acc=0.3333\n",
      "Epoch 4: loss=0.6944, acc=0.3333\n",
      "Epoch 5: loss=0.6943, acc=0.3333\n",
      "custom loop took 0.778 s\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: low-level training with GradientTape\n",
    "# We'll recreate model & optimizer to compare fairly\n",
    "model2 = build_tiny_model()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "train_loss_metric = keras.metrics.Mean(name=\"train_loss\")\n",
    "train_acc_metric = keras.metrics.BinaryAccuracy(name=\"train_acc\")\n",
    "\n",
    "# We'll run the same number of epochs and iterate datasets manually\n",
    "epochs = 5\n",
    "t0 = time.time()\n",
    "\n",
    "# Optional: wrap train_step in @tf.function to compile to graph after you confirm correctness\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model2(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model2.trainable_variables))\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # reset metrics at start of epoch (use singular API `reset_state()`)\n",
    "    train_loss_metric.reset_state()\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
    "        train_step(x_batch, tf.cast(y_batch, tf.float32))\n",
    "    print(f\"Epoch {epoch+1}: loss={train_loss_metric.result():.4f}, acc={train_acc_metric.result():.4f}\")\n",
    "\n",
    "print(\"custom loop took %.3f s\" % (time.time() - t0))\n",
    "\n",
    "# What happened: You implemented the forward pass, computed loss, computed gradients via GradientTape, \n",
    "# applied updates, and manually updated metrics. @tf.function compiles train_step into a graph for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3a86d",
   "metadata": {},
   "source": [
    "### Quick comparison & notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae06b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized: [[13  1 10  3 15  0  0  0  0  0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "fit() model predicts: 0.5008707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "custom-loop model predicts: 0.49770308\n",
      "fit() model predicts: 0.5008707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "custom-loop model predicts: 0.49770308\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: quick notes (print a prediction)\n",
    "sample = tf.constant([\"I absolutely loved this film\"])\n",
    "sample_vec = vectorize(tf.expand_dims(sample, -1))\n",
    "print(\"Vectorized:\", sample_vec.numpy())\n",
    "print(\"fit() model predicts:\", model.predict(sample_vec)[0][0])\n",
    "print(\"custom-loop model predicts:\", model2.predict(sample_vec)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531e152",
   "metadata": {},
   "source": [
    "**Observations / learning points**\n",
    "- Both methods should train the model; results will differ slightly because initial weights/optimizer states differ unless you re-used exact same model & seeds.\n",
    "- model.fit() is concise; GradientTape gives you control\n",
    "- Use @tf.function for speed on train_step but debug in eager mode first (remove @tf.function) if things break.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb9ef7",
   "metadata": {},
   "source": [
    "### Other Custom Loops\n",
    "\n",
    "* **`tf.GradientTape` is the main and modern way** to write custom training loops in TensorFlow.\n",
    "* You can also make custom loops by **overriding `model.train_step()`**, which still uses GradientTape internally but keeps compatibility with `model.fit()`.\n",
    "* Older methods exist (`optimizer.compute_gradients`, TF1-style training ops), but they are **deprecated and not recommended**.\n",
    "\n",
    "**In practice:**\n",
    "➡️ Use **GradientTape** for full control\n",
    "➡️ Use **`train_step()` override** if you want custom behavior **and** to keep using `model.fit()`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
