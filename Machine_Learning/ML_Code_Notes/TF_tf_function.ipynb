{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1c5afc",
   "metadata": {},
   "source": [
    "# @tf.function\n",
    "\n",
    "### what @tf.function is and why it matters\n",
    "- @tf.function compiles a Python function into a TensorFlow graph (AutoGraph + XLA-ready ops in some cases).\n",
    "- Result: much faster execution for numeric code because it removes Python overhead and allows graph optimizations.\n",
    "- But: compilation has costs (tracing, retracing) and restrictions (no arbitrary Python behavior inside compiled parts).\n",
    "\n",
    "Bottom line: compile hot, stable, tensor-only code paths — typically inner training/validation steps — and keep Python orchestration (logging, checkpointing, dataset iteration) in plain Python.\n",
    "\n",
    "### When to use @tf.function (practical rules)\n",
    "Use it for:\n",
    "- train_step and val_step (forward → loss → backward → apply grads).\n",
    "- Computationally heavy preprocessing you can express in TF ops (e.g., tf.strings.*, tf.image.*) — if it’s pure-TF and benefits from graph speed.\n",
    "\n",
    "**Do NOT wrap:**\n",
    "- Entire training loop if it contains Python I/O, printing, tf.summary writers not designed for inside @tf.function, or checkpoint logic. Keep outer loop in Python.\n",
    "- Code that needs to run arbitrary Python logic per step (e.g., complex Python control flow, logging per step during debugging).\n",
    "\n",
    "### Common pitfalls & how to fix them\n",
    "1) Retracing overhead\n",
    "   - If @tf.function is called on many different shapes/dtypes or with Python objects changing, TensorFlow will retrace repeatedly and kill performance.\n",
    "   - Fix: give stable shapes / dtypes, add input_signature or call with tensors of fixed tf.TensorShape (use tf.TensorSpec).\n",
    "2) Python objects inside @tf.function\n",
    "   - Python lists/dicts, file I/O, printing, random Python code — these either fail or cause retracing.\n",
    "   - Fix: move Python logic outside the function, use TF equivalents (e.g., tf.print, tf.io.write_file) or convert to tensors.\n",
    "3) Mutable Python state\n",
    "   - Modifying Python lists/dicts inside a compiled function won’t have the intended persistent effect.\n",
    "   - Fix: keep mutable state in tf.Variable or in Python outside the function.\n",
    "4) Debugging is harder\n",
    "   - Errors inside a @tf.function often show cryptic stack traces.\n",
    "   - Fix: run in eager (tf.config.run_functions_eagerly(True)) to debug, then switch back.\n",
    "5) Returning non-Tensor objects\n",
    "   - Functions should primarily return tensors or (nested) structures of tensors. Returning complex Python objects can cause issues.\n",
    "6) Random seeds & determinism\n",
    "   - tf.random.set_seed() works inside @tf.function but order of ops matters. If you seed inside function, be careful.\n",
    "7) Side effects\n",
    "   - tf.print and tf.summary can be used in @tf.function, but be mindful of semantics (use tf.summary with proper writer contexts and flush outside).\n",
    "\n",
    "### Profiling basics (what profiler gives you)\n",
    "- tf.profiler.experimental captures timelines, op-by-op cost, memory usage, kernel details, and python/CPU hotspots.\n",
    "- Typical workflow:\n",
    "  1) Start trace with tf.profiler.experimental.start(logdir)\n",
    "  2) Run a few warmup steps + the steps you want to profile\n",
    "  3) Stop the trace tf.profiler.experimental.stop()\n",
    "  4) Launch tensorboard --logdir <logdir> and use the “Profile” tab to inspect trac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf005f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install tensorboard\n",
    "%pip install tensorboard-plugin-profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e5dd3",
   "metadata": {},
   "source": [
    "### Minimal, runnable example — one cell\n",
    "\n",
    "This does three things:\n",
    "1) Builds a tiny dataset and model.\n",
    "2) Shows train_step with and without @tf.function.\n",
    "3) Runs the profiler for a few steps and writes a TensorBoard trace to ./logs/profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6542ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-cell example: @tf.function usage + profiling (runnable)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time, os, shutil\n",
    "\n",
    "# --- small dataset (toy) ---\n",
    "(x_train, y_train), _ = keras.datasets.imdb.load_data(num_words=2000)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=80)\n",
    "x_train = x_train[:1024]   # keep it small so profiling is quick\n",
    "y_train = y_train[:1024]\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# --- tiny model ---\n",
    "def make_model():\n",
    "    inp = keras.Input(shape=(80,), dtype='int32')\n",
    "    x = layers.Embedding(2000, 32)(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(32))(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(inp, out)\n",
    "\n",
    "model = make_model()\n",
    "optimizer = keras.optimizers.Adam(1e-3)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# --- train_step functions: eager vs @tf.function ---\n",
    "# Eager version (no decorator) -- easier to debug\n",
    "def train_step_eager(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x, training=True)\n",
    "        loss = loss_fn(y, pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Compiled version (fast) -- wrap only the inner step\n",
    "@tf.function\n",
    "def train_step_compiled(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x, training=True)\n",
    "        loss = loss_fn(y, pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# --- small helper to run N steps using provided train_step ---\n",
    "def run_steps(train_step_fn, dataset, steps=10):\n",
    "    it = iter(dataset)\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for i in range(steps):\n",
    "        x_batch, y_batch = next(it)\n",
    "        l = train_step_fn(x_batch, tf.cast(y_batch, tf.float32))\n",
    "        # If train_step_fn is a tf.function, l is a tensor; convert for logging\n",
    "        try:\n",
    "            losses.append(float(l))\n",
    "        except Exception:\n",
    "            losses.append(l.numpy())\n",
    "    dt = time.time() - t0\n",
    "    print(f\"{train_step_fn.__name__}: ran {steps} steps in {dt:.3f}s, avg loss {sum(losses)/len(losses):.4f}\")\n",
    "\n",
    "# Warm up: run a few eager steps to initialize variables\n",
    "print(\"Warmup (eager)...\")\n",
    "run_steps(train_step_eager, ds, steps=3)\n",
    "\n",
    "# Measure compiled vs eager\n",
    "print(\"\\nEager timing:\")\n",
    "run_steps(train_step_eager, ds, steps=10)\n",
    "\n",
    "print(\"\\nCompiled (@tf.function) timing (first call includes trace/compile overhead):\")\n",
    "run_steps(train_step_compiled, ds, steps=10)\n",
    "print(\"Compiled timing (second run, should be faster):\")\n",
    "run_steps(train_step_compiled, ds, steps=10)\n",
    "\n",
    "# --- Profiling: capture a small trace around compiled steps ---\n",
    "logdir = \"./assets/logs/profile\"\n",
    "if os.path.exists(logdir):\n",
    "    shutil.rmtree(logdir)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# Use profiler to capture a small window. We profile the compiled train_step.\n",
    "tf.profiler.experimental.start(logdir)\n",
    "print(\"\\nProfiling: running 5 compiled steps with profiler running...\")\n",
    "run_steps(train_step_compiled, ds, steps=5)\n",
    "tf.profiler.experimental.stop()\n",
    "print(f\"Profiler trace written to: {logdir}\")\n",
    "print(\"Open TensorBoard: tensorboard --logdir assets/logs/profile, then go to Profile tab to view.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2b92c",
   "metadata": {},
   "source": [
    "### How to use the example and what to look for in TensorBoard\n",
    "1) Run the cell. It will:\n",
    "    - Show warmup (eager).\n",
    "    - Show timings for eager vs compiled.\n",
    "    - Write a profiler trace to ./logs/profile.\n",
    "2) Start TensorBoard: tensorboard --logdir ./logs_path\n",
    "   - Open the Profile tab.\n",
    "   - Inspect the Trace Viewer: you’ll see CPU/GPU timelines, op durations, and which ops dominate time.\n",
    "   - Look at the “TensorFlow Stats” and “Kernel Stats” pages to identify hotspots and memory peaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir assets/logs/profile --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ea1d0",
   "metadata": {},
   "source": [
    "### Practical tips when profiling\n",
    "- Warm up first: JIT and GPU need warmup. Run a few steps before capturing the trace.\n",
    "- Profile a small window: profiling huge runs produces massive traces. Profile 5–50 steps.\n",
    "- Use tf.function: profile compiled functions to see optimized kernels and fused ops.\n",
    "- If you see retracing: profiler shows repeated traces. Fix retracing by stabilizing inputs or adding input_signature to @tf.function:\n",
    "``` py \n",
    "@tf.function(input_signature=[tf.TensorSpec([None, 80], tf.int32), tf.TensorSpec([None], tf.int32)])\n",
    "def train_step_compiled(x, y): ...\n",
    "```\n",
    "- TensorBoard helps: use the Trace Viewer to see whether CPU preprocessing or GPU compute is the bottleneck. If CPU dominates, tune tf.data (more num_parallel_calls, prefetch, caching).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218f2bd",
   "metadata": {},
   "source": [
    "### Extra advanced notes\n",
    "- tf.function + XLA: you can request XLA compilation per function (jit_compile=True in tf.function or in optimizer) if you want to experiment with backend compilation — tradeoff: compile time vs runtime speed.\n",
    "- For debugging, tf.print works inside @tf.function and prints during execution (useful when you can’t run eager).\n",
    "- If you use tf.data with map functions that are decorated with @tf.function, the map will run inside the dataset pipeline efficiently.\n",
    "- For reproducibility across @tf.function, set seeds before training and avoid Python randomness inside the function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305b150",
   "metadata": {},
   "source": [
    "### What is a TensorFlow Graph?\n",
    "\n",
    "A TensorFlow graph is a directed acyclic graph (DAG) of computation:\n",
    "- Nodes: operations (e.g., `MatMul`, `Add`, `Relu`, `Conv2D`).\n",
    "- Edges: tensors flowing between ops (data dependencies).\n",
    "- No Python control flow at runtime; only the traced ops/tensors.\n",
    "- Built when TensorFlow \"traces\" your Python function (e.g., via `@tf.function`).\n",
    "\n",
    "Conceptually, for `y = relu(Wx + b)` the graph looks like:\n",
    "```\n",
    " x ----> MatMul ----> Add ----> Relu ----> y\n",
    "          ^            ^\n",
    "          |            |\n",
    "          W            b\n",
    "```\n",
    "\n",
    "Why graphs?\n",
    "- Optimize globally (fusion, constant folding, device placement).\n",
    "- Run efficiently on CPU/GPU/TPU without Python overhead.\n",
    "- Serialize as `GraphDef`/`SavedModel` and serve elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal example: build a graph with tf.function and inspect it\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def model(x, W, b):\n",
    "    y = tf.matmul(x, W) + b\n",
    "    return tf.nn.relu(y)\n",
    "\n",
    "# Fixed shapes for a stable trace\n",
    "x = tf.random.normal([2, 3])\n",
    "W = tf.random.normal([3, 4])\n",
    "b = tf.random.normal([4])\n",
    "\n",
    "concrete = model.get_concrete_function(\n",
    "    tf.TensorSpec(x.shape, x.dtype),\n",
    "    tf.TensorSpec(W.shape, W.dtype),\n",
    "    tf.TensorSpec(b.shape, b.dtype),\n",
    ")\n",
    "\n",
    "print(\"Ops in graph:\")\n",
    "for op in concrete.graph.get_operations():\n",
    "    print(f\"- {op.name}: {op.type}\")\n",
    "\n",
    "# Save GraphDef as text for a quick look\n",
    "os.makedirs('assets', exist_ok=True)\n",
    "path_pbtxt = os.path.join('assets', 'example_graph.pbtxt')\n",
    "tf.io.write_graph(concrete.graph.as_graph_def(), 'assets', 'example_graph.pbtxt', as_text=True)\n",
    "print(f\"\\nSaved graph to {path_pbtxt}\")\n",
    "\n",
    "# Log graph for TensorBoard\n",
    "logdir = os.path.join('assets', 'logs', 'graph', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "with writer.as_default():\n",
    "    tf.summary.graph(concrete.graph)\n",
    "writer.flush()\n",
    "print(\"TensorBoard logs at:\", logdir)\n",
    "print(\"To view here, you can run:\\n%load_ext tensorboard\\n%tensorboard --logdir assets/logs/graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c517c",
   "metadata": {},
   "source": [
    "Start TensorBoard: tensorboard --logdir ./logs_path\n",
    "   - Open GRAPHS\n",
    "   - look at graph inspect nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir assets/logs/graph --port 6007 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
