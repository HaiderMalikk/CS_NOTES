{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea6234c",
   "metadata": {},
   "source": [
    "üåü Introduction to TensorFlow\n",
    "\n",
    "TensorFlow is an open-source platform developed by Google for machine learning and deep learning tasks. It allows developers to build and train models using both high-level APIs (like Keras) and low-level operations.\n",
    "\n",
    "üß† Why TensorFlow for AI Research?\n",
    "\n",
    "Scalable across CPUs, GPUs, and TPUs\n",
    "\n",
    "Integrated Keras API for quick prototyping\n",
    "\n",
    "Production-ready (used by Google internally)\n",
    "\n",
    "Tools like TensorBoard, TensorFlow Lite, TensorFlow.js, and more for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2675279",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0da16",
   "metadata": {},
   "source": [
    "Verify Metal for MACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow detects the GPU\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check if TensorFlow is running on Metal\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Force TensorFlow to run on GPU if available\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf import to install use: pip install tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\" \n",
    "Tersors\n",
    "a tensor is a multi-dimensional array that can be used for various computations. \n",
    "conastants are immutable tensors, meaning their values cannot be changed after they are created.\n",
    "\"\"\"\n",
    "x = tf.constant([[1,2], [3,4]]) # you must wrap the values in a list the outer brackets signify the tensor, this is a 2D tensor\n",
    "print(x) # prints the tensor + shape = (2,2) i.e 2 rows and 2 columns and dtype = int32 i.e 32 bit integer\n",
    "\n",
    "\"\"\" \n",
    "Computational Graph\n",
    "A computational graph is a way to represent the operations and data flow in a TensorFlow program.\n",
    "for example, if you want to add two tensors, you can create a computational graph that represents the addition operation.\n",
    "EX cont: say you have two tensors A and B, and you want to add them together. A= [[1,2],[3,4]] and B=[[5,6],[7,8]]\n",
    "the computational graph to add them will have two nodes, one for each tensor, and an edge that represents the addition operation.\n",
    "The graph will look like this:\n",
    "A ----> + ----> C\n",
    "       ^\n",
    "       |\n",
    "       B\n",
    "where A and B are the input tensors, + is the addition operation, and C is the resulting tensor after the addition operation.\n",
    "\"\"\"\n",
    "\n",
    "# Create a computational graph\n",
    "x = tf.constant([[3, 4]])\n",
    "y = tf.constant([[5, 6]])\n",
    "print(x+y) # prints the result of the addition operation = [[ 8 10]] as 3+5 = 8 and 4+6 = 10\n",
    "# unlike TF1, TF2 does not require you to create a session to run the graph.\n",
    "# a session is a way to execute the graph and get the result.\n",
    "# in TF2, the graph is executed immediately when you run the operation.\n",
    "# tf functions are used to create a graph and run it in a session. tf functions are used to create a graph and run it in a session. it compiles functions into a static graph and runs them in a session. (better performance)\n",
    "# EX: lets use the function decorator to create a graph and run it in a session. \n",
    "@tf.function\n",
    "def add_tensors(a, b):\n",
    "    return a * b # multipling as also a graph operation (computational graph)\n",
    "print(add_tensors(x, y)) # prints the result of the addition operation = [[ 15 24]] as 3*5 = 15 and 4*6 = 24\n",
    "\n",
    "\"\"\" \n",
    "Building a Model\n",
    "a model is a collection of layers that are connected together to form a neural network.\n",
    "each layer is a function that takes an input tensor and produces an output tensor.\n",
    "the input tensor can be for ex an image's grayscale value, and the output tensor will be some values that represent the image's features. \n",
    "this is then passed to the next layer, and so on, until the final output tensor is produced. which is the final prediction of the model.\n",
    "\n",
    "- We use keras to build a model, keras is a high-level API for building and training deep learning models.\n",
    "- from keras we can get the Sequential model, which is a linear stack of layers. the Sequential model is a simple way to build a model by adding layers one by one.\n",
    "- This means that the output of one layer is the input to the next layer. sequential models are easy to use and understand, and they are a good choice for most applications.\n",
    "- the end result is a model that can be trained on data and used to make predictions.\n",
    "\n",
    "- We also use Dense layers, which are fully connected layers. meaning every neuron in the layer is connected to every neuron in the previous layer.\n",
    "- this is not how all neural networks work, but it is a common way to build a model.\n",
    "- The Dense layer takes an input tensor and produces an output tensor by applying a linear transformation to the input tensor.\n",
    "\n",
    "- So to sum, Sequential model is a linear stack of layers, and Dense layers are fully connected layers we combine then to build a Stack of layers that are fully connected to each other.\n",
    "\"\"\"\n",
    "# to install keras, run the following command in the terminal: pip install keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\"\"\" \n",
    "Here we first define the sequential model, then we add a Dense layer which creates a fully connected layer\n",
    "the input layer has 784 neurons and the output layer has 10 neurons. this can be a image which is 28x28 pixels = 784 pixels\n",
    "and outputs a 10 class prediction. 0-9 for a digit classification problem.\n",
    "\n",
    "relu is a activation function that is used to introduce non-linearity in the model.\n",
    "we cannot use a linear activation function because the model will not be able to learn complex patterns in the data.\n",
    "Foe EX the XOR problem: XOR gate is as follows: \n",
    "XOR gate is a gate that outputs 1 if both inputs are different and 0 if both inputs are the same. in a table: \n",
    "0 0 0\n",
    "0 1 1\n",
    "1 0 1\n",
    "1 1 0\n",
    "on a plot they look like:\n",
    "1| 1   0\n",
    " | \n",
    "0| 0   1\n",
    " |______\n",
    "   0   1\n",
    "There is no way the separate the two classes with a straight line. i.e you cannot set apart true and flase values with a linear function.\n",
    "But using a function like signmoid or relu, we can separate the two classes with a curve beacuse there not a straight line they can bend the curve to separate the two classes.\n",
    "\n",
    "Softmax is a activation function that is used to convert the output of the model into a probability distribution.\n",
    "For EXample, if the model outputs a tensor with 10 values, softmax will convert it into a tensor with 10 values that sum to 1.\n",
    "meaning no matter how many outputs we have each output will be a probability and all the probabilities will sum to 1.\n",
    "Each nueron in the output layer will represent a class, and the value of the neuron will represent the probability of that class. this probability is 0-1\n",
    "what is a class: a class is a group of objects that are similar to each other in some way. for example in the image prediction problem\n",
    "we have 10 classes, one for each digit from 0 to 9. the model will output a tensor with 10 values, one for each class.\n",
    "then the softmax function will convert the output into a probability distribution so for example if the model outputs [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "this means the digits 0-9 has the probabilities of 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 respectively. where the model will choose 8 as the prediction.\n",
    "\n",
    "ReLU (Rectified Linear Unit) sets all negative values to zero and keeps positive values unchanged.\n",
    "In a simple NN, it adds non-linearity, helping the model learn complex patterns instead of just straight lines.\n",
    "Mathematically:\n",
    "ReLU(x)=max‚Å°(0,x)\n",
    "ReLU(x)=max(0,x)\n",
    "‚úÖ Keeps positives ‚Üí same\n",
    "‚úÖ Turns negatives ‚Üí 0\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\"\"\" \n",
    "compile and train\n",
    "- in the complile step, we define the loss function, optimizer, and metrics to be used during training.\n",
    "- in the training step, we fit the model to the data and train it for a number of epochs.\n",
    "\n",
    "in our ex wedifine some data first repersenting the 784 pixels of a image and the labels are the classes of the digits.\n",
    "so 784 random inputs and a lable for each input (both are random and generated a 1000 times)\n",
    "\n",
    "then we compile our model using the adam optimizer and sparse_categorical_crossentropy loss function.\n",
    "- the loss function is used to measure how well the model is performing (0-infinite). for Example, if the model is predicting the wrong class, the loss function will return a high value.\n",
    "  and vice versa adam is a popular optimizer that is used for most problems. sparse_categorical_crossentropy is used for multi-class classification problems like out digit classification problem.\n",
    "  optimizer is used to update the weights of the model during training. metrics (accuracy) (0-1 or 0-100%) are used to measure the performance of the model during training. we use accuracy as our metric measuring how well the model is performing.\n",
    "  loss vs metrics: loss is used to measure how well the model is performing, while metrics are used to measure the performance of the model during training.\n",
    "\n",
    "Then we fit the model to the data and train it for 10 epochs.\n",
    "- we define our data and labels using numpy and do 10 epochs of training.\n",
    "  a epoch is one pass through the entire dataset for ex if we have 1000 samples of images, then 1 epoch is 1000 samples. \n",
    "\"\"\"\n",
    "# def some data\n",
    "import numpy as np\n",
    "train_data = np.random.rand(1000, 784) # 1000 samples of 784 features for a image, this generates 784 random values each 784 values are in one row the other 999 rows are the other samples\n",
    "# the data looks like \n",
    "# [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 ..... 784 values],\n",
    "#   [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 .....784 values],\n",
    "#   ...... 1000 rows]\n",
    "# this denotes 100 samples of 784 features for a image\n",
    "train_labels = np.random.randint(10, size=(1000,)) # 1000 samples of labels from 0 to 9 i.e the classes each lable in in a row each 784values row is paired with a label row i.e just one valie 0-9\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=10)\n",
    "# Results : from the last epoch we get the loss and accuracy of the model: accuracy: 0.2501 - loss: 2.1262 our accuracy is bad but the data is random so we cannot expect a good accuracy.\n",
    "\n",
    "\"\"\" \n",
    "Model Subclassing\n",
    "- Model subclassing is a way to create custom models in TensorFlow by subclassing the Model class\n",
    "- this means we define our own model by creating a class that inherits from the Model class.\n",
    "- so we can have as many layers as we want and define our own forward pass which is the way the data flows through the model.\n",
    "- this is good for custom models that are not linear stacks of layers.\n",
    "\n",
    "in our Example the model has only two layers, but we can add as many layers as we want.\n",
    "the frist layer is 64 neurons and the second layer is 10 neurons. we pass the inputs through the first layer and then pass the output of the first layer through the second layer.\n",
    "and we return the output of the second layer as the final output of the model.\n",
    "\"\"\"\n",
    "class MyModel(tf.keras.Model): # inheriting from the Model class\n",
    "    def __init__(self): # constructor\n",
    "        super(MyModel, self).__init__() # call the constructor of the parent class so initialize the model\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu') # first layer 64 neurons + relu activation\n",
    "        self.out = tf.keras.layers.Dense(10) # output layer 10 neurons\n",
    "\n",
    "    def call(self, inputs): # forward pass\n",
    "        x = self.dense1(inputs) # pass the inputs through the first layer\n",
    "        return self.out(x) # pass the output of the first layer through the output layer\n",
    "\n",
    "\"\"\" \n",
    "Model evaluation and prediction\n",
    "- after training the model, we can evaluate the model on the test data to see how well it performs.\n",
    "\n",
    "- the evaluate method takes the test data and labels as input and returns the loss and accuracy of the model on the test data.\n",
    "- the predict method takes the test data as input and returns the predicted labels of the model on the test data.\n",
    "\"\"\"\n",
    "model.evaluate(train_data, train_labels)\n",
    "new_data = np.random.rand(10, 784) # 10 samples of 784 features for a image this will be the new unseen data\n",
    "model.predict(new_data) \n",
    "# output will be a 10x10 matrix of probabilities for each class\n",
    "#EX run\n",
    "\"\"\" \n",
    "array([[0.07370737, 0.06734557, 0.09007742, 0.09084072, 0.15472364,\n",
    "        0.10088342, 0.15501715, 0.0846708 , 0.06713989, 0.11559402],\n",
    "       [0.13559042, 0.08671413, 0.06254287, 0.10968324, 0.10897283,\n",
    "        0.06311018, 0.140859  , 0.08376517, 0.05650004, 0.1522621 ],\n",
    "       [0.06658942, 0.06215975, 0.12574631, 0.08794341, 0.24458086,\n",
    "        0.15032104, 0.03119767, 0.06260405, 0.07711761, 0.09173983],\n",
    "       [0.18518034, 0.03801233, 0.10225594, 0.05082671, 0.14458576,\n",
    "        0.08729083, 0.1261759 , 0.06465442, 0.07568176, 0.12533602],\n",
    "       [0.25580242, 0.0704459 , 0.09154338, 0.12910502, 0.15939018,\n",
    "        0.06951969, 0.07261899, 0.05154628, 0.06342862, 0.03659946],\n",
    "       [0.16323559, 0.06115254, 0.11572555, 0.0481673 , 0.2106075 ,\n",
    "        0.08192023, 0.08052816, 0.06937485, 0.05051486, 0.11877337],\n",
    "       [0.08927758, 0.09356026, 0.07743935, 0.04602603, 0.05058305,\n",
    "        0.2109955 , 0.1288731 , 0.13315801, 0.12589532, 0.04419181],\n",
    "       [0.12444694, 0.21631704, 0.09856055, 0.12358341, 0.12699795,\n",
    "        0.04524506, 0.07598381, 0.09751168, 0.03933394, 0.0520197 ],\n",
    "       [0.14829713, 0.12787852, 0.04917813, 0.08518186, 0.20955724,\n",
    "        0.06541336, 0.12108779, 0.07558911, 0.05305878, 0.06475811],\n",
    "       [0.09848133, 0.1336912 , 0.06284648, 0.1111226 , 0.17866668,\n",
    "        0.10355557, 0.10615086, 0.0537717 , 0.06240985, 0.08930375]],\n",
    "      dtype=float32)\n",
    "      \n",
    "This has 10 rows and 10 columns, each row is a sample and each column is a class.\n",
    "for Example the first row is the prediction for the first sample, and the first column is the probability of class 0.\n",
    "in simple tearms for the number 0 the model is 7% sure it is a 0, 6% sure it is a 1, 9% sure it is a 2, and so on.\n",
    "the second row then dose this again fro numbers 0-9 then dose it again for the third row and so on.\n",
    "since we had 10 samples, we have 10 rows and 10 columns for image 0-9 10 classes.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "Handling Missing Data, a few ways to handle missing data:\n",
    "- Drop the rows with missing data\n",
    "- Fill the missing data with a value\n",
    "- Fill the missing data with the mean, median, or mode of the column\n",
    "- Fill the missing data with the previous or next value in the column (forward or backward fill)\n",
    "- Fill the missing data with a value from another column\n",
    "- Fill the missing data with a value from a different dataset\n",
    "- Fill the missing data with a value from the sorrounding data (interpolation)\n",
    "\"\"\"\n",
    "x = tf.constant([1.0, float('nan'), 2.0, float('nan')]) # creating a tensor with missing data\n",
    "x_clean = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x) # replace the missing data with 0\n",
    "print(x_clean)  # [1.0, 0.0, 2.0, 0.0] \n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Tensor Boards\n",
    "- TensorBoard is a tool for visualizing the training process of a model.\n",
    "\n",
    "The ex below will save the running data of the model to a log directory, and then we can use TensorBoard to visualize the data.\n",
    "\"\"\"\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = \"logs/fit/\" # directory to save the logs\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir) # create a TensorBoard callback meaning it will save the logs to the directory\n",
    "model.fit(train_data, train_labels, epochs=10, callbacks=[tensorboard_callback]) # using the same data and labels as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visulize the tf logs using tensorboard \n",
    "# # option 1) tensorboard --logdir=logs/fit (will runn on localhost:6006)\n",
    "# option 2) within jupyter notebook\n",
    "%load_ext tensorboard # load the tensorboard extension\n",
    "%tensorboard --logdir logs/fit # run tensorboard in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 1. Dummy data for an image classification problem\n",
    "train_data = np.random.rand(1000, 784)  # 1000 samples, 784 features each\n",
    "train_labels = np.random.randint(10, size=(1000,))  # 1000 labels (classes 0-9)\n",
    "batch_size = 32\n",
    "\n",
    "# Create a TensorFlow dataset and batch it\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size)\n",
    "\n",
    "# 2. Define the model\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')  # hidden layer\n",
    "        self.out = tf.keras.layers.Dense(10)  # output layer (10 classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.out(x)\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# 3. Define a custom loss function\n",
    "# Warning: your custom loss is MSE (good for regression, not classification!)\n",
    "# Normally for classification you use SparseCategoricalCrossentropy\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - tf.one_hot(y_true, depth=10)))\n",
    "\n",
    "# 4. Fine-tuning pretrained model (example only, not used here)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 5. Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 6. Custom training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch)\n",
    "            loss = custom_loss(y_batch, logits)  # notice: y_batch vs logits, a logit is the output of the model before the softmax function is applied \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
    "\n",
    "print(\"Training complete ‚úÖ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
