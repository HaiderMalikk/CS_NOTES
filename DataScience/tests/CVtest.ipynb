{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip3 install opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246af6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenCV if needed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "# Function to download and save an image if it doesn't exist\n",
    "def get_sample_image(url, filename=\"sample_image.jpg\"):\n",
    "    if not os.path.exists(filename):\n",
    "        with urlopen(url) as response:\n",
    "            image_data = response.read()\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(image_data)\n",
    "    return filename\n",
    "\n",
    "# Download a sample image\n",
    "image_path = get_sample_image(\"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/fruits.jpg\")\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 1. Harris Corner Detection\n",
    "gray_float = np.float32(gray)\n",
    "corners = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)\n",
    "# Dilate to mark the corners\n",
    "corners = cv2.dilate(corners, None)\n",
    "# Create a copy of the original image\n",
    "img_harris = img.copy()\n",
    "# Mark corners with red color (threshold for best corners)\n",
    "img_harris[corners > 0.01 * corners.max()] = [0, 0, 255]\n",
    "\n",
    "# 2. Shi-Tomasi Corner Detection\n",
    "corners_st = cv2.goodFeaturesToTrack(gray, maxCorners=50, qualityLevel=0.01, minDistance=10)\n",
    "img_shi_tomasi = img.copy()\n",
    "# Draw circles around detected corners\n",
    "if corners_st is not None:\n",
    "    for corner in corners_st:\n",
    "        x, y = corner.ravel()\n",
    "        cv2.circle(img_shi_tomasi, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# 3. SIFT (Scale-Invariant Feature Transform)\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints_sift = sift.detect(gray, None)\n",
    "img_sift = cv2.drawKeypoints(img, keypoints_sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# 4. ORB (Oriented FAST and Rotated BRIEF)\n",
    "orb = cv2.ORB_create(nfeatures=200)\n",
    "keypoints_orb = orb.detect(gray, None)\n",
    "img_orb = cv2.drawKeypoints(img, keypoints_orb, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# 5. Canny Edge Detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Convert BGR to RGB for matplotlib display\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_harris_rgb = cv2.cvtColor(img_harris, cv2.COLOR_BGR2RGB)\n",
    "img_shi_tomasi_rgb = cv2.cvtColor(img_shi_tomasi, cv2.COLOR_BGR2RGB)\n",
    "img_sift_rgb = cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB)\n",
    "img_orb_rgb = cv2.cvtColor(img_orb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(img_harris_rgb)\n",
    "plt.title('Harris Corner Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(img_shi_tomasi_rgb)\n",
    "plt.title('Shi-Tomasi Corner Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(img_sift_rgb)\n",
    "plt.title('SIFT Features')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(img_orb_rgb)\n",
    "plt.title('ORB Features')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Canny Edge Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Function to predict what's in the image\n",
    "def predict_image_content(image_path):\n",
    "    # Read and preprocess the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_array = np.expand_dims(img_resized, axis=0)\n",
    "    img_preprocessed = preprocess_input(img_array)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_preprocessed)\n",
    "    decoded = decode_predictions(predictions, top=5)[0]\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh([pred[1] for pred in decoded], [pred[2] for pred in decoded])\n",
    "    plt.xlabel('Probability')\n",
    "    plt.title('Top 5 Predictions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top prediction\n",
    "    return f\"This is most likely a {decoded[0][1]} ({decoded[0][2]*100:.2f}% confidence)\"\n",
    "\n",
    "# Use the same image from your previous code\n",
    "result = predict_image_content(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff311a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ResNet50(weights='imagenet') # ResNet50 is a deep learning model trained on the ImageNet dataset, which contains millions of images across thousands of categories. This model is capable of recognizing a wide variety of objects and scenes in images.\n",
    "\n",
    "def process_frame(frame): # each time we capture a frame this function will be called\n",
    "    # Preprocess the frame for the model\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert BGR to RGB BGR is default in OpenCV it is Blue Green Red\n",
    "    resized = cv2.resize(frame_rgb, (224, 224)) # resize to 224x224 as required by ResNet50\n",
    "    img_array = np.expand_dims(resized, axis=0) # add batch dimension, so before its: (224, 224, 3) now its (1, 224, 224, 3) This is necessary because neural networks like ResNet50 expect inputs in batches. The added dimension represents a batch of size 1 (single image). Without this, the model would throw an error because it expects a 4D tensor input with the first dimension being the batch size. Think of it as putting your single image into a \"batch\" so the model can process it, even though you're only sending one image at a time.\n",
    "    preprocessed = preprocess_input(img_array) # preprocess the image for ResNet50 this function normalizes the image data to the range that the model was trained on and also applies any other preprocessing steps that the model requires to make sure each image is in the correct format.\n",
    "    \n",
    "    # Make prediction\n",
    "    preds = model.predict(preprocessed) # use the model to predict the class of the image\n",
    "    decoded = decode_predictions(preds, top=3)[0] # get the top 3 predictions for the image\n",
    "    \n",
    "    return decoded # return the decoded predictions\n",
    "\n",
    "def run_video_detection(source=0):  # 0 for webcam, or provide a video file path\n",
    "    # Initialize video capture source\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened(): # if nothing is detected then no image source is opened\n",
    "        print(\"Error: Could not open video source\")\n",
    "        return\n",
    "    \n",
    "    # Set a reasonable frame size this is the size of the frame where we see the video\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    # Process frames\n",
    "    last_prediction_time = time.time() # Initialize last prediction time which would be when the function is called\n",
    "    prediction_interval = 1.0  # seconds between predictions (to avoid lag)\n",
    "    current_predictions = None  # Store the current predictions current_predictions is None at the start\n",
    "    \n",
    "    # keep reading frames from the video source\n",
    "    while True:\n",
    "        ret, frame = cap.read() # read a frame from the video source, ret is a boolean indicating if the frame was read successfully\n",
    "        if not ret: # if we got nothing from the video source then break\n",
    "            break\n",
    "            \n",
    "        # Make prediction only every few frames to improve performance \n",
    "        current_time = time.time() # get the current time\n",
    "        if current_time - last_prediction_time > prediction_interval: # if the time since the last prediction is greater than the prediction interval\n",
    "            # Update predictions\n",
    "            current_predictions = process_frame(frame) # process the current frame and get predictions for it\n",
    "            last_prediction_time = current_time # update the last prediction time to the current time as we made a prediction\n",
    "            \n",
    "        # Always display the current predictions (even for frames we don't process, to avoid blinking text)\n",
    "        if current_predictions is not None: # as long as we have predictions\n",
    "            # Draw predictions on the frame (top 3 predictions)\n",
    "            y_pos = 30\n",
    "            for i, (_, label, score) in enumerate(current_predictions):\n",
    "                text = f\"{label}: {score:.2f}\"\n",
    "                # Use a contrasting outline for better visibility\n",
    "                cv2.putText(frame, text, (10, y_pos + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 4)  # Black outline\n",
    "                cv2.putText(frame, text, (10, y_pos + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)  # Green text\n",
    "        else: # if we don't have predictions yet\n",
    "            cv2.putText(frame, \"Initializing...\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            \n",
    "        # Display frame\n",
    "        cv2.imshow('Real-time Object Detection', frame)\n",
    "        \n",
    "        # Exit on 'q' press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run with webcam (use 0 for default camera)\n",
    "# To use a video file instead, replace 0 with the path to your video file\n",
    "run_video_detection(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
