{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip3 install opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246af6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenCV if needed\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "\n",
    "# Function to download and save an image if it doesn't exist\n",
    "def get_sample_image(url, filename=\"sample_image.jpg\"):\n",
    "    with urlopen(url) as response:\n",
    "        image_data = response.read()\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    return filename\n",
    "\n",
    "# Download a sample image\n",
    "image_path = get_sample_image(\"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg\")\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 1. Harris Corner Detection\n",
    "gray_float = np.float32(gray)\n",
    "corners = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)\n",
    "# Dilate to mark the corners\n",
    "corners = cv2.dilate(corners, None)\n",
    "# Create a copy of the original image\n",
    "img_harris = img.copy()\n",
    "# Mark corners with red color (threshold for best corners)\n",
    "img_harris[corners > 0.01 * corners.max()] = [0, 0, 255]\n",
    "\n",
    "# 2. Shi-Tomasi Corner Detection\n",
    "corners_st = cv2.goodFeaturesToTrack(gray, maxCorners=50, qualityLevel=0.01, minDistance=10)\n",
    "img_shi_tomasi = img.copy()\n",
    "# Draw circles around detected corners\n",
    "if corners_st is not None:\n",
    "    for corner in corners_st:\n",
    "        x, y = corner.ravel()\n",
    "        cv2.circle(img_shi_tomasi, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "# 3. SIFT (Scale-Invariant Feature Transform)\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints_sift = sift.detect(gray, None)\n",
    "img_sift = cv2.drawKeypoints(img, keypoints_sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# 4. ORB (Oriented FAST and Rotated BRIEF)\n",
    "orb = cv2.ORB_create(nfeatures=200)\n",
    "keypoints_orb = orb.detect(gray, None)\n",
    "img_orb = cv2.drawKeypoints(img, keypoints_orb, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# 5. Canny Edge Detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Convert BGR to RGB for matplotlib display\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_harris_rgb = cv2.cvtColor(img_harris, cv2.COLOR_BGR2RGB)\n",
    "img_shi_tomasi_rgb = cv2.cvtColor(img_shi_tomasi, cv2.COLOR_BGR2RGB)\n",
    "img_sift_rgb = cv2.cvtColor(img_sift, cv2.COLOR_BGR2RGB)\n",
    "img_orb_rgb = cv2.cvtColor(img_orb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the results\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(img_harris_rgb)\n",
    "plt.title('Harris Corner Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(img_shi_tomasi_rgb)\n",
    "plt.title('Shi-Tomasi Corner Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(img_sift_rgb)\n",
    "plt.title('SIFT Features')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(img_orb_rgb)\n",
    "plt.title('ORB Features')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.title('Canny Edge Detection')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "def analyze_image_with_vqa(image_path):\n",
    "    # Load model and processor\n",
    "    processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "    model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Define questions to ask about the image\n",
    "    questions = [\n",
    "        \"What is shown in this image?\",\n",
    "        \"What objects are visible in this image?\",\n",
    "        \"Where was this photo taken?\",\n",
    "        \"What is happening in this scene?\",\n",
    "        \"What colors are dominant in this image?\",\n",
    "        \"Are there any people in this image?\"\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each question\n",
    "    for question in questions:\n",
    "        # Prepare inputs\n",
    "        inputs = processor(image, question, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get model prediction\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted answer\n",
    "        idx = logits.argmax(-1).item()\n",
    "        answer = model.config.id2label[idx]\n",
    "        \n",
    "        results[question] = answer\n",
    "    \n",
    "    # Create visualization\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Generate a comprehensive description from the answers\n",
    "    description = f\"This image shows {results['What is shown in this image?']}. \"\n",
    "    description += f\"The visible objects include {results['What objects are visible in this image?']}. \"\n",
    "    \n",
    "    if results['Are there any people in this image?'].lower() not in ['no', 'none', '0']:\n",
    "        description += f\"There are people in the image. \"\n",
    "        \n",
    "    description += f\"The scene depicts {results['What is happening in this scene?']}. \"\n",
    "    description += f\"The dominant colors are {results['What colors are dominant in this image?']}. \"\n",
    "    \n",
    "    if results['Where was this photo taken?'] not in ['unknown', 'not sure']:\n",
    "        description += f\"This appears to be taken at {results['Where was this photo taken?']}.\"\n",
    "    \n",
    "    # Create visual display\n",
    "    img_annotated = img.copy()\n",
    "    h, w = img_annotated.shape[:2]\n",
    "    \n",
    "    # Add overlay\n",
    "    overlay = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    overlay_height = min(h // 2, 200)  # Larger overlay for more text\n",
    "    alpha = 0.7\n",
    "    \n",
    "    img_annotated[h-overlay_height:h, :] = cv2.addWeighted(\n",
    "        img_annotated[h-overlay_height:h, :], 1-alpha, \n",
    "        overlay[h-overlay_height:h, :], alpha, 0\n",
    "    )\n",
    "    \n",
    "    # Add description to image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "    text_color = (255, 255, 255)\n",
    "    \n",
    "    # Wrap text\n",
    "    wrapped_description = textwrap.wrap(description, width=60)\n",
    "    \n",
    "    y_pos = h - overlay_height + 20\n",
    "    for line in wrapped_description:\n",
    "        cv2.putText(img_annotated, line, (10, y_pos), font, font_scale, text_color, font_thickness)\n",
    "        y_pos += 20\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(img_annotated)\n",
    "    plt.title('Image with Analysis')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show raw QA results\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.axis('off')\n",
    "    y_pos = 0.95\n",
    "    for question, answer in results.items():\n",
    "        plt.text(0.1, y_pos, f\"Q: {question}\", fontsize=10, fontweight='bold')\n",
    "        plt.text(0.1, y_pos-0.05, f\"A: {answer}\", fontsize=9)\n",
    "        y_pos -= 0.15\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Install required package (only once)\n",
    "# !pip install transformers pillow\n",
    "\n",
    "result = analyze_image_with_vqa(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff311a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import textwrap\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "# Load VQA model and processor\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Define questions to ask about each frame\n",
    "questions = [\n",
    "    \"What is shown in this image?\",\n",
    "    \"What objects are visible?\",\n",
    "    \"What is happening in this scene?\"\n",
    "]\n",
    "\n",
    "def process_frame_vqa(frame):\n",
    "    \"\"\"Process frame using VQA model to get detailed description\"\"\"\n",
    "    # Convert OpenCV BGR to RGB and then to PIL Image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each question (limit to 3 for performance)\n",
    "    for question in questions:\n",
    "        # Prepare inputs\n",
    "        inputs = processor(pil_image, question, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted answer\n",
    "        idx = logits.argmax(-1).item()\n",
    "        answer = model.config.id2label[idx]\n",
    "        \n",
    "        results[question] = answer\n",
    "    \n",
    "    # Generate a concise description from the answers\n",
    "    description = f\"This shows {results['What is shown in this image?']}. \"\n",
    "    description += f\"Objects: {results['What objects are visible?']}. \"\n",
    "    description += f\"Scene: {results['What is happening in this scene?']}.\"\n",
    "    \n",
    "    return results, description\n",
    "\n",
    "def run_video_detection_vqa(source=0):\n",
    "    # Initialize video capture source\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source\")\n",
    "        return\n",
    "    \n",
    "    # Set frame size\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    # Process frames\n",
    "    last_prediction_time = time.time()\n",
    "    prediction_interval = 2.0  # VQA is more computationally intensive, so we process less frequently\n",
    "    current_results = None\n",
    "    current_description = None\n",
    "    \n",
    "    print(\"Starting video analysis. Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Make prediction at intervals\n",
    "        current_time = time.time()\n",
    "        if current_time - last_prediction_time > prediction_interval:\n",
    "            # Update predictions\n",
    "            current_results, current_description = process_frame_vqa(frame)\n",
    "            last_prediction_time = current_time\n",
    "            \n",
    "        # Display results on frame\n",
    "        if current_results is not None:\n",
    "            # Add semi-transparent overlay at the bottom\n",
    "            h, w = frame.shape[:2]\n",
    "            overlay = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "            overlay_height = min(h // 3, 150)\n",
    "            alpha = 0.7\n",
    "            \n",
    "            # Create a copy of frame to modify\n",
    "            display_frame = frame.copy()\n",
    "            \n",
    "            # Apply overlay\n",
    "            display_frame[h-overlay_height:h, :] = cv2.addWeighted(\n",
    "                display_frame[h-overlay_height:h, :], 1-alpha,\n",
    "                overlay[h-overlay_height:h, :], alpha, 0\n",
    "            )\n",
    "            \n",
    "            # Add description text\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.5\n",
    "            font_thickness = 1\n",
    "            text_color = (255, 255, 255)\n",
    "            \n",
    "            # Wrap text for better display\n",
    "            wrapped_text = textwrap.wrap(current_description, width=60)\n",
    "            \n",
    "            y_pos = h - overlay_height + 20\n",
    "            for line in wrapped_text:\n",
    "                cv2.putText(display_frame, line, (10, y_pos), font, font_scale, text_color, font_thickness)\n",
    "                y_pos += 20\n",
    "                \n",
    "            # Show individual Q&A results at the top\n",
    "            y_pos = 30\n",
    "            for question, answer in current_results.items():\n",
    "                # Display in simplified format\n",
    "                q_text = question.replace(\"What is shown in this image?\", \"Content:\")\n",
    "                q_text = q_text.replace(\"What objects are visible?\", \"Objects:\")\n",
    "                q_text = q_text.replace(\"What is happening in this scene?\", \"Action:\")\n",
    "                \n",
    "                text = f\"{q_text} {answer}\"\n",
    "                # Add black outline for visibility\n",
    "                cv2.putText(display_frame, text, (10, y_pos), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3)  # Black outline\n",
    "                cv2.putText(display_frame, text, (10, y_pos), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)  # White text\n",
    "                y_pos += 25\n",
    "        else:\n",
    "            display_frame = frame.copy()\n",
    "            cv2.putText(display_frame, \"Analyzing scene...\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            \n",
    "        # Display the processed frame\n",
    "        cv2.imshow('Real-time Scene Understanding', display_frame)\n",
    "        \n",
    "        # Exit on 'q' press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run with webcam\n",
    "run_video_detection_vqa(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
