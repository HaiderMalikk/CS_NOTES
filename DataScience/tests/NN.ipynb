{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8c9b23",
   "metadata": {},
   "source": [
    "# Image recorgition in pytorch using mnist DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32679c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # import torch library\n",
    "import torch.nn as nn # import nn module from torch library\n",
    "import torch.optim as optim # import optim module from torch library to use optimizers\n",
    "import torchvision # import torchvision library to use datasets and transforms (gets our mnist dataset)\n",
    "import torchvision.transforms as transforms # import transforms module from torchvision library to use transforms (to transform the images to tensors and normalize them)\n",
    "\n",
    "# 1. Load MNIST dataset\n",
    "\"\"\" \n",
    "    Load MNIST dataset\n",
    "        ↓\n",
    "    Apply transform = transforms.Compose([...])\n",
    "        ↓\n",
    "    First Step: ToTensor()\n",
    "        ↓\n",
    "        - Converts each image into a Tensor.\n",
    "        - Scales pixel values from [0, 255] → [0.0, 1.0].\n",
    "        (Every pixel divided by 255 automatically.)\n",
    "        - Changes shape: (28, 28) → (1, 28, 28)\n",
    "        (adds a channel dimension for grayscale images.)\n",
    "\n",
    "    Example:\n",
    "        Original pixel (pure black) = 0 → 0.0\n",
    "        Original pixel (pure white) = 255 → 1.0\n",
    "        Original pixel (gray 128) ≈ 0.502\n",
    "\n",
    "    Resulting Tensor Shape:\n",
    "        (batch_size, 1, 28, 28)\n",
    "        ↓\n",
    "        \n",
    "    Second Step: Normalize(mean=0.5, std=0.5)\n",
    "        ↓\n",
    "        - Shifts and scales values from [0, 1] → [-1, 1].\n",
    "        - Formula: (pixel_value - mean) / std\n",
    "        (pixel_value - 0.5) / 0.5\n",
    "\n",
    "    Example:\n",
    "        Pixel 0.0 → (0.0 - 0.5)/0.5 = -1.0  (black)\n",
    "        Pixel 1.0 → (1.0 - 0.5)/0.5 = 1.0   (white)\n",
    "        Pixel 0.5 → (0.5 - 0.5)/0.5 = 0.0   (gray)\n",
    "        Pixel 0.61 → (0.61 - 0.5)/0.5 = 0.22 (gray)\n",
    "\n",
    "    Why Normalize?\n",
    "        - Makes training easier: neural networks work better when inputs are small and centered around 0.\n",
    "        - Helps the model converge faster.\n",
    "\n",
    "    Resulting Tensor Values:\n",
    "        Range: [-1.0, 1.0]\n",
    "\n",
    "        ↓\n",
    "    Pass the normalized tensors into the Neural Network!\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    # this next TOtesnor dose 2 things \n",
    "    # it gets that first dimention the grayscale value of the pixel, each original images pixel is between 0 and 255 (0 black and 255 white) .ToTensor divides every pixel by 255 to get the value between 0 and 1 \n",
    "    # it then convert images (28x28) to tensor of shape (1, 28, 28) meaning a 1x28x28 tensor. 28x28 pixels = 784 pixels with 1 channel (grayscale i.e black/white 1 or 0 or in between) which is what we did in the last line\n",
    "    transforms.ToTensor(),  \n",
    "    # in the next step we normalize our image. from the prevois step our greyscale value is between 0-1 but it can be any value between 0 and 1 by normalizing we convert this range [-1, 1] to make it easier for the model to learn\n",
    "    # we always want some common small scale for all images to be between, usally centered around 0. in our case here we normalize to -1 and 1 so all images intensity values are between -1 and 1\n",
    "    # meaning we want, black images (0) == -1 and white images (255) == 1 and grayscale images (128) == 0. we do this by subtracting the mean from the original value and dividing by the standard deviation \n",
    "    # thats why we use 0.5 and 0.5 as the mean and std for the images (this is a common value used in deep learning) this is done to normalize the image to a common scale, if we use 0.5 and 0.5 as the mean and std, a value between 0 and 1 will be between -1 and 1\n",
    "    # Normalized-Value=Original Value−Mean/Standard Deviation. EX: black image (0 value after .ToTensor) -> (0-0.5)/0.5 = -1, white image (255 value after .ToTensor) -> (1-0.5)/0.5 = 1, grayscale image (128 value after .ToTensor) -> (0.5-0.5)/0.5 = 0\n",
    "    # if the values in between: lets say after .ToTensor our pixel (156) then it will first be between 0 and 1, 156/255 = 0.61, now we normalize -> (0.61-0.5)/0.5 = 0.22, so we get a value of 0.22 which is between -1 and 1 (normalized), so we can use this value to represent the pixel in the image\n",
    "    # we do this so that we can train our model faster and more accurately because the model has to deal with a smaller range, it also helps gradient descent and prevents bias in the model towards larger values. it also helps the model update the weights and biases faster and more accurately\n",
    "    # NOTE: the last step is to place this normalized value in place of the old pixels values and construct the 1x28x28 image with the normalized values.\n",
    "    # so it will look like: a 3d tensor but since the color channel is 1 its just a 2d tensor with 28 rows and 28 columns and 1 dimension for the color channel so it still looks like a 2d tensor .wrapped in some extra brackets all inside the tensor()\n",
    "    # tensor([[[-1.0, -1.0, -1.0, ..., -1.0, -1.0, -1.0],\n",
    "    #          [-1.0, -1.0, -1.0, ..., -1.0, -1.0, -1.0],\n",
    "    #         [-1.0, -0.23, 0.22, ..., 0.23, -0.11, -1.0],\n",
    "    #         ...,\n",
    "    #         [-1.0, -1.0, -1.0, ..., -1.0, -1.0, -1.0]]])\n",
    "    transforms.Normalize((0.5,), (0.5,))  # normalize to [-1,1] and mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# 2. Load the dataset (one for training and one for testing we use 2 so that the model sees unseen data and can generalize better)\n",
    "\"\"\" \n",
    "    Step 1: Load Training and Test Data\n",
    "        ↓\n",
    "    Use torchvision.datasets.MNIST()\n",
    "        ↓\n",
    "        - Downloads the MNIST dataset (handwritten digits 0-9) if not already available.\n",
    "        - `train=True`: load 60,000 training images.\n",
    "        - `train=False`: load 10,000 testing images.\n",
    "        - Applies the `transform` (ToTensor + Normalize) we defined earlier.\n",
    "\n",
    "    Dataset structure after loading:\n",
    "        - Each sample = (image, label)\n",
    "        - `image`: 28x28 normalized tensor (single-channel grayscale image)\n",
    "        - `label`: integer (0-9) — the actual digit.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 2: Create DataLoaders\n",
    "        ↓\n",
    "    Use torch.utils.data.DataLoader()\n",
    "        ↓\n",
    "        - Breaks the dataset into small batches (mini-batches) for training/testing.\n",
    "        - Helps iterate over the dataset efficiently.\n",
    "        - Avoids loading entire data into memory at once (memory-efficient).\n",
    "\n",
    "    Parameters:\n",
    "        - `batch_size=64` → 64 images per batch (training).\n",
    "        - `batch_size=1` → 1 image per batch (testing).\n",
    "        - `shuffle=True` → randomizes the data order every epoch (important to prevent overfitting).\n",
    "\n",
    "    How the DataLoader outputs look:\n",
    "        - Each batch: (batch_images, batch_labels)\n",
    "        - batch_images: Tensor of shape (batch_size, 1, 28, 28)\n",
    "        - batch_labels: Tensor of shape (batch_size,)\n",
    "        \n",
    "        ↓\n",
    "\n",
    "    Usally we keep the test set separate from the training set so that we can test the model on unseen data and see how well it generalizes to new data. this is important because we want to see if the model can predict the labels of new data that it has not seen before.\n",
    "    in our case since the mnist is 60000 and our trainset is also 60000 we still achive randomness from some 10000 picked images from the 60000 mnist has because we shuffle on so the train set and test set seem random\n",
    "    Now ready to train and test the model!\n",
    "\"\"\"\n",
    "# in the 2 ex's below we first load the data and then pass that training data into a data loader which is a way to iterate over the data in batches\n",
    "# for ex the mnist Data set loads a lot of data so we want to load it in batches and feed our model in batches that way we don't have to load the whole dataset into memory\n",
    "# the MNIST dataset has a total of 60k images, but we use only 64 images at a time to train the model (this is called batch size and it prevents too many images from being loaded into memory at once) and we shuffle the data every epoch (this is done to prevent overfitting) \n",
    "# the loded data looks like: (data, labels) where data = tensor([[0, 0, 0.1, ...], [0, 0, 0.1, ...] ....]) a 28x28 matrix of images and labels = tensor([0, 1, 2, ...]) a 1d tensor of labels (0-9) for the images. each image has a label (0-9) for the digit in the image\n",
    "# so the set of data which is the images and labels is passed to the loader which loads the data in baches of 64 images at a time and shuffles the data every epoch\n",
    "# the loadder itself is a iterable with the form image, lable where each image (a 28x28 normalized matrix) would have a corresponding label (0-9) for the digit in the image. so we can use this to train our model\n",
    "trainset = torchvision.datasets.MNIST(root='./assets/data', train=True, download=True, transform=transform) # download=True will download the dataset if not already present in the data folder\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # batch_size=64 means that we will train on 64 images at a time and shuffle=True means that we will shuffle the images every epoch\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./assets/data', train=False, download=True, transform=transform) # we load into the data folder the mnist dataset (a dataset of handwritten digits 0-9)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)  # batch_size=1 for one image at a time and shuffle=True\n",
    "\n",
    "# 3. Define a simple NN\n",
    "\"\"\" \n",
    "    Input (batch_size, 28, 28)   \n",
    "    # (batch of images, each 28x28 pixels)\n",
    "\n",
    "            ↓ (Flatten with .view())   \n",
    "            # Using x = x.view(-1, 28*28)\n",
    "            # Converts each 2D image into a 1D vector.\n",
    "            \n",
    "    (batch_size, 784)   \n",
    "    # Now each image is a vector of 784 pixels.\n",
    "\n",
    "            ↓ (Linear Layer: 784 → 128)   \n",
    "            # First layer: Applies weights and biases.\n",
    "            # Each of the 784 inputs is connected to each of the 128 neurons.\n",
    "            # Weights are applied here**.\n",
    "            # The network starts **detecting basic patterns like edges** here.\n",
    "\n",
    "    (batch_size, 128)   \n",
    "    # Output after applying linear transformation (Wx + b).\n",
    "\n",
    "            ↓ (ReLU activation)   \n",
    "            # Non-linearity introduced.\n",
    "            # Turns negative values into 0, keeps positives.\n",
    "            # No new weights, just changing the activations.\n",
    "\n",
    "    (batch_size, 128)   \n",
    "    # Activated output.\n",
    "\n",
    "            ↓ (Linear Layer: 128 → 64)   \n",
    "            # Second layer: Applies new weights and biases.\n",
    "            # Each of the 128 outputs is fully connected to 64 neurons.\n",
    "            # Weights are applied here again.\n",
    "            # Now the model **builds on previous features to detect more complex shapes** (like corners, parts of digits).\n",
    "\n",
    "    (batch_size, 64)   \n",
    "    # Output after second linear transformation.\n",
    "\n",
    "            ↓ (ReLU activation)   \n",
    "            # Again, non-linearity.\n",
    "            # Keeps the learning flexible (otherwise it would just be linear stacking).\n",
    "\n",
    "    (batch_size, 64)   \n",
    "    # Activated output.\n",
    "\n",
    "            ↓ (Linear Layer: 64 → 10)   \n",
    "            # Final layer: Fully connected layer to 10 output neurons.\n",
    "            # Weights applied here again.\n",
    "            # Each output neuron represents a score (logit) for one digit class (0-9).\n",
    "\n",
    "    (batch_size, 10)   \n",
    "    # Output logits (raw scores).\n",
    "\n",
    "            ↓ (Output logits)   \n",
    "            # Example: [ [0.5, -1.2, 3.4, ..., -0.8] ] index of logit = predicted class (digit0-9).\n",
    "            # Each number represents the **confidence** (before softmax) for that class.\n",
    "            # We usually apply **softmax** after this (during prediction) to convert logits to probabilities.\n",
    "            \n",
    "    ### The actual NN Class:\n",
    "        - The constructor initializes the layers and their parameters.\n",
    "        - The forward method (Forward Pass/Propagation) defines how data flows through the network from input to output passing through each layer defined in the constructor.\n",
    "            \n",
    "    ### Network Diagram for the First Layer:\n",
    "    \n",
    "                +-----------------------+\n",
    "                |      Input Layer      | (784 pixels initially)\n",
    "                +-----------------------+\n",
    "                             |\n",
    "                             |\n",
    "                     x1   x2  ...  x784 (784 inputs)\n",
    "                             |\n",
    "                             |\n",
    "                             v\n",
    "             +---------------------------------+\n",
    "             |      Weights (w1, w2, ...)      |   (weights for first layer, learned through training), \n",
    "             +---------------------------------+\n",
    "                             |\n",
    "                             |\n",
    "                             v\n",
    "              +-------------------------------+\n",
    "              | Weighted Sum (Σ(w * x) + b))  | (bias is added)\n",
    "              +-------------------------------+\n",
    "                             |\n",
    "                             v\n",
    "              +----------------------------+\n",
    "              |  Activation Function (ReLU)| (ReLU = Rectified Linear Unit = max(0, x) = x if x > 0, 0 otherwise)\n",
    "              +----------------------------+\n",
    "                             |\n",
    "                             v\n",
    "                  +---------------------+\n",
    "                  |     Output (Neuron) | (128 neurons will be teh next layer's input)\n",
    "                  +---------------------+\n",
    "\n",
    "    ### Detailed Explanation for Each Neuron:\n",
    "    For each neuron, the process is as follows:\n",
    "    - **Input**: The neuron receives the inputs from the previous layer (or directly from the image for the first layer).\n",
    "    - **Weights**: Each input is multiplied by a corresponding weight.\n",
    "    - **Weighted Sum**: The weighted sum of inputs is calculated.\n",
    "    - **Bias**: A bias is added to the weighted sum.\n",
    "    - **Activation**: The weighted sum (plus bias) is passed through an activation function (like ReLU) to get the output of the neuron.\n",
    "\n",
    "    This process is repeated for every neuron in the network. For example, in the first hidden layer, there are 128 neurons, each performing the same steps with its own weights and bias.\n",
    "\n",
    "    ### Example for Neural Network (with 3 neurons and 4 inputs):\n",
    "    Consider a simplified network with 3 neurons and 4 inputs. Let's assume the input to the layer is a vector of size 4:\n",
    "    we want to go from 4 inputs to 3 outputs where each input and output is a neuron so we go from 4 neurons to 3 neurons.\n",
    "    then the weight matrix for the first layer would be of size 4x3 (4 inputs and 3 neurons) and the bias vector would be of size 3 (1 bias for each neuron).\n",
    "    these diementions are chosen so each neuron has its own weights and bias to learn from the inputs.        \n",
    "\n",
    "        X = [x1, x2, x3, x4]  # Input vector (4 features)\n",
    "        \n",
    "        W = [w11, w12, w13] # Weight matrix (4x3)\n",
    "            [w21, w22, w23]\n",
    "            [w31, w32, w33]\n",
    "            [w41, w42, w43]\n",
    "            \n",
    "        b = [b1, b2, b3]  # Bias vector (3 biases for 3 neurons)\n",
    "\n",
    "    Each neuron in this layer will compute a weighted sum and pass it through ReLU activation:\n",
    "\n",
    "    - **Neuron 1**:\n",
    "        - Weights: [w1, w2, w3, w4]\n",
    "        - Bias: b1\n",
    "        - Weighted sum: z1 = (w1*x1) + (w2*x2) + (w3*x3) + (w4*x4) + b1\n",
    "        - Output: A1 = ReLU(z1)\n",
    "\n",
    "    - **Neuron 2**:\n",
    "        - Weights: [w5, w6, w7, w8]\n",
    "        - Bias: b2\n",
    "        - Weighted sum: z2 = (w5*x1) + (w6*x2) + (w7*x3) + (w8*x4) + b2\n",
    "        - Output: A2 = ReLU(z2)\n",
    "\n",
    "    - **Neuron 3**:\n",
    "        - Weights: [w9, w10, w11, w12]\n",
    "        - Bias: b3\n",
    "        - Weighted sum: z3 = (w9*x1) + (w10*x2) + (w11*x3) + (w12*x4) + b3\n",
    "        - Output: A3 = ReLU(z3)\n",
    "\n",
    "    [A1, A2, A3] is the input to the next layer as X is the input to the first layer. each layer will have its own weights and biases and the output of the last layer will be the input to the next layer. this is how we build a neural network with multiple layers.\n",
    "    The outputs from all the neurons are passed to the next layer (or the output layer). This is repeated across all layers of the network.\n",
    "    \n",
    "    # The code for this small network is as follows:    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class SimpleNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(4, 3)  # 4 input features -> 3 neurons\n",
    "            self.fc2 = nn.Linear(3, 2)  # 3 neurons -> 2 output classes\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))  # Apply ReLU after first layer\n",
    "            x = torch.relu(self.fc2(x))  # Apply ReLU after second layer (optional, see below)\n",
    "            return x  # Output will be a tensor like [P(Class 0), P(Class 1)]\n",
    "            \n",
    "    # For a layer with N neurons (for example, 128 neurons), the process for all the neurons in the layer happens in parallel. Here's how:\n",
    "\n",
    "    Weighted sum (for each neuron):\n",
    "        Zi = Σj=1 -> N (Wij * Xj) + Bi\n",
    "\n",
    "    Where:\n",
    "    - X is the input to the layer\n",
    "        - Size: [batch_size, 784] (for flattened 28x28 images)\n",
    "        - Xj is the j-th input feature\n",
    "    - W is the weight matrix for the layer\n",
    "        - Size: [784, 128] (784 inputs to 128 neurons)\n",
    "        - Wij is the weight connecting input j to neuron i\n",
    "    - B is the bias vector for the layer\n",
    "        - Size: [128] (one bias for each neuron i)\n",
    "    - Z is the weighted sum\n",
    "        - Size: [batch_size, 128] (one weighted sum Zi for each neuron, per example)\n",
    "\n",
    "    Then, the output of the layer after activation (ReLU) is:\n",
    "\n",
    "    Activation:\n",
    "        Ai = ReLU(Zi)\n",
    "\n",
    "    Where:\n",
    "    - A is the output matrix\n",
    "        - Size: [batch_size, 128]\n",
    "        - Ai is the output of neuron i after activation\n",
    "            \n",
    "    - What dose all this mean:\n",
    "    In a neual network if we want to pick up for ex edges we will assume that some neuron lights up (activates) when it sees a pixel associated with an edge.\n",
    "    to do this we assign a weight between nuerons in 2 layers (the input and the output) and a bias to the neuron. the weight is a number that tells us how much the input influences the output layer for 2 layers\n",
    "    for ex if pixel 144 is fully lit in the input layer some nuron in the next layer will be activated (the weight will be positive) and if pixel 144 is not lit some nuron in the next layer will not be activated (the weight will be negative).\n",
    "    But ofcourse we will have more than one neuron for this edge, \n",
    "    \n",
    "    lets say there are 10 weights and 10 nurons coming from the input layer to this 1 output neuron in the next layer that determines some edge.\n",
    "    to see if that neruon is activated we will take the weighted sum of all the inputs. if its positive the nuron will be activated and if its negative the nuron will not be activated.\n",
    "    but what if we want the nuron to be activated when, lets say the weighted sum is more than 10?, then we add a bias to the weighted sum after we add a bias to the weighted sum the output will be positive if the weighted sum is more than 10, we can see how adding bias's can give favor to some inputs over others that the model think are more important.\n",
    "    After this step we plug the sum into a activation function (like ReLU or sigmoid) to get the final output of the neuron, which will be betwen 0 and 1 repersenting the probability.\n",
    "    this one nuron that tells us about the probability of just one small edge being there in our image. this is repeated for all the other neurons in the layer which all connect with other nurons in the next layer.\n",
    "    some activations from one layer bring about other activations in the next layer the goal is that in the last layer after calculating the activations for all the outputs there will be one neuron that will be activated for the digit in the image.\n",
    "    this will be the brightest nuron, the one with the highest activation. ofcourse any nuron can be 0-1 so to see the outputs as a probability distribution we apply softmax to the outputs of the last layer. the softmax will convert the outputs to a probability distribution between 0 and 1.\n",
    "    all the neurons's activations (0-1) will add up to 1 after the softmax. Then we take the neuron with the highest activation as the predicted digit. \n",
    "    \n",
    "    The goal of having layers is that the first layer can pick up on some patterns like edges and the next layer can pick up on more complex patterns like corners and the last layer can pick up on the final digit. this is how we build a neural network with multiple layers.\n",
    "    Ofcourse this would be a example of a perfectly trained nural network. in reality we would start will all the connections but the layers would not be able to pick up any patterns, they dont know what a 9 looks like.\n",
    "    In reality we would start with random weights and biases and then train the model to learn the patterns in the data. this is done by using a loss function and an optimizer. the loss function will tell us how far off the predictions are from the actual labels and the optimizer will update the weights and biases to minimize this loss.\n",
    "    Slowly the model would fix its mistakes and update the weights to learn the patterns in the data then when presenting with 784 nurons the next layer will be able to pick up edges and patterns that make up that images number and so on in the next layers until the final layer will have learned which activation in the previous layers corresponds to which digit. \n",
    "    \n",
    "    NOTE: every nuron in the first layer is connected to every nuron in the next layer (this is called a fully connected layer) and this is how we build a neural network with multiple layers.\n",
    "\"\"\"\n",
    "class SimpleNN(nn.Module): # out NN inherits from nn.Module from torch\n",
    "    def __init__(self): # this is the constructor of the class\n",
    "        super(SimpleNN, self).__init__() # this calls the constructor of the parent class nn.Module to initialize the neural network (meaning the weights and biases in the model)\n",
    "        # in the layers below we take the output of the previous layer and feed it into the next layer this is called a feedforward neural network (in tf we use dense layers which means each layer is fully connected to the next layer) here for ex each 128 neuron in the first layer output is connected to each 64 input neuron in the second layer and so on\n",
    "        # all of these are 'dense' layers meaning each layer is fully connected to the next layer, each neuron in the previous layer is connected to each neuron in the next layer\n",
    "        self.fc1 = nn.Linear(28*28, 128) # input layer (28*28=784 pixels) to hidden layer (128 neurons)\n",
    "        self.fc2 = nn.Linear(128, 64) # we take the output of the first layer and feed it into the second layer ( so we had 128 from the hidden layer in last line) and we have 64 neurons as the output\n",
    "        self.fc3 = nn.Linear(64, 10) # we take the 64 neurons from the second layer and feed it into the output layer with 10 outputs (10 neurons for 10 classes 0-9)\n",
    "\n",
    "    def forward(self, x): # this is the forward pass of the neural network (the input x is passed through the layers of the network) in total we have 3 layers\n",
    "        x = x.view(-1, 28*28)  # the view function is used to reshape a tensor without effecting teh data, the -1 tells pytorch to figure out the dimentions of the tensor automatically, the 28*28=784 flattens each 28x28 image into a 784-dimensional 1Dvector. EX: let's say, 64 images with shape [64, 28, 28] view(-1, 28*28) will convert it to [64, 784] 64 images with 784 pixels each.\n",
    "        x = torch.relu(self.fc1(x)) # apply ReLU activation function to the output of the first layer (this is a common activation function used in deep learning) it keeps the positive values and sets the negative values to 0. the fc1 has some weights and biases applied to it in this layer which can bring out lets say edges from the image and pick up some patters which the next layer will build on and see more complex patterns. remember that the operation in the brackets (passing through NN) is done before the relu\n",
    "        x = torch.relu(self.fc2(x)) # apply ReLU activation function to the output of the second layer\n",
    "        x = self.fc3(x)  # raw scores (logits), no activation function here (we will apply softmax in the loss function). the last layoer can no relu as its a output layer it simply outputs the logits (raw scores) the softmax is applied by the creiterion (loss function) in the next step. that converts the sroces to a probability distribution 0-1 where the highest score is the predicted class (0-9)\n",
    "        return x # return the output of the last layer (the logits), for example a run where the output was 6 would look like: tensor([[-2.2477,  0.4876,  0.9632, -1.0308,  0.7245, -0.8435,  7.4994, -5.4191,-2.4032, -6.1907]]) where it preds 6 correctly as its the highest score\n",
    "\n",
    "model = SimpleNN() # create an instance of the SimpleNN class (this is our model) and we can now use this model to train and test our data\n",
    "\n",
    "# 4. Loss and optimizer\n",
    "\"\"\" \n",
    "    Step 1: Set the Loss Function\n",
    "        ↓\n",
    "    Use nn.CrossEntropyLoss()\n",
    "        ↓\n",
    "        - Perfect for **multi-class classification** problems (like MNIST: digits 0-9).\n",
    "        - Internally applies a Softmax to the model's raw outputs (logits).\n",
    "        - Then computes the Cross-Entropy loss between predicted and true labels.\n",
    "        \n",
    "        Why CrossEntropy?\n",
    "        - Softmax turns outputs into probabilities.\n",
    "        - Cross-Entropy measures how far off the predicted probabilities are from the true labels.\n",
    "        - Lower CrossEntropy = better model predictions.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 2: Set the Optimizer\n",
    "        ↓\n",
    "    Use optim.Adam(model.parameters(), lr=0.001)\n",
    "        ↓\n",
    "        - Optimizer: updates the models weights based on the computed gradients.\n",
    "        - Adam (Adaptive Moment Estimation) = smart optimizer that combines the benefits of:\n",
    "            • Momentum → considers past gradients (faster convergence).\n",
    "            • RMSProp → adjusts learning rate for each parameter individually.\n",
    "        \n",
    "        Parameters:\n",
    "        - `model.parameters()`: tells the optimizer what to update (all learnable parameters in the model).\n",
    "        - `lr=0.001`: learning rate controls **how big the update steps** are.\n",
    "            • Too high → may overshoot the optimal solution. (overfitting)\n",
    "            • Too low → learning is very slow. (underfitting)               \n",
    "            • 0.001 → good default for Adam (common in deep learning).\n",
    "\n",
    "        ↓\n",
    "\n",
    "    After setting:\n",
    "    - During training → we'll compute loss with `criterion(outputs, labels)`.\n",
    "    - Then call `loss.backward()` → calculate gradients.\n",
    "    - Then call `optimizer.step()` → update model weights.\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss() # this is the loss function we will use to train our model (this is a common loss function used in deep learning for multi-class classification problems) it applies softmax to the output of the model and then calculates the cross entropy loss between the predicted and true labels. we use this loss to then calculate the gradients in the backward pass and update the weights of the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # this is the optimizer we will use to train our model (this is a common optimizer used in deep learning) it applies gradient descent to the weights of the model ( basically takes the gradients of the loss with respect to the weights and then updates the weights). the lr is the learning rate we will use to train our model this controls how fast the model learns a higher learning rate means the model will learn faster but it can also lead to overfitting and a lower learning rate means the model will learn slower but it can also lead to better results. we use 0.001 as a common value for the learning rate (this is a common value used in deep learning) this is the default value for the Adam optimizer in pytorch\n",
    "\n",
    "# 5. Training loop \n",
    "\"\"\" \n",
    "    Step 1: Define the Number of Epochs\n",
    "        ↓\n",
    "    epochs = 3\n",
    "        ↓\n",
    "        - In this case, we train for **3 epochs** (3 passes through the dataset).\n",
    "        - Each epoch means the model has seen all 60k training images once.\n",
    "        - More epochs = better model (if not overfitting), but takes more time.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 2: Begin the Training Loop\n",
    "        ↓\n",
    "    for epoch in range(epochs):\n",
    "        ↓\n",
    "        - This loops through all epochs.\n",
    "        - For each epoch, well iterate through the entire training set once.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 3: Initialize Running Loss\n",
    "        ↓\n",
    "    running_loss = 0.0\n",
    "        ↓\n",
    "        - Used to accumulate the loss over each batch in the epoch.\n",
    "        - At the end of the epoch, well print this for visualization.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 4: Loop Through the Training Data\n",
    "        ↓\n",
    "    for images, labels in trainloader:\n",
    "        ↓\n",
    "        - in one epoch we loop through the entire training set once so for in the inner for loop we iterate over the training data once\n",
    "        - Here, we get **batch_size=64** images and their corresponding labels from the training set.\n",
    "        - so we get 64 images and labels at a time from the training set for one iteration of the loop and we iterate over the entire training set once using this foor loop\n",
    "        - so thats 60000 images in mnist / 64 images per batch = 937 iterations of the loop, so our loop runs 937 times in one epoch\n",
    "        - this means after we run 3 epochs we will have 937*3 = 2811 iterations of the loop and 60000 images will have been seen by the model\n",
    "        - to do this we pass all 64 images into the model at once in a single iteration of the loop where pytorch will take care of the batch size for us. \n",
    "        - it actually gets a tenosr like so: [batch_size, 1, 28, 28]meaning 64 tensor of 128x128 images and outputs a tensor like so: [batch_size, 10] contaning predictions for all 64 images\n",
    "        - so for images, labels in trainloader, images is a batch of 64 images and labels is a batch of 64 labels (0-9) for the images so we feed the model 64 images per iteration and get 64 predictions back pytorch will take care of the batch size for us by using a tensor, shape mentionned above.\n",
    "        - Each batch is passed through the model to compute predictions and the loss.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 5: Zero Out Previous Gradients\n",
    "        ↓\n",
    "    optimizer.zero_grad()\n",
    "        ↓\n",
    "        - Clear out old gradients from the previous batch.\n",
    "        - Without this, gradients would accumulate across batches.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 6: Forward Pass (Model Prediction)\n",
    "        ↓\n",
    "    outputs = model(images)\n",
    "        ↓\n",
    "        - Pass the batch of images through the model.\n",
    "        - Get the **logits** (raw predictions) from the model.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 7: Calculate Loss\n",
    "        ↓\n",
    "    loss = criterion(outputs, labels)\n",
    "        ↓\n",
    "        - Compute the **loss** (difference between predicted outputs and actual labels).\n",
    "        - Uses **CrossEntropyLoss** to calculate how \"wrong\" the predictions are.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 8: Backpropagation (Compute Gradients)\n",
    "        ↓\n",
    "    loss.backward()\n",
    "        ↓\n",
    "        - Perform **backpropagation** to calculate the **gradients** of the loss with respect to the models weights. this step only calculates the gradients.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 9: Optimizer Step (Update Weights)\n",
    "        ↓\n",
    "    optimizer.step()\n",
    "        ↓\n",
    "        - Update model weights using **gradient descent** (via the optimizer). this step applies the calculated gradients to the model's weights.\n",
    "        - This makes the model smarter by moving its weights in the right direction.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 10: Update Running Loss\n",
    "        ↓\n",
    "    running_loss += loss.item()\n",
    "        ↓\n",
    "        - Add the current batch's loss to the accumulated `running_loss` for visualization.\n",
    "\n",
    "        ↓\n",
    "\n",
    "    Step 11: Print Epoch Loss\n",
    "        ↓\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(trainloader):.4f}\")\n",
    "        ↓\n",
    "        - Display the average loss for the epoch.\n",
    "        - The loss should decrease as training progresses.\n",
    "        \n",
    "    \n",
    "    ### Training Loop Diagram with the 4 inputs Example :\n",
    "    We have a simple neural network:\n",
    "\n",
    "    - Input size: 4\n",
    "    - First layer: 3 neurons\n",
    "    - Second layer: 2 neurons (final output: two classes)\n",
    "\n",
    "    ------------------------------------------------------\n",
    "    \n",
    "    ### Example:\n",
    "    \n",
    "    Assume we are working with just **one batch** containing a **single input vector**:\n",
    "    Example Input x = [x1, x2, x3, x4]\n",
    "\n",
    "    Step 1: Start Training\n",
    "    ----------------------\n",
    "    - We initialize training.\n",
    "    - Number of epochs can be anything, but here we'll walk through **one batch** in **one epoch**.\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Step 2: Zero Out Previous Gradients\n",
    "    -----------------------------------\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    - Before processing the batch, we clear old gradients.\n",
    "    - Otherwise, gradients from previous batches would incorrectly accumulate.\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Step 3: Forward Pass (Make Prediction)\n",
    "    --------------------------------------\n",
    "    outputs = model(x)\n",
    "\n",
    "    - Input vector x = [x1, x2, x3, x4] is passed to the model.\n",
    "    - First Layer:\n",
    "        - Multiply x by weight matrix W1 (shape [4,3]) and add bias b1 (shape [3]).\n",
    "        - Output: [h1, h2, h3] → 3 values (one for each neuron).\n",
    "        - Apply ReLU activation: zeros out any negatives.\n",
    "    - Second Layer:\n",
    "        - Take [h1, h2, h3] as input.\n",
    "        - Multiply by weight matrix W2 (shape [3,2]) and add bias b2 (shape [2]).\n",
    "        - Output: [o1, o2] → 2 raw scores for two classes.\n",
    "\n",
    "    Result: model predicts logits like [2.5, -1.0] just like we said in the NN diagram for logits [class 0, class 1]\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Step 4: Calculate Loss\n",
    "    ----------------------\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    - Compare model output [2.5, -1.0] to the true label.\n",
    "    - Example:\n",
    "        - True label = 0 (means class 0).\n",
    "    - CrossEntropyLoss will internally:\n",
    "        - Apply softmax to get class probabilities. softmax = e^Zi/Σj=1->N (e^Zj) where Z is the output of the model = softmax([2.5, -1.0]) = [0.924, 0.076] (probabilities for class 0 and class 1) meaning its 94% sure its class 0 and 7% sure its class 1.\n",
    "        - then we compare the probabilities with the true label. EX: if the true label is 0 (class 0) then the probability for class 0 will be 0.924 and the probability for class 1 will be 0.076.\n",
    "        - Compute the negative log likelihood of the predicted class. EX: (0.924) = - log(0.924) = 0.08. with this we can see how far off the model was from the true label\n",
    "          for example if the true output was class 0, and the model predicted class 0 then the loss will be 0.08 since thats the negative log likelihood of the predicted class. this is how we calculate the loss.\n",
    "    - Loss measures **how bad** the prediction is.\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Step 5: Backpropagation (Compute Gradients)\n",
    "    -------------------------------------------\n",
    "    loss.backward()\n",
    "    \n",
    "    W = [w11, w12, w13] # Weight matrix (4x3)\n",
    "        [w21, w22, w23]\n",
    "        [w31, w32, w33]\n",
    "        [w41, w42, w43]\n",
    "            \n",
    "    b = [b1, b2, b3]  # Bias vector (3 biases for 3 neurons)\n",
    "    \n",
    "    Symbols:\n",
    "        * NOTE all of these would be matrices in reality so it would not produce a single number\n",
    "        - W1 = weights of first layer\n",
    "        - b1 = biases of first layer\n",
    "        - W2 = weights of second layer\n",
    "        - b2 = biases of second layer\n",
    "        - dL   = the computed loss\n",
    "        - dL/dW = (gradient) of loss w.r.t. weights (how sensitive the loss is to changes in weights) (in our case W1 and W2 respectively)\n",
    "        - dL/db = (gradient) of loss w.r.t. biases (how sensitive the loss is to changes in biases) (in our case b1 and b2 respectively)\n",
    "\n",
    "\n",
    "    - Compute **gradients** of the loss with respect to each model parameter:\n",
    "        - Gradients for W1, b1 (first layer weights and biases) = dL/dW1, dL/db1\n",
    "        - Gradients for W2, b2 (second layer weights and biases) = dL/dW2, dL/db2\n",
    "    - Tells each weight whether it should go up or down to reduce the loss.\n",
    "    - EX: Lets say for example from the last step we got the loss as 0.08 and we want to reduce it to 0.01, then we will calculate the gradients for each weight and bias in the model and see how much they contribute to the loss. \n",
    "      if a weight is contributing a lot to the loss then we will update it more than a weight that is not contributing much to the loss. this is how we calculate the gradients.\n",
    "      \n",
    "    Note: No weights updated yet — just calculated gradients.\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Step 6: Optimizer Step (Update Weights)\n",
    "    ---------------------------------------\n",
    "    optimizer.step()\n",
    "\n",
    "    - Now use the calculated gradients to **update** the model weights:\n",
    "        -> W1 = W1 - learning_rate * dL/dW1 (dL/dW1 is the gradient)\n",
    "        -> b1 = b1 - learning_rate * dL/db1\n",
    "        -> W2 = W2 - learning_rate * dL/dW2\n",
    "        -> b2 = b2 - learning_rate * dL/db2\n",
    "    - We move the weights a little bit in some direction to reduce the loss by using the old weights and the gradients we calculated in the previous step to get the new weights.\n",
    "\n",
    "    ------------------------------------------------------\n",
    "\n",
    "    Summary:\n",
    "    - For every batch: zero gradients → predict → compute loss → calculate gradients → update weights.\n",
    "    - After many epochs and batches, the model will (hopefully) predict better!\n",
    "\"\"\"\n",
    "eval = 0\n",
    "epochs = 3  # 3 epochs, each epoch means the model has seen all the training data once, its basically a pass through the entire dataset in our case 60k images (the more epochs the more accurate the model will be but the more time it will take to train)\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0 # this is the loss for the current epoch (we will print this at the end of each epoch) just a vusualization of the loss for us\n",
    "    for images, labels in trainloader: # this is the training loop we will iterate over the training data in batches of 64 images (the batch size we defined above) and labels are the true labels for the images (i.e if the image is a 0,1,2,3,4,5,6,7,8 or 9), images is a batch of 64 images and labels is a batch of 64 labels\n",
    "        optimizer.zero_grad() # this is the first step in the training loop we will set the gradients to zero (this is important because we don't want to accumulate the gradients from the previous batch) this is done to avoid the gradients from the previous batch to be added to the current batch\n",
    "        outputs = model(images) # this is the forward pass we will pass the images through the model and get the output (the logits) this is done by calling the model (which is an instance of the SimpleNN class) and passing the images to it, image is a batch of 64 images\n",
    "        eval += 1\n",
    "        loss = criterion(outputs, labels) # this is the loss calculation we will calculate the loss between the output of the model (the logits) and the true labels (the labels) this is done by calling the criterion (which is an instance of the CrossEntropyLoss class) and passing the outputs and labels to it\n",
    "        loss.backward() # after we calculate the loss in the previous step we will calculate the gradients of the loss with respect to the weights of the model (this is done by calling the backward method on the loss) this is called backpropagation (this is a common step in deep learning) after this we will have the gradients of the loss with respect to the weights of the model\n",
    "        optimizer.step() # after we calculate the gradients in the previous step we will update the weights of the model (this is done by calling the step method on the optimizer) this is called gradient descent\n",
    "        running_loss += loss.item() # this is just a visualization of the loss we will add the loss of the current batch to the running loss (this is done by calling the item method on the loss) this will give us the loss as a number (not a tensor) so we can print it\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(trainloader):.4f}\") # this is just a visualization of the loss we will print the loss for the current epoch, each epoch our loss should decrease\n",
    "\n",
    "print(\"Training complete ✅\", eval) # after this our training for the model class is done our model has now been updated with the weights and biases for the training data we passed to it\n",
    "\n",
    "# 6. Inference: show image + prediction\n",
    "\"\"\" \n",
    "    # Step 1: Set the model to evaluation mode\n",
    "    model.eval()  # Set the model to evaluation mode (important!)\n",
    "    ↓\n",
    "    - This disables certain behaviors like dropout and batch normalization.\n",
    "    - The model will behave consistently during testing.\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 2: Start looping through the test data\n",
    "    for i, (image, label) in enumerate(testloader):  # Get images and true labels from the testloader\n",
    "    ↓\n",
    "    - The `enumerate` function is used to loop through batches of images and labels.\n",
    "    - We keep track of the index `i` to limit the number of examples (5 in this case).\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 3: Disable gradient calculations\n",
    "    with torch.no_grad():  # Disable gradient calculations for inference\n",
    "    ↓\n",
    "    - Inference does not require gradients, so we use `torch.no_grad()` to save memory and speed up the process.\n",
    "    - This prevents PyTorch from calculating the gradients during the forward pass.\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 4: Get the model's predictions\n",
    "    output = model(image)  # Pass the image through the model to get raw logits\n",
    "    ↓\n",
    "    - The model takes the image as input and produces raw scores (logits).\n",
    "    - These logits will be used to determine the predicted class.\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 5: Get the predicted class\n",
    "    _, predicted = torch.max(output, 1)  # Get the index of the highest logit, which is the predicted class index's corresponds to the predicted class\n",
    "    ↓\n",
    "    - The `torch.max()` function returns the index of the highest logit along the row (dim=1), which corresponds to the predicted class.\n",
    "    - The `predicted` tensor contains the predicted class (e.g., 0-9 for MNIST digits).\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 6: Visualize the image\n",
    "    plt.imshow(image.view(28,28), cmap='gray')  # Reshape and display the image (28x28) in grayscale\n",
    "    ↓\n",
    "    - The image tensor is reshaped to 28x28 (since it's a 1x28x28 tensor).\n",
    "    - `cmap='gray'` ensures the image is shown in grayscale.\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 7: Add title and label to the image\n",
    "    plt.title(f\"Predicted: {predicted.item()} (True: {label.item()})\")  # Show predicted and true labels as title\n",
    "    ↓\n",
    "    - The title of the plot includes both the predicted and true labels.\n",
    "    - The `.item()` function extracts the integer value from the tensor for display.\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 8: Hide the axis and display the image\n",
    "    plt.axis('off')  # Turn off axis for a cleaner display\n",
    "    plt.show()  # Show the image with the prediction and true label\n",
    "    ↓\n",
    "    - `plt.axis('off')` hides the axis for better visualization.\n",
    "    - `plt.show()` renders the image with the title (predicted vs true label).\n",
    "\n",
    "    ↓\n",
    "\n",
    "    # Step 9: Limit the number of examples\n",
    "    if i == 4:  # Stop after 5 images (we only want to show 5 test examples)\n",
    "        break\n",
    "    ↓\n",
    "    - We stop after 5 iterations to avoid displaying too many images.\n",
    "    - The loop breaks after 5 images, so only 5 predictions are shown.\n",
    "\"\"\"\n",
    "model.eval()  # set model to evaluation mode (important!)\n",
    "\n",
    "# Get a few test images to see the predictions\n",
    "import matplotlib.pyplot as plt # to visualize the images we will use matplotlib\n",
    "\n",
    "for i, (image, label) in enumerate(testloader): # from the testloader grab the images and labels (true values) and use a index i to keep track of the number of images we have seen, the lable is a iterable of 1x1 tensors and the image of 28x28 normalized values exactly what our model expects\n",
    "    with torch.no_grad(): # this is important because we don't want to calculate the gradients in the inference step (this is done by calling the no_grad method on torch) this will save memory and make the inference faster\n",
    "        output = model(image) # pass the image in the NN and get the output, our constructor has no inputs but when we pass a vaiable in our model it will call the forward method in the SimpleNN class and pass the image to it and get the output as the raw scores (logits) from the last layer (no softmax applied her but still its a probability distribution)\n",
    "        _, predicted = torch.max(output, 1)  # get the index of the highest logit, which is the predicted class. the max function returns the value and the index. in the model return statment i gave the example of a run, so it would return and hence output would store for a test run of 6 something like tensor([[-2.2477,  0.4876,  0.9632, -1.0308,  0.7245, -0.8435,  7.4994, -5.4191,-2.4032, -6.1907]]) getting the value is useless so we only grab the index as the index corresponds to the predicted class index 0-9 for digits 0-9. the 1 is the dimension we want to get the maximum value from 0 is for cols and 1 is for rows, so dim 1 tells it to look at each row and get the maximum value ffrom there which is perfect for us a the output is a row (1d tensor): predected for our ex: tensor([6]), use .item for value\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(image.view(28,28), cmap='gray') # convert the image to 2D (28x28) and use the gray color map to display the image in black and white (when we loaded the msist data set you can see its a tensor of 1x28x28 so we need to convert it to 2D 28x28 to display it) the view method is used to reshape the tensor to 2D (28x28) and the cmap parameter is used to set the color map to gray (black and white) which we know it is as the first dimension is 1 meaning only 1 channel (grayscale) \n",
    "    plt.title(f\"Predicted: {predicted.item()} (True: {label.item()})\") # plot titel with the predicted class and the true class. we have the predicted 1d tensor from predicted var, it looks like: tensor([6]) so we need to use .item() to get the value from the tensor which is 6. also lable is the same tensor : tensor([6]) so we use .item() to get the value from the tensor which is 6. this is just a visualization of the predicted and true class\n",
    "    plt.axis('off') # turn off the axis (this is just for visualization purposes) we have a image so we don't need the axis\n",
    "    plt.show() # show the image\n",
    "\n",
    "    # Just show 5 examples\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "# Save the model (optional)\n",
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, './assets/mnist_model_checkpoint.pth')\n",
    "\n",
    "# if you want to load later and go straight to test you can do this:\n",
    "# Later, to load everything\n",
    "\n",
    "# checkpoint = torch.load('mnist_model_checkpoint.pth')\n",
    "# model = SimpleNN()\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "# then do test like usual.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9361c",
   "metadata": {},
   "source": [
    "# Image recognition in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110238c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the MNIST dataset\n",
    "# The MNIST dataset consists of handwritten digits (0-9) and their labels.\n",
    "# We load both the training and test datasets as seperate variables so one we can train on and the other we can test on\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Normalize the images to a range of [-1, 1]\n",
    "# TensorFlow images are typically normalized to a 0 to 1 range by dividing by 255.0.\n",
    "# However, we will normalize to the range [-1, 1] by subtracting 0.5 and dividing by 0.5.\n",
    "# this is the equation we used in the pytorch example above\n",
    "train_images = train_images.astype(\"float32\") / 255.0 # Convert to float32 and normalize to [0, 1]\n",
    "test_images = test_images.astype(\"float32\") / 255.0 # Convert to float32 and normalize to [0, 1]\n",
    "train_images = (train_images - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "test_images = (test_images - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "\n",
    "# 3. Define the CNN model architecture\n",
    "# This is a simple neural network model with 2 hidden layers.\n",
    "# The first layer is a dense layer that flattens the 28x28 image into a 1D vector of size 784.\n",
    "# then the first layers output is fed into the second layer which has 128 so we go from 784 to 128 neurons\n",
    "# then the second layers output is fed into the third layer which has 64 neurons and finally the last layer has 10 neurons (one for each digit 0-9)\n",
    "model = models.Sequential([ \n",
    "    layers.Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into a 784-dimensional vector\n",
    "    layers.Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    layers.Dense(64, activation='relu'),   # Second hidden layer with 64 neurons and ReLU activation\n",
    "    layers.Dense(10)                       # Output layer with 10 units, one for each digit (0-9)\n",
    "])\n",
    "\n",
    "# 4. Compile the model\n",
    "# what compiling dose is it sets up the model for training by specifying the optimizer, loss function, and metrics to track.\n",
    "# it compiles our model with specified optimizer, loss function, and metrics.\n",
    "# The optimizer is Adam, which is efficient and popular for training deep neural networks. it basically updates the weights of the model during training\n",
    "# We use SparseCategoricalCrossentropy loss since we're dealing with multiple classes (0-9), this is called the loss function and it comuputes the difference between the predicted and true labels.\n",
    "# The metric 'accuracy' will give us the model's performance during training. this calculates the accuracy of the model during training and testing by comparing the predicted labels with the true labels\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # the from_logits=True argument indicates that the model's output is raw logits (not probabilities), so we don't need to apply softmax here\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Train the model, in this step we use the training data to impove the model by updating the weights and biases of the model\n",
    "# We train the model for 3 epochs, 1 epoch means the model has seen all the training data once.\n",
    "# The shuffle=True argument ensures that the data is shuffled before each epoch.\n",
    "epochs = 3\n",
    "history = model.fit(train_images, train_labels, epochs=epochs, batch_size=64, shuffle=True)\n",
    "\n",
    "# 6. Evaluate the model on the test dataset\n",
    "# After training, we evaluate how well the model performs on the unseen test data.\n",
    "# The test accuracy will indicate how well the model generalizes to new data.\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) # verbose=2 means we want to see the output of the evaluation\n",
    "print(f\"Test Loss: {test_loss:.4f} Test accuracy: {test_acc:.4f},\")  # Print out the test accuracy\n",
    "\n",
    "# 7. Make predictions on the test dataset\n",
    "# We use the trained model to predict the labels of the test images.\n",
    "# The predictions will contain the logits (raw scores before applying softmax).\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# 8. Visualize a few test images and their predicted labels\n",
    "# We'll plot the first few test images along with the predicted labels from the model.\n",
    "# The predictions array contains the raw output from the model; we use np.argmax to get the class index with the highest score.\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 5))  # Set up the figure size for displaying multiple images\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i+1)  # Arrange the images in a row\n",
    "    plt.imshow(test_images[i], cmap=plt.cm.binary)  # Display the image in grayscale\n",
    "     # Title will show the predicted label (converted from logits to class index using np.argmax which is a numpy function that returns the indices of the maximum values along an axis in our case the axis is 0 so it will return the index of the max value in the array)\n",
    "     # and remembrer since the indexes of the logits array ex: [[-2.2477, 0.4876 ...]] correspond to the predicted class (0-9) so we use np.argmax to get the index of the max value in the array which is the predicted class if the max logit is 1 then the predicted class is 1 as the index of 1 in the array corresponds to the number 1\n",
    "    plt.title(f\"Pred: {np.argmax(predictions[i])}\")  \n",
    "    plt.axis('off')  # Hide the axes for a cleaner look\n",
    "\n",
    "plt.show()  # Display the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Normalize the images to a range of [-1, 1]\n",
    "train_images = train_images.astype(\"float32\") / 255.0  # Convert to float32 and normalize to [0, 1]\n",
    "test_images = test_images.astype(\"float32\") / 255.0  # Convert to float32 and normalize to [0, 1]\n",
    "train_images = (train_images - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "test_images = (test_images - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "\n",
    "# 3. Define the CNN model architecture\n",
    "model = models.Sequential([ \n",
    "    layers.Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images into a 784-dimensional vector\n",
    "    layers.Dense(128, activation='relu'),  # First hidden layer with 128 neurons and ReLU activation\n",
    "    layers.Dense(64, activation='relu'),   # Second hidden layer with 64 neurons and ReLU activation\n",
    "    layers.Dense(10)                       # Output layer with 10 units, one for each digit (0-9)\n",
    "])\n",
    "\n",
    "# 4. Custom Softmax function\n",
    "# We'll apply softmax manually inside the training loop (after logits are produced).\n",
    "def softmax(logits):\n",
    "    return tf.nn.softmax(logits)\n",
    "\n",
    "# 5. Custom Loss Function\n",
    "# Here we define a custom loss function, using sparse categorical cross-entropy.\n",
    "# here we first calculate the loss between the true labels and the predicted labels (the logits) and then we take the mean of that loss it dose that for all the whole batch we pass in\n",
    "# the reduec mean gets the mean for all dimensions of the tensor (the loss) and returns a single value (the mean loss) this is done to get the average loss for the batch we passed in\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(y_true, y_pred))\n",
    "\n",
    "# 6. Custom Training Loop\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "num_batches = len(train_images) // batch_size # calculate the number of batches so we know how many times to iterate over the training data\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "# 7. Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step in range(num_batches): # iterate over the number of batches so we go over the whole training data one time for our epoch\n",
    "        # 8. Get the batch of data\n",
    "        batch_start = step * batch_size # calculate the start index of the batch\n",
    "        batch_end = (step + 1) * batch_size # calculate the end index of the batch\n",
    "        images_batch = train_images[batch_start:batch_end] # get the batch of images from the training data\n",
    "        labels_batch = train_labels[batch_start:batch_end] # get the batch of labels from the training data\n",
    "\n",
    "        # 9. Convert to tensors\n",
    "        images_batch = tf.convert_to_tensor(images_batch, dtype=tf.float32)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch, dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape: # this is the context manager that records the operations for automatic differentiation\n",
    "            # Forward pass: Get the logits from the model (no activation yet)\n",
    "            logits = model(images_batch, training=True)  \n",
    "            # Apply softmax to the logits\n",
    "            y_pred = softmax(logits)\n",
    "            # Calculate the loss\n",
    "            loss = custom_loss(labels_batch, y_pred)\n",
    "        \n",
    "        # 10. Backpropagation\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Print loss every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_batches}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "model.save('./assets/tf_mnist_model.h5')\n",
    "\n",
    "# 11. Evaluate the model on the test dataset\n",
    "# After training, we evaluate how well the model performs on the unseen test data.\n",
    "test_images_tensor = tf.convert_to_tensor(test_images, dtype=tf.float32)\n",
    "test_labels_tensor = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
    "\n",
    "logits_test = model(test_images_tensor) # get the logits from the model (no activation yet)\n",
    "y_pred_test = softmax(logits_test) # apply softmax to the logits\n",
    "\n",
    "# Compute test loss\n",
    "test_loss = custom_loss(test_labels_tensor, y_pred_test) # calculate the loss between the true labels and the predicted labels (the logits) and then we take the mean of that loss it dose that for all the whole batch we pass in\n",
    "\n",
    "# Compute test accuracy\n",
    "correct_preds = tf.equal(tf.argmax(y_pred_test, axis=1, output_type=tf.int64), tf.cast(test_labels_tensor, tf.int64)) # this gets the index of the max value in the logits and compares it to the true labels (the labels) and returns a boolean tensor of the same shape as the logits (the predictions) where True means the prediction was correct and False means it was incorrect\n",
    "test_acc = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) # this gets the mean of the boolean tensor (the predictions) and returns a single value (the mean accuracy) this is done to get the average accuracy for the batch we passed in\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 12. Make predictions on the test dataset\n",
    "# We use the trained model to predict the labels of the test images.\n",
    "predictions = y_pred_test\n",
    "\n",
    "# 13. Visualize a few test images and their predicted labels\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
    "    plt.title(f\"Pred: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
