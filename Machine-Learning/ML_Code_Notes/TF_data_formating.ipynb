{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ceb4eb0",
   "metadata": {},
   "source": [
    "# Prepare data format (TFRecord recommended)\n",
    "\n",
    "### What you’re doing:\n",
    "You’re taking raw examples (text, labels, maybe metadata) and encoding them into a binary format that TensorFlow can read fast and reliably. The usual choice is TFRecord. TFRecord stores serialized tf.train.Example protobufs. This reduces I/O overhead, makes shuffling and parallel reading efficient, and keeps the training code cleanly separated from raw file parsing.\n",
    "\n",
    "\n",
    "\n",
    "### Why TFRecord?\n",
    "- IO efficiency: Sequential binary reads are faster and easier to optimize than many small text reads.\n",
    "- Compatibility: tf.data.TFRecordDataset plugs directly into TensorFlow pipelines.\n",
    "- Schema stability: You define feature names and types that the parser expects.\n",
    "- Preprocessing options: You can precompute token ids and store them (fast) or store raw text and tokenize on-the-fly (flexible).\n",
    "  \n",
    "### TFRecord structure basics\n",
    "A single TFRecord file is a sequence of serialized tf.train.Example objects. Each Example contains a features map of named fields. Each field is a Feature which can be:\n",
    "- bytes_list (for raw strings or serialized objects),\n",
    "- int64_list (for integers, labels, token ids),\n",
    "- float_list (for floats).\n",
    "  \n",
    "Typical features for text classification\n",
    "- text (bytes) — the raw text string or pre-tokenized string.\n",
    "- label (int64) — class label.\n",
    "- Optional: input_ids (int64_list) — token ids if pre-tokenized.\n",
    "- Optional: attention_mask (int64_list) — 1/0 mask for padding.\n",
    "- Optional: metadata (bytes) — JSON or other small metadata.\n",
    "  \n",
    "### Why not just use Hugging Face transformers for embedding our raw text?\n",
    "- **Task-Specific Needs**: Not all ML tasks require the power and complexity of LLMs. For example, a simple sentiment analysis task on a small dataset might perform well with a lightweight tokenizer and embeddings tailored to the dataset, rather than using a large pre-trained model.\n",
    "- **Flexibility**: TFRecord allows you to preprocess once and reuse the data efficiently across multiple training runs. Hugging Face embeddings can be computed on-the-fly, but this adds computational overhead during training.\n",
    "- **Scalability**: For large datasets, precomputing embeddings and storing them in TFRecord can save time and resources.\n",
    "- **Customizability**: TFRecord lets you store additional features (e.g., metadata, labels) alongside embeddings, which can be useful for complex pipelines.\n",
    "- **Integration**: TFRecord integrates seamlessly with TensorFlow's `tf.data` API, enabling efficient data loading and preprocessing.\n",
    "\n",
    "Using a Hugging Face transformer to embed text is a great option when you want to leverage pre-trained language models for feature extraction. However, not every machine learning task requires large language models (LLMs). Here's why we are making our own embeddings and data formats instead of relying solely on online libraries:\n",
    "\n",
    "### Example of a non-LLM task\n",
    "Consider a recommendation system for a small e-commerce platform. Instead of using LLMs, you might:\n",
    "- Tokenize product descriptions with a simple regex tokenizer.\n",
    "- Use a small vocabulary to create embeddings.\n",
    "- Combine these embeddings with user interaction data (e.g., clicks, purchases) stored in TFRecord format.\n",
    "\n",
    "This approach is lightweight, interpretable, and sufficient for the task, without the overhead of LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23deeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to write TFrecords\n",
    "import tensorflow as tf\n",
    "\n",
    "def _bytes_feature(value: bytes):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value: int):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _int64_list_feature(values):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "\n",
    "# this function takes in a list of texts and labels and writes them to a TFRecord file\n",
    "# basically it converts the text ex and label into tf.train.Example and writes them to a TFRecord file in a format good for tensorflow\n",
    "# the ids are optional but here we give each example an input_ids field as well\n",
    "def write_examples(output_path, texts, labels, input_ids_list=None):\n",
    "    with tf.io.TFRecordWriter(output_path) as w:\n",
    "        for i, text in enumerate(texts):\n",
    "            feature = {\n",
    "                'text': _bytes_feature(text.encode('utf-8')),\n",
    "                'label': _int64_feature(int(labels[i])),\n",
    "            }\n",
    "            if input_ids_list is not None:\n",
    "                feature['input_ids'] = _int64_list_feature(input_ids_list[i])\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            w.write(example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7dd7a",
   "metadata": {},
   "source": [
    "### How to parse TFRecords in tf.data\n",
    "When sequences have variable length (token ids), store them as VarLenFeature and convert to dense with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we mean by parsing is we take the data we \n",
    "import tensorflow as tf\n",
    "\n",
    "feature_description = {\n",
    "    'text': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'input_ids': tf.io.VarLenFeature(tf.int64),  # variable-length list\n",
    "}\n",
    "\n",
    "def _parse_function(serialized_example, max_len=128):\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    text = example['text']  # tf.string\n",
    "    label = tf.cast(example['label'], tf.int32)\n",
    "    input_ids_sparse = example.get('input_ids')\n",
    "    if input_ids_sparse is not None:\n",
    "        input_ids = tf.sparse.to_dense(input_ids_sparse, default_value=0)  # shape=(None,)\n",
    "        input_ids = input_ids[:max_len]\n",
    "        pad_len = max_len - tf.shape(input_ids)[0]\n",
    "        input_ids = tf.cond(pad_len > 0,\n",
    "                            lambda: tf.pad(input_ids, [[0, pad_len]]),\n",
    "                            lambda: input_ids)\n",
    "        input_ids = tf.cast(input_ids, tf.int32)\n",
    "    else:\n",
    "        input_ids = tf.zeros([max_len], dtype=tf.int32)  # fallback\n",
    "    return input_ids, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f821a8",
   "metadata": {},
   "source": [
    "### Quick test: write and read TFRecord examples\\nThis cell demonstrates writing a TFRecord using the `write_examples` function defined above, then reading and parsing the file to inspect stored features and how `_parse_function` pads/truncates `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world style TFRecord write + parse demo (sentiment mini-dataset)\n",
    "import tensorflow as tf\n",
    "import os, re\n",
    "\n",
    "########################################\n",
    "# 1. Simulated raw dataset (product review sentiment)\n",
    "########################################\n",
    "texts = [\n",
    "    \"I love this phone, battery life is great\",\n",
    "    \"Terrible customer service, not recommended\",\n",
    "    \"Camera quality is amazing and fast\",\n",
    "    \"The screen cracked easily and support was slow\",\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative sentiment\n",
    "\n",
    "# Simple whitespace/punctuation tokenizer + vocab build (REAL systems use better tokenizers)\n",
    "def tokenize(text):\n",
    "    # keep words/apostrophes; lowercase for normalization\n",
    "    return re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "\n",
    "# Build vocabulary (reserve 0 for padding)\n",
    "all_tokens = []\n",
    "for t in texts:\n",
    "    all_tokens.extend(tokenize(t))\n",
    "vocab_tokens = sorted(set(all_tokens))\n",
    "vocab = {tok: i+1 for i, tok in enumerate(vocab_tokens)}  # ids start at 1\n",
    "\n",
    "# Convert texts to list of token ids\n",
    "input_ids_list = []\n",
    "for t in texts:\n",
    "    toks = tokenize(t)\n",
    "    ids = [vocab[x] for x in toks]\n",
    "    input_ids_list.append(ids)\n",
    "\n",
    "# Show vocab + tokenization mapping\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Vocabulary mapping (token -> id):\", vocab)\n",
    "for i, t in enumerate(texts):\n",
    "    print(f\"Text {i} tokens: {tokenize(t)} -> ids: {input_ids_list[i]} (len={len(input_ids_list[i])})\")\n",
    "\n",
    "# 2. Write TFRecord file with raw text, label, and token id list\n",
    "output_path = '/tmp/sentiment_demo.tfrecord'\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "write_examples(output_path, texts, labels, input_ids_list)\n",
    "print(f\"\\nWrote TFRecord file: {output_path} bytes={os.path.getsize(output_path)}\")\n",
    "\n",
    "# 3. Inspect raw serialized examples\n",
    "raw_ds = tf.data.TFRecordDataset([output_path])\n",
    "print(\"\\nSerialized example byte lengths:\")\n",
    "for i, raw in enumerate(raw_ds.take(len(texts))):\n",
    "    print(f\" Example {i}: {len(raw.numpy())} bytes (scalar tensor shape {raw.shape})\")\n",
    "\n",
    "# 4. Define feature schema and parse function (variable-length -> fixed length)\n",
    "feature_description = {\n",
    "    'text': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'input_ids': tf.io.VarLenFeature(tf.int64),\n",
    "}\n",
    "def parse_function(serialized, max_len=12):\n",
    "    ex = tf.io.parse_single_example(serialized, feature_description)\n",
    "    # raw bytes -> text, label cast\n",
    "    text = ex['text']\n",
    "    label = tf.cast(ex['label'], tf.int32)\n",
    "    # sparse -> dense list of ids\n",
    "    sparse_ids = ex['input_ids']\n",
    "    dense_ids = tf.sparse.to_dense(sparse_ids, default_value=0)  # original length\n",
    "    # truncate\n",
    "    dense_ids = dense_ids[:max_len]\n",
    "    pad_len = max_len - tf.shape(dense_ids)[0]\n",
    "    # right pad with zeros if shorter\n",
    "    dense_ids = tf.cond(pad_len > 0, lambda: tf.pad(dense_ids, [[0, pad_len]]), lambda: dense_ids)\n",
    "    dense_ids = tf.cast(dense_ids, tf.int32)\n",
    "    return {'text': text, 'input_ids': dense_ids, 'label': label}\n",
    "\n",
    "# Recreate raw dataset (raw_ds was partially consumed above)\n",
    "raw_ds = tf.data.TFRecordDataset([output_path])\n",
    "parsed_ds = raw_ds.map(lambda x: parse_function(x, max_len=12))\n",
    "\n",
    "# 5. Show each parsed example (after padding/truncation)\n",
    "print(\"\\nParsed examples (fixed length input_ids, max_len=12):\")\n",
    "for i, ex in enumerate(parsed_ds.take(len(texts))):\n",
    "    print(f\" Example {i}:\")\n",
    "    print(\"   text:\", ex['text'].numpy().decode('utf-8'))\n",
    "    print(\"   label:\", int(ex['label'].numpy()))\n",
    "    print(\"   input_ids (len=12):\", ex['input_ids'].numpy())\n",
    "\n",
    "# 6. Batching (how model will typically consume data)\n",
    "batched = parsed_ds.batch(2)\n",
    "print(\"\\nBatches (size=2):\")\n",
    "for bi, batch in enumerate(batched):\n",
    "    print(f\" Batch {bi}:\")\n",
    "    print(\"   labels:\", batch['label'].numpy())\n",
    "    print(\"   input_ids shape:\", batch['input_ids'].shape)\n",
    "    print(\"   first review text:\", batch['text'][0].numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000e419",
   "metadata": {},
   "source": [
    "# Beginner Explanation: What just happened in the TFRecord demo\n",
    "\n",
    "We'll walk through **every printed section** from the code cell above and connect it to why data formatting (TFRecord + parsing) matters for machine learning.\n",
    "\n",
    "---\n",
    "## A. Why format data at all?\n",
    "Raw data (text, numbers, images) lives in many messy forms. Before training a model we need:\n",
    "1. **Consistency** – every training example should have the same *shape* (e.g. a fixed-length vector of token ids).\n",
    "2. **Speed** – binary formats (TFRecord) load faster than reading many tiny text files.\n",
    "3. **Portability** – you can move TFRecords between machines without changing parsing code.\n",
    "4. **Separation of concerns** – expensive tokenization/preprocessing can be done once (during writing) instead of every epoch.\n",
    "\n",
    "So we \"format\" the raw dataset into a structured, efficient representation that the `tf.data` pipeline can stream.\n",
    "\n",
    "---\n",
    "## B. Building a mini real-world dataset\n",
    "We pretended we have a **sentiment classification** dataset: short product review sentences labeled 1 (positive) or 0 (negative).\n",
    "\n",
    "Texts:\n",
    "- Positive: \"I love this phone, battery life is great\" (label 1)\n",
    "- Negative: \"Terrible customer service, not recommended\" (label 0)\n",
    "- Positive: \"Camera quality is amazing and fast\" (label 1)\n",
    "- Negative: \"The screen cracked easily and support was slow\" (label 0)\n",
    "\n",
    "Goal: Turn each sentence into a numeric vector (token ids) + its label.\n",
    "\n",
    "---\n",
    "## C. Tokenization and vocabulary\n",
    "Printed lines:\n",
    "```\n",
    "Vocabulary size: 25\n",
    "Vocabulary mapping (token -> id): {...}\n",
    "Text 0 tokens: ['i', 'love', 'this', 'phone', 'battery', 'life', 'is', 'great'] -> ids: [10, 13, 24, 15, 3, 12, 11, 9] (len=8)\n",
    "...\n",
    "```\n",
    "What this means:\n",
    "- We split each sentence into lowercase word tokens using a simple regex (`tokenize`).\n",
    "- We built a **vocabulary**: each unique token gets an integer id (starting at 1). Id **0** is reserved for padding.\n",
    "- For every text we converted tokens to their ids – these integer lists are variable length (5, 6, 8, ...). Models prefer fixed length, so we'll pad later.\n",
    "\n",
    "Why do this upfront?\n",
    "- Numeric ids are what embedding layers / neural networks expect.\n",
    "- Storing them now inside TFRecord avoids recomputing tokenization every training epoch.\n",
    "\n",
    "---\n",
    "## D. Writing the TFRecord file\n",
    "Printed line:\n",
    "```\n",
    "Wrote TFRecord file: /tmp/sentiment_demo.tfrecord bytes=457\n",
    "```\n",
    "Meaning:\n",
    "- We called `write_examples(...)`: for each example we created a `tf.train.Example` protobuf with fields:\n",
    "  - `text` (raw bytes of the original sentence)\n",
    "  - `label` (0 or 1)\n",
    "  - `input_ids` (list of token ids)\n",
    "- All examples were serialized and appended into one TFRecord file.\n",
    "\n",
    "Why store both `text` and `input_ids`?\n",
    "- Flexibility: You can later re-tokenize differently if you wish (you still have raw text).\n",
    "- Speed: You have precomputed ids ready for training now.\n",
    "\n",
    "---\n",
    "## E. Raw serialized examples\n",
    "Printed lines:\n",
    "```\n",
    "Serialized example byte lengths:\n",
    " Example 0: 99 bytes (scalar tensor shape ())\n",
    " ...\n",
    "```\n",
    "Meaning:\n",
    "- Each line shows the size in bytes of one serialized `tf.train.Example` record.\n",
    "- Shape `()` means a scalar Tensor whose value is the bytes blob.\n",
    "- Different lengths happen because sentences and token id lists vary.\n",
    "\n",
    "---\n",
    "## F. Parsing (turn bytes back into tensors)\n",
    "Printed block for each example:\n",
    "```\n",
    "Parsed examples (fixed length input_ids, max_len=12):\n",
    " Example 0:\n",
    "   text: I love this phone, battery life is great\n",
    "   label: 1\n",
    "   input_ids (len=12): [10 13 24 15  3 12 11  9  0  0  0  0]\n",
    "...\n",
    "```\n",
    "Meaning:\n",
    "- We read each serialized record and used `tf.io.parse_single_example` with a **feature description**:\n",
    "  - `FixedLenFeature([], tf.string)` for `text` and `label` (single values).\n",
    "  - `VarLenFeature(tf.int64)` for `input_ids` (because length varies per example).\n",
    "- `VarLenFeature` returns a sparse representation; we turned it into a dense vector.\n",
    "- We then enforced a **fixed length** (`max_len=12`):\n",
    "  - If original length < 12 → pad zeros on the right.\n",
    "  - If original length > 12 → truncate (not shown here, but that's what the slice does).\n",
    "- Result: every example now has a uniform `input_ids` shape `(12,)` suitable for batching and passing to a model.\n",
    "\n",
    "Why pad/truncate?\n",
    "- Neural nets usually operate on fixed-size tensors for simplicity and speed (especially when using batches).\n",
    "- Padding with zeros lets us keep original relative positions of real tokens at the front.\n",
    "\n",
    "---\n",
    "## G. Batching\n",
    "Printed lines:\n",
    "```\n",
    "Batches (size=2):\n",
    " Batch 0:\n",
    "   labels: [1 0]\n",
    "   input_ids shape: (2, 12)\n",
    "   first review text: I love this phone, battery life is great\n",
    "...\n",
    "```\n",
    "Meaning:\n",
    "- We grouped examples into batches of 2.\n",
    "- `input_ids shape: (2, 12)` means: 2 examples per batch, each with length 12 vector.\n",
    "- Batching lets the model process multiple examples in parallel on the GPU/accelerator.\n",
    "\n",
    "---\n",
    "## H. What is actually happening under the hood?\n",
    "Step-by-step flow for one example:\n",
    "1. Raw Python string (\"Terrible customer service, not recommended\").\n",
    "2. Tokenize into words → `['terrible','customer','service','not','recommended']`.\n",
    "3. Map words to ids (ids are choosen based on a fixed vocab file each token gets a unique int if and that id is used to look up the corrisposding embedding vector) → `[22,6,19,14,17]`.\n",
    "4. Create Feature protobuf: `{text: bytes, label: int64, input_ids: int64_list}`.\n",
    "5. Serialize and write to TFRecord file.\n",
    "6. Later: Read raw bytes from file via `TFRecordDataset`.\n",
    "7. Parse bytes back into structured tensors (string, int64, sparse list).\n",
    "8. Convert sparse list to dense, enforce fixed length (pad zeros) → `[22,6,19,14,17,0,0,0,0,0,0,0]`.\n",
    "9. Batch with other examples for training.\n",
    "\n",
    "---\n",
    "## I. Why TFRecord + tf.data instead of plain Python lists?\n",
    "- **Streaming**: `tf.data` can prefetch, shuffle, interleave files efficiently.\n",
    "- **Scalability**: Works the same for 4 examples or 40 million.\n",
    "- **Performance**: Binary sequential reads are fast and reduce Python overhead.\n",
    "- **Clean training loop**: Model code sees ready-to-use tensors; no custom per-epoch tokenization logic.\n",
    "\n",
    "---\n",
    "## J. The extra TensorFlow log line\n",
    "```\n",
    "Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
    "```\n",
    "This just means the dataset iteration reached the end. It's informational, not an error.\n",
    "\n",
    "---\n",
    "## K. Summary in plain words\n",
    "We converted human-readable sentences into a machine-friendly, uniform numeric format stored efficiently on disk. Then we loaded that formatted data, padded it to equal lengths, and batched it—exactly what a training loop needs. \"Data formatting\" here is the process of transforming raw, messy input into clean, consistent tensors.\n",
    "\n",
    "If you have questions about any single line, just ask which one and I'll zoom in further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
